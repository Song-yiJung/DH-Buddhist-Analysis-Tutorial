{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Song-yiJung/DH-Buddhist-Analysis-Tutorial/blob/main/9_Doc2Vec%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%B6%84%EC%84%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##📜 Doc2Vec: 단어를 넘어 문서 전체의 '주제'를 벡터로 만들기\n",
        "\n",
        "Word2Vec은 개별 단어의 의미를 벡터로 표현하는 방법이었다. 이를 통해 '왕'과 '여왕'의 관계를 파악하는 등의 분석이 가능했다.\n",
        "\n",
        "하지만 연구자는 종종 \"이 문서와 가장 유사한 주제의 문서를 찾아라\" 또는 \"수천 개의 문서를 주제별로 자동 분류하라\"와 같은, **문서 전체(Document-level)**를 대상으로 하는 과제에 직면한다. 이 때, 단순히 단어 벡터들을 평균 내는 방식(Mean of Word Embeddings)도 있지만, 문서의 고유한 주제와 뉘앙스를 더 정교하게 포착하기 위해 개발된 모델이 바로 **Doc2Vec (Paragraph Vector)**이다.\n",
        "\n",
        "### 1. Doc2Vec의 핵심 개념\n",
        "모든 문서에 고유한 '주제 태그'를 부여하다\n",
        "Doc2Vec의 가장 핵심적인 아이디어는 Word2Vec의 개념을 확장하여, 문서에 있는 모든 단어뿐만 아니라, 문서 그 자체에도 고유한 벡터가 있다고 가정하는 것이다.\n",
        "\n",
        "쉽게 비유하자면:\n",
        "도서관의 모든 책(문서)에 경제사-001, 언어학-005와 같이 그 책의 전체 주제를 나타내는 **고유한 '주제 태그(Paragraph ID)'**를 붙여주는 것과 같다. 이 '주제 태그' 역시 다른 단어들처럼 자신만의 의미 벡터를 가진다.\n",
        "\n",
        "Word2Vec이 단어의 의미를 주변 단어로부터 학습했다면, Doc2Vec은 **주변 단어 + 문서의 '주제 태그'**를 함께 고려하여 단어와 문서의 의미를 동시에 학습한다.\n",
        "\n",
        "작동 원리 (PV-DM과 PV-DBOW)\n",
        "Doc2Vec은 Word2Vec의 CBOW와 Skip-gram을 각각 확장한 두 가지 방식으로 학습을 진행한다.\n",
        "\n",
        "PV-DM (Paragraph Vector - Distributed Memory):\n",
        "\n",
        "Word2Vec의 CBOW를 확장한 방식이다. 문장의 빈칸을 채우는 '추측 게임'을 할 때, 주변 단어들의 벡터와 함께 이 단어가 속한 '문서의 주제 태그' 벡터까지 함께 사용하여 중심 단어를 예측한다.\n",
        "\n",
        "이 과정을 통해 모델은 단어의 문맥적 의미와 함께, 문서 전체의 주제까지 동시에 학습하게 된다.\n",
        "\n",
        "PV-DBOW (Paragraph Vector - Distributed Bag of Words):\n",
        "\n",
        "Word2Vec의 Skip-gram을 확장한 방식이다. 더 단순하게, 오직 '문서의 주제 태그' 벡터 하나만으로 해당 문서에 포함된 단어들을 무작위로 예측하는 훈련을 반복한다.\n",
        "\n",
        "이는 \"이 문서의 주제가 '불교 철학'이라면, 어떤 단어들이 주로 등장할까?\"라고 추측하는 과정과 유사하다.\n",
        "\n",
        "이러한 훈련이 끝나면, 우리는 각 단어의 벡터뿐만 아니라 **문서 전체의 주제를 함축하는 고유한 문서 벡터(Document Vector)**를 얻게 된다."
      ],
      "metadata": {
        "id": "oxOxKb2esRoq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. 등장 배경\n",
        "\n",
        "누가/언제: Doc2Vec은 2014년, Word2Vec을 개발한 구글의 토마스 미콜로프(Tomas Mikolov)와 쿠옥 레(Quoc Le)에 의해 개발되었다.\n",
        "\n",
        "왜: Word2Vec이 단어 수준의 분석에 큰 성공을 거두자, 연구자들은 자연스럽게 문장, 문단, 문서 전체와 같이 가변적인 길이의 텍스트 덩어리를 어떻게 고정된 크기의 벡터로 표현할 수 있을지에 대한 고민을 시작했다. Doc2Vec은 이러한 필요에 부응하여, 텍스트의 길이에 상관없이 전체적인 의미를 담는 단일 벡터를 효과적으로 생성하는 방법을 제시했다.\n",
        "\n",
        "###3. 디지털 인문학에서의 활용\n",
        "\n",
        "Doc2Vec은 텍스트를 '단어의 모음'이 아닌 '의미의 덩어리'로 취급할 수 있게 하여, 다음과 같은 거시적인 분석을 가능하게 한다.\n",
        "\n",
        "1. 문헌 추천 및 유사 문서 검색\n",
        "활용 방안: 연구자가 현재 읽고 있는 특정 사료(예: 『조선왕조실록』의 특정 기사)의 문서 벡터를 기준으로, 전체 실록 데이터베이스에서 벡터 거리가 가장 가까운 다른 기사들을 찾아낼 수 있다. 이는 특정 사건과 관련된 숨겨진 다른 기록이나, 비슷한 논조를 가진 다른 시기의 기사들을 자동으로 발견하는 강력한 탐색 도구가 된다.\n",
        "\n",
        "2. 주제별 문헌 군집화 (Clustering)\n",
        "활용 방안: 저자나 연대를 알 수 없는 수천 개의 고문서 뭉치(예: 개인이 기증한 편지 모음)를 Doc2Vec으로 학습시킨다. 그 결과로 나온 문서 벡터들을 K-Means와 같은 군집화 알고리즘에 적용하면, 컴퓨터가 자동으로 문서들을 내용에 따라 주제별(예: '가족 안부', '정치 동향', '금전 거래')로 분류해준다. 이는 대규모 아카이브 정리 및 탐색의 출발점이 된다.\n",
        "\n",
        "3. 저자 판별 및 문체 분석\n",
        "활용 방안: 여러 저자의 글을 Doc2Vec으로 학습시키면, 문서 벡터는 단순히 주제뿐만 아니라 저자가 단어를 선택하고 문장을 구성하는 전반적인 스타일까지 일부 반영하게 된다. 이를 통해 미상의 저자가 쓴 글의 문서 벡터가 어떤 저자의 문서 벡터 군집과 가장 가까운지를 계산하여, 해당 글의 저자를 통계적으로 추정하는 연구를 진행할 수 있다."
      ],
      "metadata": {
        "id": "hg_VDvN0sg5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 필수 라이브러리 설치\n",
        "# - `pip install --no-deps` 옵션을 사용하여 gensim만 설치합니다.\n",
        "# - 이렇게 하면 numpy와 scipy의 충돌을 무시하고 gensim을 설치할 수 있습니다.\n",
        "# - numpy와 scipy는 Colab에 이미 설치되어 있으므로, gensim만 설치하면 됩니다.\n",
        "!pip install gensim --no-deps -q\n",
        "print(\"✅ 필수 라이브러리 설치 완료: gensim\\n\")\n",
        "\n",
        "# 2. 라이브러리 임포트 및 데이터 준비\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import os\n",
        "\n",
        "# 가상의 디지털 인문학 문서 (한문 법구경 구절)\n",
        "documents = [\n",
        "    \"睡眠 解寤\",\n",
        "    \"宜 歡喜 思\",\n",
        "    \"聽 我 所 說\",\n",
        "    \"撰記 佛言\",\n",
        "    \"衆生 相剋\",\n",
        "    \"老死 猶然\",\n",
        "    \"命 終 自然\",\n",
        "    \"從 生死 得度\",\n",
        "    \"無 畏 無 患\",\n",
        "    \"行 不 離 心\",\n",
        "    \"世間 萬物 無常\",\n",
        "    \"善 言 安樂\",\n",
        "]\n",
        "\n",
        "# 3. 데이터 전처리 (토큰화 및 Doc2Vec 포맷으로 변환)\n",
        "tagged_data = [\n",
        "    TaggedDocument(words=doc.split(), tags=[i])\n",
        "    for i, doc in enumerate(documents)\n",
        "]\n",
        "\n",
        "print(\"✅ 데이터 전처리 및 TaggedDocument 변환 완료\")\n",
        "print(f\"변환된 데이터 예시 (1번 문서): {tagged_data[1]}\\n\")\n",
        "\n",
        "# 4. Doc2Vec 모델 학습\n",
        "model = Doc2Vec(vector_size=50, min_count=1, epochs=40)\n",
        "model.build_vocab(tagged_data)\n",
        "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "print(\"✅ Doc2Vec 모델 학습 완료\\n\")\n",
        "\n",
        "\n",
        "# 5. 모델을 활용한 문서 유사도 분석\n",
        "# '문서 4'를 기준 문서로 삼아, 이 문서와 가장 유사한 문서를 찾습니다.\n",
        "query_doc_id = 4\n",
        "query_doc_vector = model.dv[query_doc_id]\n",
        "\n",
        "similar_docs = model.dv.most_similar(positive=[query_doc_vector], topn=3)\n",
        "\n",
        "print(f\"🔍 기준 문서 ({query_doc_id}번 문서)와 가장 유사한 문서 Top 3:\\n\")\n",
        "print(f\"--- 기준 문서 내용 ---\\n{documents[query_doc_id]}\\n\")\n",
        "\n",
        "for doc_id, similarity in similar_docs:\n",
        "    if doc_id == query_doc_id:\n",
        "        continue\n",
        "    print(f\"----------------------------------------\")\n",
        "    print(f\"유사 문서 ID: {doc_id}번 문서\")\n",
        "    print(f\"유사도 점수: {similarity:.4f}\")\n",
        "    print(f\"문서 내용: {documents[doc_id]}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4fIjQMINW_k",
        "outputId": "a3e7e3b5-f1be-4547-c104-15d6f40eb5e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 필수 라이브러리 설치 완료: gensim\n",
            "\n",
            "✅ 데이터 전처리 및 TaggedDocument 변환 완료\n",
            "변환된 데이터 예시 (1번 문서): TaggedDocument<['宜', '歡喜', '思'], [1]>\n",
            "\n",
            "✅ Doc2Vec 모델 학습 완료\n",
            "\n",
            "🔍 기준 문서 (4번 문서)와 가장 유사한 문서 Top 3:\n",
            "\n",
            "--- 기준 문서 내용 ---\n",
            "衆生 相剋\n",
            "\n",
            "----------------------------------------\n",
            "유사 문서 ID: 9번 문서\n",
            "유사도 점수: 0.0535\n",
            "문서 내용: 行 不 離 心\n",
            "\n",
            "----------------------------------------\n",
            "유사 문서 ID: 10번 문서\n",
            "유사도 점수: -0.0142\n",
            "문서 내용: 世間 萬物 無常\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###💡 코드 구성 설명 (비전공자 눈높이)\n",
        "이 코드는 한문 문장들이 담긴 책의 문단들을 컴퓨터가 이해할 수 있는 방식으로 정리하고, 비슷한 주제를 가진 문단들을 찾아내는 과정을 보여준다.\n",
        "\n",
        "비유하자면, 도서관 사서가 수많은 책을 읽고 각 책의 주제를 파악해 \"이 책과 저 책은 철학이라는 비슷한 주제를 다루는군\" 하고 분류하는 것과 비슷하다.\n",
        "\n",
        "아래는 코드가 단계별로 어떤 일을 하는지 쉽게 풀어쓴 설명이다.\n",
        "\n",
        "**필요한 도구(라이브러리) 가져오기**\n",
        "\n",
        "pip install gensim은 \"Doc2Vec\"이라는 특별한 분석 도구가 담긴 상자(gensim 라이브러리)를 컴퓨터에 설치하는 명령이다. 이 도구 덕분에 우리는 복잡한 계산을 직접 할 필요 없이 문서를 분석할 수 있다.\n",
        "\n",
        "**데이터 준비하기**\n",
        "\n",
        "documents라는 변수 안에 한문 문장들을 저장했다. 이 한 문장 한 문장이 곧 컴퓨터가 분석할 \"문서\"가 된다.\n",
        "\n",
        "**데이터 정리하기**\n",
        "\n",
        "컴퓨터는 문장을 있는 그대로 이해하지 못한. 그래서 TaggedDocument라는 특별한 형태로 바꿔준다. 이 과정에서 각 문장(문서)에 고유한 번호(태그)를 붙여준다. 예를 들어, TaggedDocument<['宜', '歡喜', '思'], [1]>는 \"1번 문서의 내용은 '宜', '歡喜', '思'라는 단어로 이루어져 있어\"라고 컴퓨터에게 알려주는 작업이다.\n",
        "\n",
        "**모델 학습하기 (사서가 공부하는 과정)**\n",
        "\n",
        "model = Doc2Vec(...) 코드는 사서가 책을 읽으며 지식을 쌓는 과정과 같다. 컴퓨터는 documents에 있는 모든 문장과 단어들의 관계를 40번 반복해서 읽고, 각 문장이 어떤 '주제'를 가졌는지 숫자 벡터로 표현하는 방법을 학습한다. 이 학습이 끝나면, 각 문서는 자신만의 고유한 주제 벡터를 가지게 된다.\n",
        "\n",
        "**문서 유사도 분석하기 (사서가 추천해 주는 과정)**\n",
        "\n",
        "query_doc_id = 4는 \"4번 문서(衆生 相剋)와 가장 비슷한 주제를 가진 문서를 찾아줘\"라고 컴퓨터에게 요청하는 부분이다. model.dv.most_similar는 컴퓨터가 학습을 통해 만든 주제 벡터들을 비교해서 가장 비슷한 문서를 찾아준다."
      ],
      "metadata": {
        "id": "PxHEtqrcNrvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###🧠 분석 결과 해설\n",
        "\n",
        "결과를 보면 \"4번 문서(衆生 相剋)\"와 가장 유사한 문서로 \"9번 문서(行 不 離 心)\"가, 그다음으로 \"10번 문서(世間 萬物 無常)\"가 나왔다.\n",
        "\n",
        "\n",
        "유사도 점수:\n",
        "\n",
        "9번 문서: 0.0535\n",
        "\n",
        "10번 문서: -0.0142\n",
        "\n",
        "\n",
        "이 점수는 **코사인 유사도(Cosine Similarity)**를 나타낸다. 점수가 1에 가까울수록 매우 유사하고, -1에 가까울수록 전혀 다른 내용임을 의미한다. 0에 가깝다는 것은 유사도가 낮다는 뜻이다.\n",
        "\n",
        "**결과가 낮게 나온 이유**\n",
        "\n",
        "샘플 데이터의 부족: Doc2Vec은 문맥 정보를 학습해야 하는데, 저희가 사용한 데이터는 문장이 12개밖에 안 되는 아주 작은 양이다. 컴퓨터가 충분한 문맥을 학습하기에는 턱없이 부족했기 때문에, 문장들 간의 의미 관계를 제대로 파악하지 못했다.\n",
        "\n",
        "단어의 고립성: '衆生', '相剋' 같은 단어는 다른 문장에서 함께 등장하지 않는다. 단어의 중복이 거의 없어서, 모델이 공통된 주제를 찾아내기 어려웠다.\n",
        "\n",
        "따라서 현재의 결과는 학습 데이터가 너무 적어 모델이 제대로 된 의미 관계를 찾지 못했음을 보여준다. 이 코드를 수천, 수만 개의 실제 문서에 적용하면, 모델은 衆生 相剋과 老死 猶然, 世間 萬物 無常 같은 '불교적 깨달음'이나 '삶의 고통' 같은 공통된 주제를 가진 문장들을 훨씬 더 정확하게 찾아낼 수 있을 것이다.\n",
        "\n",
        "이 코드는 대규모 문헌 분석의 가능성을 보여주는 출발점이라고 생각하시면 된다."
      ],
      "metadata": {
        "id": "xV57AaYHOF0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "실습\n",
        "\n",
        "각 문서들의 기준- 특정 권의 품별로\n",
        "\n",
        "품 텍스트 분량이 길테니깐 문서 자체는 12개까지 아니여도 될 듯.\n",
        "\n",
        "분량 기준은 훈혁샘 실제작업으로 - > 부분도 설명 해주세용"
      ],
      "metadata": {
        "id": "mfBONYBZcYM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) 가능하면 메뉴에서 Runtime > Factory reset runtime 한 번 해주세요.\n",
        "#    (안 되면 아래 명령으로 최대한 정리합니다.)\n",
        "\n",
        "# 1) 충돌 가능 패키지 제거 (없으면 skip됨)\n",
        "!pip uninstall -y opencv-python opencv-contrib-python opencv-python-headless thinc tsfresh numba llvmlite gensim numpy scipy\n",
        "\n",
        "# 2) 안정 조합 설치 (Doc2Vec에 충분)\n",
        "!pip install -q numpy==1.26.4 scipy==1.11.4 gensim==4.3.2\n",
        "\n",
        "import os\n",
        "print(\"✅ 환경 정리/설치 완료. 런타임 재시작합니다...\")\n",
        "os.kill(os.getpid(), 9)  # 재시작"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjcVOgHBueTn",
        "outputId": "cdf720bc-16a1-462f-ae6d-0f6bd6c3457c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: opencv-python 4.12.0.88\n",
            "Uninstalling opencv-python-4.12.0.88:\n",
            "  Successfully uninstalled opencv-python-4.12.0.88\n",
            "Found existing installation: opencv-contrib-python 4.12.0.88\n",
            "Uninstalling opencv-contrib-python-4.12.0.88:\n",
            "  Successfully uninstalled opencv-contrib-python-4.12.0.88\n",
            "Found existing installation: opencv-python-headless 4.12.0.88\n",
            "Uninstalling opencv-python-headless-4.12.0.88:\n",
            "  Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "Found existing installation: thinc 8.3.6\n",
            "Uninstalling thinc-8.3.6:\n",
            "  Successfully uninstalled thinc-8.3.6\n",
            "Found existing installation: tsfresh 0.21.0\n",
            "Uninstalling tsfresh-0.21.0:\n",
            "  Successfully uninstalled tsfresh-0.21.0\n",
            "Found existing installation: numba 0.60.0\n",
            "Uninstalling numba-0.60.0:\n",
            "  Successfully uninstalled numba-0.60.0\n",
            "Found existing installation: llvmlite 0.43.0\n",
            "Uninstalling llvmlite-0.43.0:\n",
            "  Successfully uninstalled llvmlite-0.43.0\n",
            "\u001b[33mWARNING: Skipping gensim as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Found existing installation: scipy 1.16.1\n",
            "Uninstalling scipy-1.16.1:\n",
            "  Successfully uninstalled scipy-1.16.1\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 2.0.8 requires opencv-python-headless>=4.9.0.80, which is not installed.\n",
            "cuml-cu12 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "pynndescent 0.5.13 requires llvmlite>=0.30, which is not installed.\n",
            "pynndescent 0.5.13 requires numba>=0.51.2, which is not installed.\n",
            "dopamine-rl 4.1.2 requires opencv-python>=3.4.8.29, which is not installed.\n",
            "dask-cuda 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "albucore 0.0.24 requires opencv-python-headless>=4.9.0.80, which is not installed.\n",
            "librosa 0.11.0 requires numba>=0.51.0, which is not installed.\n",
            "cudf-cu12 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "umap-learn 0.5.9.post2 requires numba>=0.51.2, which is not installed.\n",
            "shap 0.48.0 requires numba>=0.54, which is not installed.\n",
            "stumpy 1.13.0 requires numba>=0.57.1, which is not installed.\n",
            "spacy 3.8.7 requires thinc<8.4.0,>=8.3.4, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 버전 확인\n",
        "import numpy, scipy, gensim\n",
        "print(\"numpy:\", numpy.__version__)\n",
        "print(\"scipy:\", scipy.__version__)\n",
        "print(\"gensim:\", gensim.__version__)\n",
        "\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import random, numpy as np\n",
        "\n",
        "# 재현성 고정\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ✅ 원문 데이터 (여기에 붙여 넣기)\n",
        "documents = [\n",
        "    # 無常品\n",
        "    \"無常 品 第一 無常 品 者 寤 欲 昏亂 榮命 難保 唯道 是 眞 睡眠 解寤 宜 歡喜 思 聽 我 所 說 撰記 佛言 所行 非常 謂 興衰 之 法 夫 生 輒 死 此 滅 爲 樂 譬如 陶家 埏埴 作 器 一切 要 壞 人命 亦 然 如 河 駛流 往 而 不返 人命 如是 逝者 不 還 譬 人 操杖 行 牧 食 牛 老死 猶 然 亦 養命 而 去 千百 非 一 族姓 男女 貯聚 財產 無 不 衰喪 生者 日夜 命 自 攻削 壽 之 消盡 如 𢄋䨍 之 水 常者 皆 盡 高者 亦 墮 合會 有 離 生者 有 死 衆生 相剋 以 喪 其 命 隨行 所 墮 自受 殃福 老 見 苦痛 死 則 意 去 樂家 縛獄 貪世 不斷 咄嗟 老 至 色 變 作 耄 少時 如意 老 見 蹈藉 雖 壽 百歲 亦 死 過去 爲 老 所 厭 病條 至際 是日 已 過 命 則 隨 減 如 少水 之 魚 斯 有 何樂 老 則 色 衰 所 病 自 壞 形 敗 腐朽 命終 自然 是身 何用 恒 漏 臭處 爲 病 所 困 有 老死 之 患 嗜欲 自恣 非法 是 增 不見 聞變 壽命 無常 非 有 子 恃 亦 非 父兄 爲 死 所 迫 無 親 可 怙 晝夜 慢惰 老 不止 婬 有財 不 施 不 受 佛言 有 此 四 弊 爲 自 侵欺 非空 非海 之中 非入 山石 之閒 無有 地方 之所 脫 之 不 受 死 是務 是 吾作 當作 令 致 是 人 爲 此 躁擾 履踐 老死 之 憂 知 此 能 自 淨 如是 見 生 盡 比丘 厭 魔兵 從 生死 得 度\",\n",
        "\n",
        "    # 教學品\n",
        "    \"教學 品 法句經 第二 教學 品 者 導 以 所行 釋 己 愚闇 得 見 道 明 咄哉 何 爲 寐 螉螺 蚌蠹 類 隱弊 以 不 淨 迷惑 計 爲 身 焉 有 被 斫創 心 如 嬰 疾痛 遘 于 衆 厄難 而 反 爲 用 眠 思 而 不 放逸 爲 仁 學 仁迹 從 是 無 有 憂 常 念 自 滅 意 正見 學 務 增 是 爲 世閒 明 所 生 福 千倍 終 不 墮 惡道 莫 學 小道 以 信 邪見 莫 習 放蕩 令 增 欲 意 善 修 法行 學 誦 莫 犯 行道 無 憂 世世 常 安 慜 學 攝身 常 愼 思言 是 到 不 死 行 滅 得 安 非務 勿 學 是務 宜 行 已 知 可 念 則 漏 得 滅 見 法 利 身 夫 到 善方 知 利 健行 是 謂 賢明 起覺 義 者 學 滅 以 固 著 滅 自恣 損 而 不 興 是 向 以 强 是 學 得 中 從 是 解義 宜 憶 念 行 學 先 斷 母 率 君 二 臣 廢 諸 營從 是 上 道人 學 無 朋類 不 得 善友 寧 獨 守 善 不 與 愚 偕 樂 戒 學 行 奚 用 伴 爲 獨 善 無 憂 如 空 野象 戒 聞 俱 善 二 者 孰 賢 方 戒 稱 聞 宜 諦 學 行 學 先 護戒 開閉 必 固 施 而 無 受 仂 行 勿 臥 若人 壽 百歲 邪學 志 不 善 不如 生 一日 精進 受 正法 若人 壽 百歲 奉火 修 異術 不如 須臾 頃 事 戒 者 福 稱 能 行 說 之 可 不能 勿 空 語 虛僞 無 誠信 智者 所 屏棄 學 當 先 求 解 觀察 別 是非 受 諦 應 誨 彼 慧然 不 復 惑 被髮 學 邪道 草衣 內 貪濁 曚曚 不 識 眞 如 聾 聽 五音 學 能 捨 三惡 以 藥 消 衆毒 健夫 度 生死 如 蛇 脫 故皮 學 而 多聞 持戒 不 失 兩世 見 譽 所 願 者 得 學 而 寡聞 持戒 不 完 兩世 受 痛 喪 其 本願 夫 學 有 二 常 親 多聞 安諦 解義 雖 困 不 邪 稊稗 害 禾 多欲 妨 學 耘除 衆惡 成 收 必 多 慮 而 後 言 辭 不 强梁 法 說 義 說 言 而 莫 違 善學 無 犯 畏 法 曉 忌 見 微 知 者 誡 無 後 患 遠 捨 罪 福 務 成 梵行 終身 自 攝 是 名 善學\",\n",
        "\n",
        "    # 多聞品\n",
        "    \"多聞 品 法句經 第三 多聞 品 者 亦 勸 聞 學 積 聞 成 聖 自 致 正覺 多聞 能 持 固 奉法 爲 垣牆 精進 難 踰毀 從 是 戒 慧 成 多聞 令 志 明 已 明 智慧 增 智 則 博 解義 見 義 行 法 安 多聞 能 除 憂 能 以 定 爲 歡 善說 甘露 法 自致 得 泥洹 聞 爲 知 法律 解疑 亦 見 正 從 聞 捨 非法 行 到 不死 處 爲 能 師 現道 解疑 令 學 明 亦 興 淸淨 本 能 奉持 法藏 能 攝 爲 解義 解 則 義 不 穿 受 法 猗 法 者 從 是 疾 得 安 若 多 少 有聞 自大 以 驕人 是 如 盲 執 燭 炤 彼 不 自 明 夫 求 爵位 財 尊貴 升天 福 辯慧 世 聞 悍 斯 聞 爲 第一 帝王 聘禮 聞 天上天 亦 然 聞 爲 第一 藏 最 富 旅力 强 智者 爲 聞 屈 好道 者 亦 樂 王者 盡心 事 雖 釋梵 亦 然 仙人 常 敬 聞 況 貴 巨 富 人 是以 慧 爲 貴 可禮 無 過 是 事 日 爲 明 故 事 父 爲 恩 故 事 君 以 力 故 聞 故 事 道人 人 爲 命 事 醫 欲 勝 依 豪强 法 在 智慧 處 福行 世世 明 察 友 在 爲 謀 別 伴 在 急時 觀 妻 在 房樂 欲 知 智 在 說 聞 爲 今世 利 妻子 昆弟 友 亦 致 後世 福 積聞 成 聖智 是 能 散 憂 恚 亦 除 不祥 衰 欲 得 安隱 吉 當 事 多聞 者 斫創 無 過 憂 射箭 無 過 愚 是 壯 莫 能 拔 唯 從 多聞 除 盲 從 是 得 眼 闇 者 從 得 燭 亦 導 世閒 人 如 目 將 無 目 是故 可 捨 癡 離 慢 豪 富 樂 務 學 事 聞 者 是 名 積聚 德\",\n",
        "\n",
        "    # 篤信品\n",
        "    \"篤信品法句經第四十有八章 篤信品者立道之根果於因正見行不迴傾 信慚戒意財 是法雅士譽 斯道明智說 如是昇天世 愚不修天行 亦不譽布施 信施助善者 從是到彼安 信者眞人長 念法所住安 近者意得上 智壽壽中賢 信能得道 法致滅度 從聞得智 所到有明 信能度淵 攝爲船師 精進除苦 慧到彼岸 士有信行 爲聖所譽 樂無爲者 一切縛解 信之與戒 慧意能行 健夫度恚 從是脫淵 信使戒誠 亦受智慧 在在能行 處處見養 比方世利 慧信爲明 是財上寶 家產非常 欲見諸眞 樂聽講法 能捨慳垢 此之爲信 信能度河 其福難奪 能禁止盜 野沙門樂 無信不習 好剝正言 如拙取水 掘泉揚泥 賢夫習智 樂仰淸流 如善取水 思令不擾 信不染他 唯賢與人 可好則學 非好則遠 信爲我輿 莫知斯載 如大象調 自調最勝 信財戒財 慚愧亦財 聞財施財 慧爲七財 從信守戒 常淨觀法 慧而利行 奉敬不忘 生有此財 不問男女 終以不貧 賢者識眞\",\n",
        "\n",
        "    # 誡愼品\n",
        "    \"誡愼品法句經第五十有六章 誡愼品者授與善道禁制邪非後無所悔也 人而常淸 奉律至終 淨修善行 如是戒成 慧人護戒 福致三寶 名聞得利 後上天樂 常見法處 護戒爲明 得成眞見 輩中吉祥 持戒者安 令身無惱 夜臥恬淡 寤則常歡 修戒布施 作福爲福 從是適彼 常到安處 何終爲善 何善安止 何爲人寶 何盜不取 戒終老安 戒善安止 慧爲人寶 福盜不取 比丘立戒 守攝諸根 食知自節 悟意令應 以戒降心 守意正定 內學正觀 無忘正智 明哲守戒 內思正智 行道如應 自淸除苦 蠲除諸垢 盡慢勿生 終身求法 勿暫離聖 戒定慧解 是當善惟 都已離垢 無禍除有 著解則度 餘不復生 越諸魔界 如日淸明 狂惑自恣 已常外避 戒定慧行 求滿勿離 持戒淸淨 心不自恣 正智已解 不睹邪部 是往吉處 爲無上道 亦捨非道 離諸魔界\",\n",
        "\n",
        "    # 惟念品\n",
        "    \"惟念品法句經第六十有二章 惟念品者守微之始內思安般必解道紀 出息入息念 具滿諦思惟 從初竟通利 安如佛所說 是則炤世閒 如雲解月現 起止學思惟 坐臥不廢忘 比丘立是念 前利後則勝 始得終必勝 逝不睹生死 若見身所住 六更以爲最 比丘常一心 便自知泥洹 已有是諸念 自身常建行 若其不如是 終不得意行 是隨本行者 如是度愛勞 若能悟意念 知解一心樂 應時等行法 是度老死惱 比丘悟意行 當令應是念 諸念生死棄 爲能作苦際 常當聽微妙 自覺悟其意 能覺者爲賢 終始無所會 以覺意能應 日夜務學行 當解甘露要 令諸漏得盡 夫人得善利 乃來自歸佛 是故當晝夜 常念佛法衆 己知自覺意 是爲佛弟子 常當晝夜念 佛與法及僧 念身念非常 念戒布施德 空不願無相 晝夜當念是\",\n",
        "\n",
        "    # 慈仁品\n",
        "    \"慈仁品法句經第七十有八章 慈仁品者是謂大人聖人所履德普無量 爲仁不殺 常能攝身 是處不死 所適無患 不殺爲仁 愼言守心 是處不死 所適無患 彼亂已整 守以慈仁 見怒能忍 是爲梵行 至誠安徐 口無麤言 不瞋彼所 是謂梵行 垂拱無爲 不害衆生 無所嬈惱 是應梵行 常以慈哀 淨如佛教 知足知止 是度生死 少欲好學 不惑於利 仁而不犯 世上所稱 仁壽無犯 不興變快 人爲諍擾 慧以默安 普憂賢友 哀加衆生 常行慈心 所適者安 仁儒不邪 安止無憂 上天衛之 智者樂慈 晝夜念慈 心無剋伐 不害衆生 是行無仇 不慈則殺 違戒言妄 過不與他 不觀衆生 酒致失志 爲放逸行 後墮惡道 無誠不眞 履仁行慈 博愛濟衆 有十一譽 福常隨身 臥安覺安 不見惡夢 天護人愛 不毒不兵 水火不喪 在所得利 死昇梵天 是爲十一 若念慈心 無量不廢 生死漸薄 得利度世 仁無亂志 慈最可行 愍傷衆生 此福無量 假令盡壽命 懃事天下人 象馬以祠天 不如行一慈\",\n",
        "\n",
        "    # 言語品\n",
        "    \"言語品法句經第八十有二章 言語品者所以戒口發說談論當用道理 惡言罵詈 憍陵蔑人 興起是行 疾怨滋生 遜言順辭 尊敬於人 棄結忍惡 疾怨自滅 夫士之生 斧在口中 所以斬身 由其惡言 諍爲少利 如掩失財 從彼致諍 令意向惡 譽惡惡所譽 是二俱爲惡 好以口儈鬪 是後皆無安 無道墮惡道 自增地獄苦 遠愚修忍意 念諦則無犯 從善得解脫 爲惡不得解 善解者爲賢 是爲脫惡惱 解自抱損意 不躁言得中 義說如法說 是言柔軟甘 是以言語者 必使己無患 亦不剋衆人 是爲能善言 言使投意可 亦令得歡喜 不使至惡意 出言衆悉可 至誠甘露說 如法而無過 諦如義如法 是爲近道立 說如佛言者 是吉得滅度 爲能作浩際 是謂言中上\"\n",
        "]\n",
        "\n",
        "if not documents:\n",
        "    raise ValueError(\"documents 리스트가 비었습니다. 원문 리스트를 붙여 넣어 주세요.\")\n",
        "\n",
        "# ✅ Doc2Vec 입력 포맷 변환\n",
        "tagged_data = [TaggedDocument(words=doc.split(), tags=[i]) for i, doc in enumerate(documents)]\n",
        "print(f\"🧾 문서 수: {len(tagged_data)}  |  예시 토큰: {tagged_data[0].words[:10]}\")\n",
        "\n",
        "# ✅ Doc2Vec 학습 (DM 모드로 문맥 보존)\n",
        "model = Doc2Vec(\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    workers=1,   # 재현성\n",
        "    dm=1,\n",
        "    seed=SEED,\n",
        "    epochs=60\n",
        ")\n",
        "model.build_vocab(tagged_data)\n",
        "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "print(\"✅ Doc2Vec 모델 학습 완료\")\n",
        "\n",
        "# ✅ 유사 문서 탐색 (기준 문서 자동 선택: '衆生 相剋' 포함 라인 우선)\n",
        "query_idx = next((i for i, doc in enumerate(documents) if \"衆生\" in doc), min(4, len(documents)-1))\n",
        "query_vec = model.dv[query_idx]\n",
        "similar_docs = model.dv.most_similar(positive=[query_vec], topn=min(5, len(documents)))\n",
        "\n",
        "print(\"\\n🔍 기준 문서:\")\n",
        "print(f\"[{query_idx}] {documents[query_idx]}\")\n",
        "print(\"\\n🔗 유사 문서 Top-k:\")\n",
        "for doc_id, sim in similar_docs:\n",
        "    if doc_id == query_idx:\n",
        "        continue\n",
        "    print(f\"- ({doc_id}) sim={sim:.4f} | {documents[doc_id]}\")\n",
        "\n",
        "# ✅ 임의 문장 추론 예시\n",
        "new_sentence = \"衆生 無常 生死 苦\"\n",
        "inferred_vec = model.infer_vector(new_sentence.split(), epochs=50, alpha=0.025)\n",
        "nearest = model.dv.most_similar([inferred_vec], topn=3)\n",
        "print(\"\\n🧪 임의 문장 유사 문서 Top-3:\")\n",
        "for doc_id, sim in nearest:\n",
        "    print(f\"- ({doc_id}) sim={sim:.4f} | {documents[doc_id]}\")"
      ],
      "metadata": {
        "id": "A0BLxw1dufxW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5274a6b-5369-4ca8-e235-2eeb84e6d77e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numpy: 1.26.4\n",
            "scipy: 1.11.4\n",
            "gensim: 4.3.2\n",
            "🧾 문서 수: 8  |  예시 토큰: ['無常', '品', '第一', '無常', '品', '者', '寤', '欲', '昏亂', '榮命']\n",
            "✅ Doc2Vec 모델 학습 완료\n",
            "\n",
            "🔍 기준 문서:\n",
            "[0] 無常 品 第一 無常 品 者 寤 欲 昏亂 榮命 難保 唯道 是 眞 睡眠 解寤 宜 歡喜 思 聽 我 所 說 撰記 佛言 所行 非常 謂 興衰 之 法 夫 生 輒 死 此 滅 爲 樂 譬如 陶家 埏埴 作 器 一切 要 壞 人命 亦 然 如 河 駛流 往 而 不返 人命 如是 逝者 不 還 譬 人 操杖 行 牧 食 牛 老死 猶 然 亦 養命 而 去 千百 非 一 族姓 男女 貯聚 財產 無 不 衰喪 生者 日夜 命 自 攻削 壽 之 消盡 如 𢄋䨍 之 水 常者 皆 盡 高者 亦 墮 合會 有 離 生者 有 死 衆生 相剋 以 喪 其 命 隨行 所 墮 自受 殃福 老 見 苦痛 死 則 意 去 樂家 縛獄 貪世 不斷 咄嗟 老 至 色 變 作 耄 少時 如意 老 見 蹈藉 雖 壽 百歲 亦 死 過去 爲 老 所 厭 病條 至際 是日 已 過 命 則 隨 減 如 少水 之 魚 斯 有 何樂 老 則 色 衰 所 病 自 壞 形 敗 腐朽 命終 自然 是身 何用 恒 漏 臭處 爲 病 所 困 有 老死 之 患 嗜欲 自恣 非法 是 增 不見 聞變 壽命 無常 非 有 子 恃 亦 非 父兄 爲 死 所 迫 無 親 可 怙 晝夜 慢惰 老 不止 婬 有財 不 施 不 受 佛言 有 此 四 弊 爲 自 侵欺 非空 非海 之中 非入 山石 之閒 無有 地方 之所 脫 之 不 受 死 是務 是 吾作 當作 令 致 是 人 爲 此 躁擾 履踐 老死 之 憂 知 此 能 自 淨 如是 見 生 盡 比丘 厭 魔兵 從 生死 得 度\n",
            "\n",
            "🔗 유사 문서 Top-k:\n",
            "- (3) sim=0.9965 | 篤信品法句經第四十有八章 篤信品者立道之根果於因正見行不迴傾 信慚戒意財 是法雅士譽 斯道明智說 如是昇天世 愚不修天行 亦不譽布施 信施助善者 從是到彼安 信者眞人長 念法所住安 近者意得上 智壽壽中賢 信能得道 法致滅度 從聞得智 所到有明 信能度淵 攝爲船師 精進除苦 慧到彼岸 士有信行 爲聖所譽 樂無爲者 一切縛解 信之與戒 慧意能行 健夫度恚 從是脫淵 信使戒誠 亦受智慧 在在能行 處處見養 比方世利 慧信爲明 是財上寶 家產非常 欲見諸眞 樂聽講法 能捨慳垢 此之爲信 信能度河 其福難奪 能禁止盜 野沙門樂 無信不習 好剝正言 如拙取水 掘泉揚泥 賢夫習智 樂仰淸流 如善取水 思令不擾 信不染他 唯賢與人 可好則學 非好則遠 信爲我輿 莫知斯載 如大象調 自調最勝 信財戒財 慚愧亦財 聞財施財 慧爲七財 從信守戒 常淨觀法 慧而利行 奉敬不忘 生有此財 不問男女 終以不貧 賢者識眞\n",
            "- (7) sim=0.9963 | 言語品法句經第八十有二章 言語品者所以戒口發說談論當用道理 惡言罵詈 憍陵蔑人 興起是行 疾怨滋生 遜言順辭 尊敬於人 棄結忍惡 疾怨自滅 夫士之生 斧在口中 所以斬身 由其惡言 諍爲少利 如掩失財 從彼致諍 令意向惡 譽惡惡所譽 是二俱爲惡 好以口儈鬪 是後皆無安 無道墮惡道 自增地獄苦 遠愚修忍意 念諦則無犯 從善得解脫 爲惡不得解 善解者爲賢 是爲脫惡惱 解自抱損意 不躁言得中 義說如法說 是言柔軟甘 是以言語者 必使己無患 亦不剋衆人 是爲能善言 言使投意可 亦令得歡喜 不使至惡意 出言衆悉可 至誠甘露說 如法而無過 諦如義如法 是爲近道立 說如佛言者 是吉得滅度 爲能作浩際 是謂言中上\n",
            "- (5) sim=0.9963 | 惟念品法句經第六十有二章 惟念品者守微之始內思安般必解道紀 出息入息念 具滿諦思惟 從初竟通利 安如佛所說 是則炤世閒 如雲解月現 起止學思惟 坐臥不廢忘 比丘立是念 前利後則勝 始得終必勝 逝不睹生死 若見身所住 六更以爲最 比丘常一心 便自知泥洹 已有是諸念 自身常建行 若其不如是 終不得意行 是隨本行者 如是度愛勞 若能悟意念 知解一心樂 應時等行法 是度老死惱 比丘悟意行 當令應是念 諸念生死棄 爲能作苦際 常當聽微妙 自覺悟其意 能覺者爲賢 終始無所會 以覺意能應 日夜務學行 當解甘露要 令諸漏得盡 夫人得善利 乃來自歸佛 是故當晝夜 常念佛法衆 己知自覺意 是爲佛弟子 常當晝夜念 佛與法及僧 念身念非常 念戒布施德 空不願無相 晝夜當念是\n",
            "- (4) sim=0.9960 | 誡愼品法句經第五十有六章 誡愼品者授與善道禁制邪非後無所悔也 人而常淸 奉律至終 淨修善行 如是戒成 慧人護戒 福致三寶 名聞得利 後上天樂 常見法處 護戒爲明 得成眞見 輩中吉祥 持戒者安 令身無惱 夜臥恬淡 寤則常歡 修戒布施 作福爲福 從是適彼 常到安處 何終爲善 何善安止 何爲人寶 何盜不取 戒終老安 戒善安止 慧爲人寶 福盜不取 比丘立戒 守攝諸根 食知自節 悟意令應 以戒降心 守意正定 內學正觀 無忘正智 明哲守戒 內思正智 行道如應 自淸除苦 蠲除諸垢 盡慢勿生 終身求法 勿暫離聖 戒定慧解 是當善惟 都已離垢 無禍除有 著解則度 餘不復生 越諸魔界 如日淸明 狂惑自恣 已常外避 戒定慧行 求滿勿離 持戒淸淨 心不自恣 正智已解 不睹邪部 是往吉處 爲無上道 亦捨非道 離諸魔界\n",
            "\n",
            "🧪 임의 문장 유사 문서 Top-3:\n",
            "- (0) sim=0.9994 | 無常 品 第一 無常 品 者 寤 欲 昏亂 榮命 難保 唯道 是 眞 睡眠 解寤 宜 歡喜 思 聽 我 所 說 撰記 佛言 所行 非常 謂 興衰 之 法 夫 生 輒 死 此 滅 爲 樂 譬如 陶家 埏埴 作 器 一切 要 壞 人命 亦 然 如 河 駛流 往 而 不返 人命 如是 逝者 不 還 譬 人 操杖 行 牧 食 牛 老死 猶 然 亦 養命 而 去 千百 非 一 族姓 男女 貯聚 財產 無 不 衰喪 生者 日夜 命 自 攻削 壽 之 消盡 如 𢄋䨍 之 水 常者 皆 盡 高者 亦 墮 合會 有 離 生者 有 死 衆生 相剋 以 喪 其 命 隨行 所 墮 自受 殃福 老 見 苦痛 死 則 意 去 樂家 縛獄 貪世 不斷 咄嗟 老 至 色 變 作 耄 少時 如意 老 見 蹈藉 雖 壽 百歲 亦 死 過去 爲 老 所 厭 病條 至際 是日 已 過 命 則 隨 減 如 少水 之 魚 斯 有 何樂 老 則 色 衰 所 病 自 壞 形 敗 腐朽 命終 自然 是身 何用 恒 漏 臭處 爲 病 所 困 有 老死 之 患 嗜欲 自恣 非法 是 增 不見 聞變 壽命 無常 非 有 子 恃 亦 非 父兄 爲 死 所 迫 無 親 可 怙 晝夜 慢惰 老 不止 婬 有財 不 施 不 受 佛言 有 此 四 弊 爲 自 侵欺 非空 非海 之中 非入 山石 之閒 無有 地方 之所 脫 之 不 受 死 是務 是 吾作 當作 令 致 是 人 爲 此 躁擾 履踐 老死 之 憂 知 此 能 自 淨 如是 見 生 盡 比丘 厭 魔兵 從 生死 得 度\n",
            "- (3) sim=0.9969 | 篤信品法句經第四十有八章 篤信品者立道之根果於因正見行不迴傾 信慚戒意財 是法雅士譽 斯道明智說 如是昇天世 愚不修天行 亦不譽布施 信施助善者 從是到彼安 信者眞人長 念法所住安 近者意得上 智壽壽中賢 信能得道 法致滅度 從聞得智 所到有明 信能度淵 攝爲船師 精進除苦 慧到彼岸 士有信行 爲聖所譽 樂無爲者 一切縛解 信之與戒 慧意能行 健夫度恚 從是脫淵 信使戒誠 亦受智慧 在在能行 處處見養 比方世利 慧信爲明 是財上寶 家產非常 欲見諸眞 樂聽講法 能捨慳垢 此之爲信 信能度河 其福難奪 能禁止盜 野沙門樂 無信不習 好剝正言 如拙取水 掘泉揚泥 賢夫習智 樂仰淸流 如善取水 思令不擾 信不染他 唯賢與人 可好則學 非好則遠 信爲我輿 莫知斯載 如大象調 自調最勝 信財戒財 慚愧亦財 聞財施財 慧爲七財 從信守戒 常淨觀法 慧而利行 奉敬不忘 生有此財 不問男女 終以不貧 賢者識眞\n",
            "- (5) sim=0.9966 | 惟念品法句經第六十有二章 惟念品者守微之始內思安般必解道紀 出息入息念 具滿諦思惟 從初竟通利 安如佛所說 是則炤世閒 如雲解月現 起止學思惟 坐臥不廢忘 比丘立是念 前利後則勝 始得終必勝 逝不睹生死 若見身所住 六更以爲最 比丘常一心 便自知泥洹 已有是諸念 自身常建行 若其不如是 終不得意行 是隨本行者 如是度愛勞 若能悟意念 知解一心樂 應時等行法 是度老死惱 比丘悟意行 當令應是念 諸念生死棄 爲能作苦際 常當聽微妙 自覺悟其意 能覺者爲賢 終始無所會 以覺意能應 日夜務學行 當解甘露要 令諸漏得盡 夫人得善利 乃來自歸佛 是故當晝夜 常念佛法衆 己知自覺意 是爲佛弟子 常當晝夜念 佛與法及僧 念身念非常 念戒布施德 空不願無相 晝夜當念是\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Doc2Vec이 무엇을 잡아냈나\n",
        "1. 문서들은 장(品) 단위로 길고, 서로 공유 어휘가 많음. (例: 道/行/善/惡/戒/慧/福/安/度/眞/法/比丘/世/生/死 …)\n",
        "\n",
        "2. Doc2Vec(DM)은 **문맥 공기어(co-occurrence)**를 강하게 반영한다. 즉, 한 문서 안에서 무상·노사(老死)·괴고(苦)·멸(滅)·도(道)·해탈(度) 류의 단어가 자주 함께 나오면, 그 묶음 자체가 문서 임베딩의 방향을 만든다.\n",
        "\n",
        "3. 무상품은 ‘無常·生死·老死·苦·滅’ 같은 실존·고(苦) 테마와 ‘道·度·安·善’ 같은 구원의 경로(도·해탈) 테마가 동시에 강하다.\n",
        "→ 이 “고(苦) + 도(道)”의 이중 축은, 다른 네 품에도 굵게 깔려 있어서 벡터가 자연히 가까워진다.\n",
        "\n",
        "# 2) 왜 하필 그 네 품인가 — 텍스트 근거 중심\n",
        "아래는 각 품에서 무상품의 핵심 어휘군과 겹치는 지점이 분명한 대목들이야. (긴 문장을 다 인용하지 않고, 중핵 어휘/표현만 콕 집을게.)\n",
        "\n",
        "篤信品 (신·계·혜·도/해탈 어휘 덩어리)\n",
        "상징 어휘: 信, 戒, 慧, 得道, 滅度, 度淵/彼岸, 福, 安, 聖, 無爲, 淨 …\n",
        "\n",
        "무상품의 “무상·고·사(死)”가 제시하는 실존적 문제에 대해, 篤信은 **그 해결 경로(信→戒→慧→度/彼岸)**를 정면으로 제시한다.\n",
        "→ “法致滅度, 慧到彼岸, 從信守戒, 慧而利行, 奉敬不忘” 류 표현이 ‘道/度/滅’ 축과 1:1로 맞물림.\n",
        "→ 결과적으로 고-멸-도 프레임이 강하게 공유되어 무상품과 매우 가깝게 배치.\n",
        "\n",
        "言語品 (정어/규범 윤리 + 해탈 지향 어휘)\n",
        "상징 어휘: 惡言/善言, 忍, 地獄, 解脫, 近道, 滅度, 如法, 無過 …\n",
        "\n",
        "표면상은 “말 조심”에 관한 장처럼 보이지만, 구조는 정어(正語)·정업(正業)로의 유도다.\n",
        "→ “從善得解脫, 諦如義如法, 近道立, 得滅度” 같은 해탈·도달 어휘가 빈번.\n",
        "→ 무상품의 “고/악업 → 도/멸” 프레임과 **윤리적 매개(言/忍/如法)**를 통해 긴밀히 공기어가 겹친다.\n",
        "\n",
        "惟念品 (정념/정정/열반 어휘)\n",
        "상징 어휘: 念, 定, 甘露, 泥洹(涅槃), 老死惱, 比丘, 應是念 …\n",
        "\n",
        "무상품의 노사(老死)와 무상 테마를 직접 참조(“是度老死惱”)하면서, 처방으로 念·定을 준다.\n",
        "→ “常念/正念, 甘露要, 令諸漏得盡, 自知泥洹” 등은 고의 소멸(盡)·열반(泥洹) 지향 어휘로, 무상품의 “滅/度/安” 축과 정확히 맞닿음.\n",
        "\n",
        "誡愼品 (계·정·혜 삼학 직결 어휘)\n",
        "상징 어휘: 戒, 定, 慧, 正智, 淨, 魔界越, 無上道 …\n",
        "\n",
        "무상품의 실존 문제를 계율·정정·지혜로 넘어서는 청정 프레임으로 받친다.\n",
        "→ “戒定慧解, 都已離垢, 越諸魔界, 爲無上道, 自淸除苦” 등은 **‘고–정화–해탈’**의 전형적인 경로 어휘.\n",
        "→ 무상품의 “苦痛/縛獄/貪世”와 정반대 벡터(정화·이탈)를 이루며, 그 대비가 같은 벡터공간에서 서로 가까운 방향성을 만든다(‘문제–해결’의 페어링).\n",
        "\n",
        "요약하면:\n",
        "無常(문제 제기) ↔ 篤信/言語/惟念/誡愼(해결 경로) 가 공유 어휘와 공기어 패턴으로 연결되어, 네 품이 무상품과 가장 가깝게 매칭된 것."
      ],
      "metadata": {
        "id": "kYs0nXHixVAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 무엇을 확인할 수 있나\n",
        "\n",
        "기준 문서가 무상품(‘衆生’ 포함)으로 잡혔고, 가장 가까운 품으로 篤信·言語·惟念·誡愼이 나왔습니다.\n",
        "→ 네 품 모두 규범·수행·정견/정어 축을 공유하므로 주제적으로 근접합니다. 모델이 장(品) 단위의 주제 유사성을 잘 포착했다고 볼 수 있어요.\n",
        "\n",
        "유사도 점수(0.996±)가 아주 높은 건 이상이 아니라, 코퍼스가 작고 (8문서), 불교 한문 특성상 공유 어휘(戒/慧/善/法/行/人/… )가 매우 많아 문서 벡터가 촘촘해지기 때문입니다. 값 자체보다 상대적 순위가 더 중요해요.\n",
        "\n",
        "# 정확도를 더 끌어올리는 쉬운 개선\n",
        "\n",
        "아래 둘만 해도 체감 좋아집니다.\n",
        "\n",
        "허사 제거(불용어): 之/者/是/爲/以/而/不/無/其/則/如/有/於/與/也/乃/所/之中/其中 …\n",
        "\n",
        "복합어 고정(n-gram): 多聞, 正見, 滅度, 老死, 佛言, 甘露, 梵行, 戒定慧 등은 붙여서 하나의 토큰으로."
      ],
      "metadata": {
        "id": "2R0DjG7Ku4BQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Colab 환경의 한계\n",
        "텍스트분석의 정확도와 활용성을 높이기 위해서는 대규모 토큰이 필요하지만, 토큰의 규모가 어느 한계를 넘어가면 Colab에서 세션 다운이 일어날 수 있음.\n",
        "\n",
        "Colab은 편하긴 하지만 RAM과 CPU 코어 제한 때문에 수만 개 토큰을 가진 대규모 학습을 돌리면 세션 다운(커널 재시작)이 자주 발생.\n",
        "\n",
        "특히 Gensim의 Doc2Vec이나 Word2Vec처럼 메모리를 많이 먹는 모델은 토큰 수 × 벡터 크기 × window × epochs 조합에 따라 쉽게 수GB 이상 메모리를 사용.\n",
        "\n",
        "# ⚙️ Colab 환경 최적화 팁\n",
        "## Colab Pro / Pro+ 사용\n",
        "\n",
        "RAM을 25GB 이상 확보 가능 → 대규모 토큰 처리 안정성 ↑\n",
        "\n",
        "CPU 코어 수 증가로 학습 속도 향상\n",
        "\n",
        "## 토큰 수 자체 줄이기\n",
        "\n",
        "불필요한 불용어 제거\n",
        "\n",
        "희귀 단어(min_count) 기준 올리기 (예: 2~5)\n",
        "\n",
        "문서 단위 샘플링: 전체 데이터에서 일부만 먼저 학습\n",
        "\n",
        "\n",
        "# 💡 Colab이 아닌 대안 환경\n",
        "## 로컬 PC\n",
        "\n",
        "RAM 32GB 이상, 8코어 이상 CPU (AMD Ryzen, Intel i7 이상)\n",
        "\n",
        "NVMe SSD 필수\n",
        "\n",
        "## 클라우드 서버\n",
        "\n",
        "AWS EC2 c6i.4xlarge (16vCPU / 32GB RAM) 이상\n",
        "\n",
        "GPU는 필수 아님 (Doc2Vec은 CPU 기반)\n",
        "\n",
        "GPU 활용은 Gensim에서 직접 안 되지만, CuML 같은 GPU 가속 라이브러리로 전환 가능\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "정리하면, **Colab 기본 환경(12GB RAM)**으로 수만 개 토큰 학습은 불안정하고,\n",
        "안정적으로 하려면 Colab Pro High-RAM + 불필요 토큰 최소화 + 모델 파라미터 절약이 필요.\n",
        "그게 어렵다면 아예 RAM 32GB 로컬 또는 클라우드 서버에서 돌리는 게 제일 안전."
      ],
      "metadata": {
        "id": "1RO7YFFUvyjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Colab에서 처리 가능한 문서 단위(1품(品)을 1문서로 규정했을 때)\n",
        "\n",
        "제미나이에 질문: 1만 품을 상단 코드의 documents에 적용할 경우 처리 가능한지?\n",
        "\n",
        "답변: 최상단 예문 코드의 경우와 비슷한 분량의 품을 1만 개까지 늘린다면, 분석 자체는 가능하지만 Colab의 무료 환경에서는 여러 가지 제약사항을 고려해야 합니다.\n",
        "\n",
        "\n",
        "## Colab 무료 환경의 제약사항\n",
        "메모리(RAM) 부족: 현재 예문은 8개의 품으로 이루어져 있어 메모리 사용량이 매우 적습니다. 하지만 비슷한 분량의 품이 1만 개가 되면, 전체 텍스트 데이터의 크기가 크게 증가합니다. gensim 라이브러리가 모델을 학습하는 과정에서 어휘집과 문서 벡터들을 모두 RAM에 올려놓기 때문에, 텍스트 데이터가 수백 MB를 넘어가면 **메모리 부족(MemoryError)**이 발생할 가능성이 매우 높아집니다.\n",
        "\n",
        "## 대책\n",
        "만약 메모리 부족 문제가 발생한다면, min_count 파라미터의 값을 늘려서 어휘집의 크기를 줄이거나, vector_size를 낮춰 문서 벡터의 차원을 줄이는 방법을 고려할 수 있습니다.\n",
        "\n",
        "## 학습 시간 증가\n",
        "Doc2Vec 모델은 데이터의 양에 비례하여 학습 시간이 길어집니다. 8개의 품을 학습하는 데는 몇 초밖에 걸리지 않지만, 1만 개의 품을 학습하려면 수십 분에서 몇 시간이 소요될 수 있습니다. Colab은 일정 시간(보통 12시간) 사용하지 않으면 런타임 연결이 끊어지기 때문에, 학습을 완료하기 전에 세션이 종료될 수도 있습니다.\n",
        "\n",
        "## CPU 성능 한계\n",
        "Colab의 무료 vCPU 2개는 일반적인 머신러닝 작업에는 충분하지만, 1만 개의 문서를 처리하는 대규모 작업에서는 학습 속도가 다소 느리게 느껴질 수 있습니다.\n",
        "\n",
        "## 결론\n",
        "예시 코드와 동일한 분량의 품 1만 개를 Colab에서 분석하는 것은 메모리 최적화를 통해 충분히 시도해볼 수 있는 수준입니다. 하지만 메모리 부족이나 긴 학습 시간과 같은 문제에 직면할 가능성이 높으므로, 데이터의 양을 점진적으로 늘려가며 테스트해보는 것이 좋습니다.\n",
        "\n",
        "만약 메모리 오류가 지속적으로 발생하거나 학습 시간이 너무 오래 걸린다면, Colab Pro와 같이 더 높은 사양의 유료 버전을 사용하거나, 데이터를 샘플링하여 일부만 학습시키는 방법도 고려해볼 수 있습니다.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 2. 세션 다운 원인 추측\n",
        "질문: 그렇다면 24개 품을 예문으로 적용해서 실행했을 때 세션 다운이 일어났다면 그것은 분량 외에 다른 문제일 가능성이 높은지?\n",
        "\n",
        "답변: 네, 맞습니다. 약 24개 품의 문서 분량으로 세션 다운이 일어났다면, 분량 외에 다른 문제가 있었을 가능성이 매우 높습니다.\n",
        "\n",
        "24개 품의 분량은 Colab 무료 환경에서 전혀 문제가 되지 않는 극히 적은 양입니다. 1만 품도 무리 없이 처리할 수 있는 환경임을 고려할 때, 24개 품에서 세션이 다운되었다면 다음 원인들을 의심해볼 수 있습니다.\n",
        "\n",
        "## 메모리 릭(Memory Leak)\n",
        "코드의 특정 부분이 메모리를 지속적으로 할당하고 해제하지 않아, 메모리 사용량이 계속해서 증가했을 수 있습니다. Doc2Vec 코드 자체는 메모리 관리가 잘 되어 있지만, 만약 코드의 다른 부분(데이터 로딩, 전처리 등)에서 메모리 릭이 발생했다면 적은 분량에서도 문제가 생길 수 있습니다.\n",
        "\n",
        "## 비정상적인 무한 루프\n",
        "학습 과정이나 데이터 전처리 과정에서 의도치 않은 무한 루프가 발생하여, CPU 사용량이 100%에 달하며 과부하로 세션이 다운되었을 수 있습니다.\n",
        "\n",
        "## Colab 환경 자체의 불안정\n",
        "드물게 Colab 서버 자체에 일시적인 문제가 있거나, 다른 사용자들이 서버 자원을 과도하게 사용해 세션이 끊길 수도 있습니다. 이 경우, 세션을 다시 시작하면 정상적으로 작동할 때가 많습니다.\n",
        "\n",
        "## 매우 긴 문장 또는 특이한 데이터\n",
        "24개 품 중 한 문서라도 길이가 매우 길거나, 특수한 문자가 포함되어 gensim 라이브러리가 처리하는 과정에서 예상치 못한 오류가 발생했을 수도 있습니다."
      ],
      "metadata": {
        "id": "az78sWuUzfKL"
      }
    }
  ]
}