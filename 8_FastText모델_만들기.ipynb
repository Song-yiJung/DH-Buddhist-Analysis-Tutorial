{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9ZwBhoDi1WojaQmQEG7SX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Song-yiJung/DH-Buddhist-Analysis-Tutorial/blob/main/8_FastText%EB%AA%A8%EB%8D%B8_%EB%A7%8C%EB%93%A4%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#📜 튜토리얼 8: Word2Vec의 진화, FastText의 원리와 한문 분석 활용\n",
        "\n",
        " Word2Vec과 GloVe는 단어의 의미를 벡터 공간에 표현하는 강력한 방법론이다. 하지만 이 모델들은 결정적인 약점을 가진다. 바로 어휘 사전에 없는 단어(Out-of-Vocabulary, OOV)에 대해서는 벡터를 생성할 수 없다는 점이다. 예를 들어, 훈련 데이터에 없던 '왕께서'라는 단어가 등장하면, Word2Vec은 이 단어를 처리하지 못한다.\n",
        "\n",
        "또한, '의지', '의지력', '의지하다'처럼 형태는 다르지만 의미가 유사한 단어들의 관계를 효과적으로 반영하지 못하는 한계도 있다. 이러한 문제를 해결하기 위해 등장한 것이 바로 FastText이다.\n",
        "\n"
      ],
      "metadata": {
        "id": "U8p-agJ5i_TU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1. 핵심 개념: 단어를 조각(Subword)의 합으로 바라보다.**\n",
        "\n",
        "FastText의 가장 핵심적인 아이디어는 단어를 더 이상 나눌 수 없는 하나의 단위(atomic unit)로 보지 않고, 여러 개의 작은 문자 조각(character n-gram)들의 집합으로 간주하는 것이다.\n",
        "\n",
        "\n",
        "* Word2Vec의 관점: apple이라는 단어는 그저 apple이라는 하나의 덩어리이다.\n",
        "\n",
        "* FastText의 관점: apple이라는 단어는 <ap, app, ppl, ple, le> 등 n-gram 문자 조각들의 합으로 이루어져 있다. (여기서 <와 >는 단어의 시작과 끝을 알리는 특수 기호이다.)\n",
        "\n",
        "##**2.작동 원리**\n",
        "FastText는 Word2Vec의 Skip-gram 모델을 기반으로 훈련을 진행한다. 그러나 결정적인 차이는, 단어 전체의 벡터를 학습하는 동시에 그 단어를 구성하는 모든 subword들의 벡터까지 함께 학습한다는 점이다.\n",
        "\n",
        "한 단어의 최종 벡터는, 그 단어를 구성하는 모든 subword 벡터들의 총합으로 계산된다.\n",
        "\n",
        "이러한 접근 방식은 두 가지 강력한 장점을 낳는다.\n",
        "\n",
        "* 신조어 및 희귀 단어 처리: 훈련 데이터에 없던 단어(예: applesauce)가 등장하더라도, FastText는 그 단어를 구성하는 subword(app, ppl, le, sau, uce 등)들의 벡터를 이미 알고 있다. 따라서 이 subword 벡터들을 합하여 처음 보는 단어의 의미 벡터를 추정해낼 수 있다.\n",
        "\n",
        "* 형태소(Morpheme) 정보의 활용: '먹다', '먹으니', '먹어서'와 같은 단어들은 '먹-'이라는 공통된 subword를 공유한다. 따라서 이들의 벡터는 자연스럽게 의미 공간에서 가까운 위치에 자리 잡게 된다. FastText는 단어의 내부 구조를 학습에 반영하는 것이다.\n",
        "\n",
        "##**3.등장 배경**\n",
        "\n",
        "* 누가/언제: FastText는 2016년, 페이스북 AI 연구소(Facebook AI Research, FAIR) 팀에 의해 개발되었다. 이 팀에는 Word2Vec의 창시자인 토마스 미콜로프도 포함되어 있었다.\n",
        "\n",
        "* 왜: Word2Vec의 OOV 문제와 형태론적 한계를 극복하기 위해 개발되었다. 특히 한국어, 터키어, 독일어처럼 어미 변화나 단어 합성이 빈번하게 일어나는 **형태론적으로 풍부한 언어(morphologically rich languages)**를 더 잘 처리하기 위한 목적이 컸다."
      ],
      "metadata": {
        "id": "mvRRJ3TDjL8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**4. 디지털 인문학에서의 활용**\n",
        "FastText의 특징은 고문헌을 다루는 디지털 인문학 연구자에게 매우 강력한 이점을 제공한다.\n",
        "\n",
        "* 희귀 단어 및 고유명사 처리: 고문헌에는 현대에는 쓰이지 않는 수많은 희귀 어휘나 고유명사가 등장한다. Word2Vec은 이들을 학습 데이터 부족으로 무시할 수 있지만, FastText는 subword 정보를 통해 의미 있는 벡터를 생성하여 분석에 포함시킬 수 있다.\n",
        "\n",
        "* 오탈자 및 OCR 오류에 대한 강건함(Robustness): OCR(광학 문자 인식)로 디지털화된 텍스트에는 오탈자가 많다. 예를 들어, 訓民正音이 訓民正입으로 잘못 인식되었을 때, Word2Vec은 이 둘을 완전히 다른 단어로 취급한다. 하지만 FastText는 訓民正이라는 subword를 공유하기 때문에 두 단어가 매우 유사하다고 판단할 수 있다. 이는 노이즈가 많은 원문 자료 분석의 정확도를 크게 향상시킨다.\n",
        "\n",
        "##**5. 불교 텍스트(한문) 분석에서의 활용**\n",
        "한자(漢字)의 조어(造語) 능력 반영: 한문은 佛, 國, 寺와 같은 개별 한자가 모여 佛國寺라는 새로운 의미의 단어를 만드는 조어적 특성이 강하다. FastText의 subword 접근법은 이러한 한자의 특성과 잘 부합한다. 모델은 佛의 의미와 國의 의미를 모두 학습하여, 이들이 결합된 佛國의 의미 벡터를 더 정교하게 표현할 수 있다.\n",
        "\n",
        "* 이두(吏讀) 및 비표준 한자 처리: 향찰, 구결, 이두 등 한자의 음과 뜻을 빌려 우리말을 표기했던 비표준적인 용례나, 문헌에만 등장하는 희귀 한자를 분석할 때, FastText는 subword 정보를 통해 그 의미를 유추할 수 있는 가능성을 제공한다.\n",
        "\n",
        "* 판본(版本) 비교 및 이체자(異體字) 분석: 동일한 경전의 다른 판본에는 종종 형태가 약간 다른 이체자(예: 國 vs 囯)나 필사 오류가 나타난다. FastText는 이들을 완전히 다른 단어로 보지 않고, 형태적 유사성을 기반으로 비슷한 벡터를 생성하므로 판본 간의 내용을 비교 분석할 때 더 안정적인 결과를 도출한다."
      ],
      "metadata": {
        "id": "7H-tFf-AjrjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# numpy, scipy를 포함한 관련 라이브러리를 강제 업그레이드합니다.\n",
        "!pip install --upgrade numpy scipy gensim -q\n",
        "\n",
        "print(\"✅ 라이브러리 설치 및 업그레이드 완료!\")\n",
        "print(\"🔴 중요: 이제 메뉴에서 [런타임] > [런타임 다시 시작]을 실행하여 변경사항을 적용해주세요.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o50ErkcbkPFD",
        "outputId": "1469bf62-8fbe-428c-cf84-03a8ba3acdfd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h✅ 라이브러리 설치 및 업그레이드 완료!\n",
            "🔴 중요: 이제 메뉴에서 [런타임] > [런타임 다시 시작]을 실행하여 변경사항을 적용해주세요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnhuDkmOiCdm",
        "outputId": "a4b91103-e13b-453b-d212-117fcebe7e02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ FastText 모델 훈련 완료.\n",
            "\n",
            "--- 🚀 FastText 모델 탐색 시작 ---\n",
            "\n",
            "[질문 1] '왕은'과 의미적으로 가장 유사한 단어는?\n",
            "▶️ 결과: [('사랑했다', 0.25949525833129883), ('여왕은', 0.13111019134521484), ('신뢰했다', 0.10132020711898804), ('백성에게', 0.03607366606593132), ('여왕이', 0.028564922511577606), ('명을', -0.05006198585033417), ('신하에게', -0.06469079852104187), ('내렸다', -0.07000070065259933), ('백성을', -0.09878890961408615), ('왕이', -0.10186902433633804)]\n",
            "\n",
            "[질문 2] 어휘 사전에 없는 단어 '왕께서'와 유사한 단어는?\n",
            "▶️ 결과: [('백성에게', 0.2519212067127228), ('신하를', 0.15065337717533112), ('신뢰했다', 0.13707150518894196), ('왕은', 0.06956880539655685), ('명을', 0.06899220496416092), ('왕이', 0.0647834986448288), ('여왕이', -0.004489712882786989), ('백성을', -0.022257423028349876), ('사랑했다', -0.027204785495996475), ('신하에게', -0.03953119367361069)]\n"
          ]
        }
      ],
      "source": [
        "# 1. 필요 라이브러리 설치\n",
        "!pip install --upgrade gensim -q\n",
        "# (런타임 다시 시작이 필요할 수 있음)\n",
        "\n",
        "# 2. 라이브러리 불러오기\n",
        "from gensim.models import FastText\n",
        "\n",
        "# 3. 실습용 데이터 준비\n",
        "# '왕'의 여러 형태가 등장하는 코퍼스\n",
        "tokenized_corpus = [\n",
        "    ['왕은', '신하에게', '명을', '내렸다'],\n",
        "    ['여왕은', '백성에게', '명을', '내렸다'],\n",
        "    ['왕이', '백성을', '사랑했다'],\n",
        "    ['여왕이', '신하를', '신뢰했다']\n",
        "]\n",
        "\n",
        "# 4. FastText 모델 훈련\n",
        "model = FastText(sentences=tokenized_corpus, sg=1, vector_size=100, window=3, min_count=1, seed=42)\n",
        "print(\"✅ FastText 모델 훈련 완료.\")\n",
        "\n",
        "\n",
        "# 5. 결과 탐색\n",
        "print(\"\\n--- 🚀 FastText 모델 탐색 시작 ---\")\n",
        "\n",
        "# 질문 1: 학습된 단어('왕은')와 유사한 단어 찾기\n",
        "print(\"\\n[질문 1] '왕은'과 의미적으로 가장 유사한 단어는?\")\n",
        "# '왕이', '여왕은', '여왕이' 등이 나올 것으로 기대\n",
        "similar_words = model.wv.most_similar('왕은')\n",
        "print(\"▶️ 결과:\", similar_words)\n",
        "\n",
        "# 질문 2 (핵심): 학습 데이터에 없던 단어('왕께서')의 유사 단어 찾기\n",
        "# Word2Vec이었다면 이 코드에서 오류가 발생한다.\n",
        "print(\"\\n[질문 2] 어휘 사전에 없는 단어 '왕께서'와 유사한 단어는?\")\n",
        "try:\n",
        "    oov_similar_words = model.wv.most_similar('왕께서')\n",
        "    print(\"▶️ 결과:\", oov_similar_words)\n",
        "except KeyError:\n",
        "    print(\"▶️ 결과: Word2Vec이었다면 여기서 오류가 발생했을 것입니다.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📊 결과 분석: FastText는 어떻게 단어의 의미를 추측할까?"
      ],
      "metadata": {
        "id": "bX5X4qPNkikV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[질문 1] '왕은'과 유사한 단어는?\n",
        "\n",
        "▶️ 결과: [('사랑했다', 0.259...), ('여왕은', 0.131...), ...]\n",
        "\n",
        "* 결과 해석:\n",
        "\n",
        "가장 먼저 주목할 점은, 모델이 우리가 기대했던 '여왕은'을 두 번째로 높은 순위에 정확히 찾아냈다는 것이다. 이는 FastText가 '왕은'과 '여왕은'이 문장 속에서 비슷한 역할(주어)을 한다는 문맥적 유사성을 성공적으로 학습했음을 보여준다.\n",
        "\n",
        "\"그런데 왜 '사랑했다'가 1위일까?\"\n",
        "\n",
        "이것은 데이터가 매우 적기 때문에 나타나는 현상이다. 컴퓨터는 '왕은 백성을 사랑했다'라는 문장을 통해, '왕은'이라는 단어가 '사랑했다'와 매우 가까운 위치에 함께 등장했다는 사실을 강하게 학습했다. 데이터가 수천, 수만 개로 늘어난다면 '왕은'과 '여왕은'이 공유하는 더 다양한 문맥이 많아져 순위가 바뀌겠지만, 이렇게 작은 데이터에서는 바로 옆에 있었던 단어의 영향력이 더 크게 나타날 수 있다."
      ],
      "metadata": {
        "id": "J6JGoyMlmFIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###[질문 2] 어휘 사전에 없는 단어 '왕께서'와 유사한 단어는? (FastText의 진정한 힘 ✨)\n",
        "\n",
        "▶️ 결과: [('백성에게', 0.251...), ('신하를', 0.150...), ...]\n",
        "\n",
        "* 결과 해석 (가장 중요한 부분):\n",
        "\n",
        "이 결과는 FastText의 가장 강력한 장점을 보여준다. '왕께서'라는 단어는 우리가 학습시킨 4개의 문장 어디에도 존재하지 않는다. 만약 Word2Vec이었다면, 이 질문에 대해 \"사전에 없는 단어입니다\"라며 오류를 발생시켰을 것이다.\n",
        "\n",
        "\"FastText는 어떻게 처음 보는 단어의 의미를 추측했을까?\"\n",
        "\n",
        "FastText는 '왕께서'라는 단어를 통째로 보지 않고, 더 작은 문자 조각(subword)으로 분해해서 생각한다.\n",
        "\n",
        "* 분해: 컴퓨터는 '왕께서'를 <왕, 왕께, 께서, 서> 와 같은 여러 조각으로 나눈다.\n",
        "\n",
        "* 지식 활용: FastText는 이미 '왕은'과 '왕이'를 학습하면서 '왕'이라는 조각의 의미 벡터를 알고 있다. 또한 '께서'라는 조각의 벡터도 학습한다.\n",
        "\n",
        "* 조합 및 추정: 이제 컴퓨터는 자신이 알고 있는 '왕' 조각의 의미와 '께서' 조각의 의미를 조합하여, 처음 보는 단어인 '왕께서'의 전체 의미를 새롭게 추정해낸다.\n",
        "\n",
        "이렇게 만들어진 '왕께서'의 추정된 의미는, 문장 속에서 비슷한 역할(주체)을 나타내는 다른 단어들, 즉 목적어와 함께 쓰이는 '백성에게', '신하를' 등과 가깝다고 판단된 것이다."
      ],
      "metadata": {
        "id": "GmatCmcnmTJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**✨ 최종 결론 및 인문학적 의의**\n",
        "이 실습은 우리에게 두 가지 중요한 사실을 알려준다.\n",
        "\n",
        "FastText는 '조립식'으로 의미를 이해한다: 단어를 통째로 외우는 Word2Vec과 달리, FastText는 단어의 내부 구조(subword)를 학습하여 처음 보는 단어나 오탈자가 등장해도 그 의미를 유추해내는 뛰어난 유연성을 가진다.\n",
        "\n",
        "한문/고문헌 분석의 강력한 도구: 왕(王), 왕이(王亦), 왕께서처럼 형태가 조금씩 다른 단어가 많이 등장하거나, OCR 오류가 있는 고문헌을 분석할 때, FastText는 이들을 모두 의미적으로 연결하여 분석할 수 있는 매우 강력하고 현실적인 도구가 된다."
      ],
      "metadata": {
        "id": "hDpTQOGImquY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FastText 적용에 적합한 텍스트 유형\n",
        "\n",
        "FastText의 강점은 어휘 사전에 없는 단어(Out-of-Vocabulary, OOV)와 형태적 변형에 매우 강건하다는 점이다. 따라서 다음과 같은 특징을 가진 텍스트에 적용할 때 그 진가가 드러난다.\n",
        "\n",
        "###1. OCR로 디지털화된 근대 불교 자료 (잡지, 신문 등)\n",
        "대상 텍스트: 20세기 초반에 발간된 불교 잡지나 신문 등, 현대적인 인쇄물을 OCR(광학 문자 인식) 기술로 변환한 텍스트.\n",
        "\n",
        "* 적용 이유: OCR 과정에서는 글자가 깨지거나 비슷한 모양의 다른 글자로 잘못 인식되는 경우(예: 佛 → 仏)가 빈번하다. Word2Vec은 佛과 仏을 완전히 다른 단어로 인식하지만, FastText는 두 글자의 형태(subword)가 매우 유사하다고 판단하여 의미적으로 가까운 벡터를 생성한다.\n",
        "\n",
        "* 알아낼 수 있는 것: OCR 오류에 강건한 의미 분석. 텍스트에 노이즈가 많더라도, 핵심 단어들의 의미 관계를 더 안정적으로 추출하여 근대 불교 담론의 핵심 주제나 키워드를 파악할 수 있다.\n",
        "\n",
        "###2. 판본이 다양하거나 이체자(異體字)가 많은 경전\n",
        "대상 텍스트: 『법화경』, 『화엄경』처럼 시대와 지역에 따라 다양한 판본(예: 고려대장경본, 송본, 명본)이 존재하는 경전.\n",
        "\n",
        "* 적용 이유: 다른 판본들은 동일한 단어를 다른 글자(이체자, 예: 峯 vs. 峰)로 표기하거나 필사 과정에서 약간의 차이를 보이는 경우가 많다. FastText는 이러한 형태적 차이가 있는 단어들도 subword의 유사성을 통해 의미적으로 가깝다고 판단할 수 있다.\n",
        "\n",
        "* 알아낼 수 있는 것: 판본 간의 의미적 일관성 및 차이 측정. 두 판본의 텍스트로 각각 FastText 모델을 학습시킨 뒤, 특정 핵심 단어(예: 心)의 벡터 값이나 주변 단어들을 비교할 수 있다. 이를 통해 두 판본이 특정 개념을 얼마나 일관되게, 혹은 다르게 사용하고 있는지 계량적으로 분석하는 정교한 문헌 비교 연구가 가능하다.\n",
        "\n",
        "###3. 진언(眞言)이나 음차(音譯) 단어가 많은 밀교(密敎) 계통 경전\n",
        "대상 텍스트: 『대일경(大日經)』, 『금강정경(金剛頂經)』 등 다라니(陀羅尼)나 산스크리트어 음차어가 많이 등장하는 텍스트.\n",
        "\n",
        "* 적용 이유: 옴 마니 반메 훔과 같은 진언이나, 아뇩다라삼먁삼보리와 같은 긴 음차어들은 일반적인 한문 텍스트에서는 거의 등장하지 않는 희귀 단어(rare words)이다. Word2Vec은 학습 데이터가 부족하여 이들의 의미를 제대로 학습하기 어렵다.\n",
        "\n",
        "* 알아낼 수 있는 것: 희귀 단어의 의미 관계 추정. FastText는 이러한 희귀 단어들의 subword 정보를 통해, 비록 완전하진 않더라도 그 단어의 벡터를 추정해낸다. 이를 통해 특정 진언이 어떤 다른 개념어들과 함께 등장하는 경향이 있는지, 즉 그 진언의 문맥적 기능이 무엇인지 탐색하는 새로운 연구의 실마리를 제공할 수 있다.\n",
        "\n",
        "###4. 이두(吏讀), 구결(口訣) 등이 혼용된 향찰 문헌\n",
        "대상 텍스트: 원효의 저술이나 향가 등, 한자의 음과 뜻을 빌려 우리말을 표기한 방식이 나타나는 문헌.\n",
        "\n",
        "* 적용 이유: 이두나 향찰에서 사용된 한자는 표준적인 한문 용법을 따르지 않기 때문에, 대부분의 단어가 OOV로 처리될 수 있다.\n",
        "\n",
        "* 알아낼 수 있는 것: 비표준적 용례의 의미 복원. FastText는 夜(밤 야)라는 글자가 의미(밤)로 쓰일 때와 음(-이시여)으로 쓰일 때의 subword 패턴 차이를 학습할 수 있다. 이를 통해 비표준적으로 사용된 단어의 벡터를 생성하고, 그것이 어떤 의미의 단어들과 유사한지 탐색함으로써 향찰 해독 연구에 새로운 계량적 단서를 제공할 가능성이 있다.\n",
        "\n",
        "## 결론\n",
        "FastText는 완벽하게 정제된 표준 텍스트보다, **오탈자, 이체자, 희귀어, 비표준적 용례가 빈번하게 나타나는 '현실 세계의 고문헌'**을 분석할 때 그 진정한 가치를 발휘한다. 이는 연구자가 텍스트의 불완전성을 극복하고 더 깊은 의미 구조에 접근하도록 돕는 강력한 디지털 문헌학(Digital Philology) 도구이다."
      ],
      "metadata": {
        "id": "gpvC5uHLnVNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###📜 FastText 강점 시연을 위한 비교 분석 코드"
      ],
      "metadata": {
        "id": "u4dldWw1nwBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 필요 라이브러리 설치 및 업그레이드\n",
        "!pip install --upgrade numpy scipy gensim jieba opencc-python-reimplemented -q\n",
        "\n",
        "print(\"✅ 라이브러리 설치 및 업그레이드 완료!\")\n",
        "# [알림] 위 셀 실행 후 버전 충돌 오류가 발생하면, 메뉴에서 [런타임] > [런타임 다시 시작]을 한 번 실행한 뒤 아래 셀을 실행하세요."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0q2y1Te3nujg",
        "outputId": "e3c9dae9-4003-4581-a624-9cd3acc5b196"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/481.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/481.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.8/481.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h✅ 라이브러리 설치 및 업그레이드 완료!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 라이브러리 불러오기 ---\n",
        "import jieba\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from opencc import OpenCC\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. 데이터 준비: 깨끗한 원문 vs. 손상된 원문 ---\n",
        "# 『반야심경』의 일부 구절을 사용\n",
        "doc_clean = \"觀自在菩薩行深般若波羅蜜多時照見五蘊皆空度一切苦厄\"\n",
        "\n",
        "# '觀'->'見', '蜜'->'密' 로 일부러 손상된 텍스트를 생성 (OCR 오류 등 가정)\n",
        "doc_corrupted = \"見自在菩薩行深般若波羅密多時照見五蘊皆空度一切苦厄\"\n",
        "\n",
        "corpus_raw = [doc_clean, doc_corrupted]\n",
        "print(\"--- 원본 데이터 ---\")\n",
        "print(f\"깨끗한 원문: {doc_clean}\")\n",
        "print(f\"손상된 원문: {doc_corrupted}\")\n",
        "\n",
        "\n",
        "# --- 2. 텍스트 전처리 및 토큰화 ---\n",
        "cc = OpenCC('t2s')\n",
        "corpus_simplified = [cc.convert(doc) for doc in corpus_raw]\n",
        "\n",
        "important_words = ['观自在菩萨', '般若波罗蜜多', '五蕴', '空']\n",
        "for word in important_words:\n",
        "    jieba.add_word(word)\n",
        "\n",
        "tokenized_corpus = [jieba.lcut(doc) for doc in corpus_simplified]\n",
        "print(\"\\n✅ 토큰화 완료!\")\n",
        "\n",
        "\n",
        "# --- 3. 두 종류의 모델 훈련 ---\n",
        "# 동일한 데이터로 Word2Vec과 FastText 모델을 각각 훈련\n",
        "model_w2v = Word2Vec(sentences=tokenized_corpus, sg=1, vector_size=100, window=2, min_count=1, seed=42)\n",
        "model_ft = FastText(sentences=tokenized_corpus, sg=1, vector_size=100, window=2, min_count=1, seed=42)\n",
        "print(\"✅ Word2Vec 및 FastText 모델 훈련 완료!\")\n",
        "\n",
        "\n",
        "# --- 4. 비교 분석 결과 탐색 ---\n",
        "print(\"\\n--- 🚀 모델 성능 비교 분석 ---\")\n",
        "\n",
        "# 분석 대상 단어 (간체자로 변환)\n",
        "word_clean = cc.convert(\"觀自在菩薩\")\n",
        "word_corrupted = cc.convert(\"見自在菩薩\")\n",
        "word_oov = cc.convert(\"觀音菩薩\") # 훈련 데이터에 없는 단어\n",
        "\n",
        "# [비교 1] 손상된 단어의 벡터 유사도 비교\n",
        "try:\n",
        "    # 두 단어의 벡터를 가져옴\n",
        "    vec_clean_ft = model_ft.wv[word_clean]\n",
        "    vec_corrupted_ft = model_ft.wv[word_corrupted]\n",
        "\n",
        "    # 코사인 유사도 계산\n",
        "    similarity_ft = np.dot(vec_clean_ft, vec_corrupted_ft) / (np.linalg.norm(vec_clean_ft) * np.linalg.norm(vec_corrupted_ft))\n",
        "    print(f\"\\n[FastText] '{word_clean}' vs '{word_corrupted}' 유사도: {similarity_ft:.4f}\")\n",
        "\n",
        "except KeyError:\n",
        "    print(f\"\\n[FastText] 어휘 사전에 '{word_clean}' 또는 '{word_corrupted}'가 없습니다.\")\n",
        "\n",
        "try:\n",
        "    # Word2Vec은 손상된 단어를 모르므로 오류 발생 예상\n",
        "    similarity_w2v = model_w2v.wv.similarity(word_clean, word_corrupted)\n",
        "    print(f\"[Word2Vec] '{word_clean}' vs '{word_corrupted}' 유사도: {similarity_w2v:.4f}\")\n",
        "except KeyError:\n",
        "    print(f\"[Word2Vec] '{word_corrupted}'는 어휘 사전에 없는 단어라 유사도 계산이 불가능합니다.\")\n",
        "\n",
        "\n",
        "# [비교 2] OOV(Out-of-Vocabulary) 단어 처리 능력 비교\n",
        "print(f\"\\n--- [비교 2] 훈련 데이터에 없는 '{word_oov}' 처리 능력 ---\")\n",
        "\n",
        "print(f\"\\n[FastText] '{word_oov}'와 가장 유사한 단어는?\")\n",
        "try:\n",
        "    oov_similar_words_ft = model_ft.wv.most_similar(word_oov)\n",
        "    print(\"▶️ 결과:\", oov_similar_words_ft)\n",
        "except KeyError:\n",
        "     print(\"▶️ 결과: 오류 발생\")\n",
        "\n",
        "print(f\"\\n[Word2Vec] '{word_oov}'와 가장 유사한 단어는?\")\n",
        "try:\n",
        "    oov_similar_words_w2v = model_w2v.wv.most_similar(word_oov)\n",
        "    print(\"▶️ 결과:\", oov_similar_words_w2v)\n",
        "except KeyError:\n",
        "    print(\"▶️ 결과: Word2Vec은 어휘 사전에 없는 단어라 의미를 추론할 수 없습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MBQjHSYn2ZI",
        "outputId": "86ad0f31-0d7a-4866-be5a-ce39fdf8e9fd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 원본 데이터 ---\n",
            "깨끗한 원문: 觀自在菩薩行深般若波羅蜜多時照見五蘊皆空度一切苦厄\n",
            "손상된 원문: 見自在菩薩行深般若波羅密多時照見五蘊皆空度一切苦厄\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 1.244 seconds.\n",
            "DEBUG:jieba:Loading model cost 1.244 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ 토큰화 완료!\n",
            "✅ Word2Vec 및 FastText 모델 훈련 완료!\n",
            "\n",
            "--- 🚀 모델 성능 비교 분석 ---\n",
            "\n",
            "[FastText] '观自在菩萨' vs '见自在菩萨' 유사도: 0.4624\n",
            "[Word2Vec] '见自在菩萨'는 어휘 사전에 없는 단어라 유사도 계산이 불가능합니다.\n",
            "\n",
            "--- [비교 2] 훈련 데이터에 없는 '观音菩萨' 처리 능력 ---\n",
            "\n",
            "[FastText] '观音菩萨'와 가장 유사한 단어는?\n",
            "▶️ 결과: [('菩萨', 0.3275032937526703), ('时', 0.2021409571170807), ('厄', 0.19780491292476654), ('照见', 0.1731913983821869), ('五蕴皆空', 0.125163733959198), ('苦', 0.1225053146481514), ('观自在菩萨', 0.09614233672618866), ('般若波罗蜜多', 0.02636975422501564), ('自', 0.022403616458177567), ('见', -0.0013889598194509745)]\n",
            "\n",
            "[Word2Vec] '观音菩萨'와 가장 유사한 단어는?\n",
            "▶️ 결과: Word2Vec은 어휘 사전에 없는 단어라 의미를 추론할 수 없습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###결과 분석\n",
        "\n",
        "1. 손상된 단어의 유사도: [FastText] '观自在菩萨' vs '见自在菩萨' 유사도: 0.4624\n",
        "\n",
        "* 해석: FastText는 观과 见이라는 한 글자 차이에도 불구하고, 두 단어가 나머지 自在菩萨라는 긴 subword(내부 단어)를 공유하고 있기 때문에, 이 둘이 의미적으로 꽤 관련이 있다고 판단했다. 0.4624라는 점수는 \"완전히 똑같지는 않지만, 상당히 유사하다\"고 모델이 추론했음을 보여준다. 반면, Word2Vec은 见自在菩萨를 완전히 모르는 단어로 취급하여 비교 자체를 거부했다.\n",
        "\n",
        "* 인문학적 의미: 이는 FastText가 판본 이체자(異體字)나 OCR 오류에 훨씬 더 강건하게 텍스트의 의미적 유사성을 파악할 수 있음을 증명한다. 연구자는 사소한 글자 차이에 구애받지 않고, 여러 판본 간의 핵심적인 의미 흐름을 비교 분석할 수 있다."
      ],
      "metadata": {
        "id": "tClPH6B7oAMh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. OOV(사전에 없는 단어) 처리 능력: [FastText] '观音菩萨'와 가장 유사한 단어는?\n",
        "해석: 이 결과는 FastText의 진정한 강점을 보여준다.\n",
        "\n",
        "* 성공적인 추론: Word2Vec과 달리, FastText는 훈련 데이터에 없던 '观音菩萨'의 벡터를 성공적으로 생성했다. 이는 '观'과 '菩萨'라는 subword 벡터들을 조합하여 만들어낸 결과이다.\n",
        "\n",
        "* 문맥적 연결: 추론된 '观音菩萨'의 벡터와 가장 가깝다고 찾아낸 단어들은 '菩萨', '照见'(비추어보다), '五蕴皆空'(오온이 모두 공하다) 등이다. 이는 FastText가 '观音菩萨'를 원문의 핵심인 '观自在菩萨'와 유사한, **'지혜로 세상을 관찰하여 깨달음을 얻는 존재'**라는 문맥 속에 정확히 배치했음을 의미한다.\n",
        "\n",
        "* 인문학적 의미: 연구자는 훈련 코퍼스에 등장하지 않는 희귀한 인명, 지명, 개념어가 등장하더라도, FastText를 통해 그 단어의 문맥적 의미를 추정하고 다른 개념들과의 관계를 탐색할 수 있다. 이는 미지의 단어로 가득한 고문헌 연구에서 새로운 발견의 실마리를 제공하는 강력한 도구가 된다."
      ],
      "metadata": {
        "id": "Pnuaa-rcoTpm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###실습해보자!"
      ],
      "metadata": {
        "id": "kdUH_G14rVUD"
      }
    }
  ]
}