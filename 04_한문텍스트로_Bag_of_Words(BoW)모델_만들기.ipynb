{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Song-yiJung/DH-Buddhist-Analysis-Tutorial/blob/main/4_%ED%95%9C%EB%AC%B8%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%A1%9C_Bag_of_Words(BoW)%EB%AA%A8%EB%8D%B8_%EB%A7%8C%EB%93%A4%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 한문 텍스트로 Bag-of-Words(BoW) 모델👜 만들기\n",
        "\n",
        "## **Bag-of-Words(BoW): '단어 가방 모델', 컴퓨터가 글을 이해하도록 돕는 간단하고 효과적인 방법**\n",
        "\n",
        "* 문장이나 문서에 어떤 단어가 몇 번 등장했는지만을 여기는 텍스트 표현 방법.\n",
        "\n",
        "* 왜 단어 가방 모델일까? 단어들의 순서는 전혀 고려하지 않고, 모든 단어를 하나의 **가방**에 넣은 뒤 각 단어의 출현 빈도(frequency)를 세는 방식이기 때문이다.\n",
        "\n",
        "## 핵심원리\n",
        "\n",
        "* 어휘 사전 생성 (Vocabulary Building): 가지고 있는 모든 문서의 모든 단어를 중복 없이 모아 하나의 '어휘 사전'을 만든다.\n",
        "* 벡터화 (Vectorization): 각 문서를 어휘 사전에 있는 단어들의 등장 횟수를 기준으로 숫자 벡터(Vector)로 변환한다.\n"
      ],
      "metadata": {
        "id": "Snxi52-0J5mJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [실습] 컴퓨터는 어떻게 글자를 숫자로 바꾸나?"
      ],
      "metadata": {
        "id": "W1IYj96Va4Dp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku9bWnYIJP7k"
      },
      "outputs": [],
      "source": [
        "# 1. 도구 준비하기\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "도구 준비하기\n",
        "* 짧은 한글 문장을 숫자의 배열(벡터)로 바꾸는 과정.\n",
        "* 데이터 분석 도구 상자인 sklearn에서 CountVectorizer(단어 계수기:문장 속 단어의 개수를 세어주는 전문 도구) 도구  불러오기(import )    "
      ],
      "metadata": {
        "id": "tjbSTWCYbE1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 분석할 문장 준비하기\n",
        "documents = [\n",
        "    \"나는 오늘 사과를 먹었다\",\n",
        "    \"나는 오늘 축구를 했다\"\n",
        "]"
      ],
      "metadata": {
        "id": "nSuTaB7_X-CI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 단어 계수기 작동 준비\n",
        "vectorizer = CountVectorizer()"
      ],
      "metadata": {
        "id": "kH5-HzMKdpHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**단어 계수기** 작동 준비\n",
        "\n",
        "* 위에서 가져온 CountVectorizer라는 '단어 계수기' 도구를 실제로 사용할 수 있도록 vectorizer라는 이름으로 하나 복사해서 준비시키는 과정\n",
        "\n",
        "* 이제 vectorizer를 통해 단어를 세고 문장을 숫자로 바꿀 수 있다."
      ],
      "metadata": {
        "id": "y-klQGv5drSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 학습과 변환\n",
        "bow_matrix = vectorizer.fit_transform(documents)"
      ],
      "metadata": {
        "id": "iTLhqEmJd7qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습과 변환\n",
        "* 학습 (fit): vectorizer가 우리가 준 documents의 모든 문장을 훑어보면서 어떤 종류의 단어들이 있는지 목록을 만든다. 이걸 '어휘 사전(Vocabulary)'이라고 부른다.\n",
        "* 변환 (transform): 위에서 만든 '어휘 사전'을 기준으로, 각 문장에 어떤 단어가 몇 번씩 들어있는지 숫자를 센다. 그리고 그 결과를 바탕으로 각 문장을 숫자 배열(벡터)로 변환한다."
      ],
      "metadata": {
        "id": "mcSNqNBreCIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 결과 확인하기\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "print(\"1단계: 생성된 어휘 사전\")\n",
        "print(vocabulary)"
      ],
      "metadata": {
        "id": "B3bDuWFNeNXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "출력: ['나는' '먹었다' '사과를' '오늘' '축구를' '했다']\n",
        "\n",
        "\"네가 만든 '어휘 사전' 좀 보여줘\" 라고 vectorizer에게 요청하는 코드입니다. 그 결과, documents에서 찾아낸 6개의 고유한 단어 목록을 보여줍니다. 이 목록의 순서가 매우 중요합니다. 왜냐하면 이 순서가 바로 아래에서 볼 숫자 배열의 각 자리와 일치하기 때문이다."
      ],
      "metadata": {
        "id": "Hfj-jlLMeQpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n----------------------------------------\\n\")\n",
        "print(\"2단계: 변환된 벡터\")\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "id": "BtVntlFQeTdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"숫자로 변환한 최종 결과를 보기 쉽게 표(행렬) 형태로 보여줘\" 라는 코드\n",
        "\n",
        "---\n",
        "### 코드 해설\n",
        "\n",
        "bow_matrix\n",
        "\n",
        "컴퓨터는 단어 개수를 센 결과를 메모리 절약을 위해 효율적인 방식(희소 행렬, sparse matrix)으로 저장하고 있다. 이 bow_matrix 변수 안에 그 결과가 들어있다.\n",
        "\n",
        ".toarray()\n",
        "\n",
        "이것이 핵심이다. 컴퓨터의 압축 파일 같은 bow_matrix를 \"사람이 보기 편한 네모난 표(배열, array)로 활짝 펼쳐줘!\" 라고 지시하는 명령어이다.\n",
        "\n",
        "print(...)\n",
        "\n",
        "이렇게 표 형태로 펼쳐진 결과를 화면에 보여준다.\n",
        "\n",
        "결과 해석\n",
        "\n",
        "결과로 나온 숫자 표(행렬)는 다음과 같은 규칙을 가진다.\n",
        "\n",
        "가로 한 줄 (행): 우리가 처음에 넣었던 문장 하나를 의미한다.\n",
        "\n",
        "세로 한 칸 (열): 앞에서 만든 '어휘 사전'의 단어 하나를 의미한다\n",
        ".\n",
        "숫자: 해당 줄의 문장에 해당 칸의 단어가 몇 번 등장했는지를 나타낸다.\n",
        "\n",
        "\n",
        "결국 이 코드는 컴퓨터 내부의 복잡한 데이터 구조를 우리가 엑셀 시트처럼 한눈에 파악할 수 있는 간단한 표로 변환하여 보여주는 과정이다.\n"
      ],
      "metadata": {
        "id": "z_KNS8R7eVon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "사람이 쓰는 '문장'을 단어의 등장 횟수를 기준으로 컴퓨터가 이해할 수 있는 '숫자 신호'로 깔끔하게 번역해주는 과정이다."
      ],
      "metadata": {
        "id": "CT9GOH6SeYuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📚 Bag-of-Words(BoW) 모델의 기원: 누가, 언제, 왜?\n",
        "\n",
        "BoW는 특정 한 명이 \"발명\"한 모델이라기보다는, 특정 문제를 해결하기 위해 자연스럽게 발전한 개념에 가깝다.\n",
        "\n",
        "* 누가/언제 (Who/When): BoW의 근본적인 아이디어는 1950년대부터 정보 검색(Information Retrieval) 분야에서 등장하기 시작했다. 컴퓨터 과학의 초기 연구자들이 어떻게 하면 방대한 문서 더미에서 사용자가 원하는 정보를 효율적으로 찾아낼 수 있을지를 고민하던 시기이다. 이 개념은 1960년대 코넬 대학교의 제라드 솔튼(Gerard Salton) 교수가 개발한 'SMART 정보 검색 시스템'과 같은 초기 검색 엔진 연구를 통해 구체화되고 널리 알려졌다.\n",
        "\n",
        "* 왜 (Why): 개발 목적은 매우 명확했습니다. 바로 **'컴퓨터를 이용한 문서 검색과 분류'**를 위해서였다.\n",
        "\n",
        "* 문제: 컴퓨터는 '사과'와 '축구'라는 단어의 의미 차이를 모른다. 따라서 \"사과에 대한 문서를 찾아줘\"라는 명령을 그대로 이해할 수 없다.\n",
        "\n",
        "* 해결책 (BoW): 만약 모든 문서를 단어 빈도수 벡터로 바꿔놓는다면 어떨까? 사용자의 검색어(\"사과\") 또한 벡터 [... '사과':1 ...]로 만들 수 있다. 그러면 컴퓨터는 복잡한 의미를 이해할 필요 없이, 단순히 수학적 계산(벡터 간의 유사도 측정)을 통해 검색어 벡터와 가장 '가까운' 문서 벡터들을 찾아내면 된다.\n",
        "\n",
        "이 간단하면서도 강력한 아이디어는 초기 검색 엔진, 스팸 메일 필터링, 문서 자동 분류 등 다양한 분야의 기술적 토대가 되었다.\n"
      ],
      "metadata": {
        "id": "iW13n7YffhuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🍎 예시: 작은 도서관에서 '사과'에 대한 책 찾기 🔍\n",
        "\n",
        "여기에 3권의 책만 있는 아주 작은 도서관이 있고, 컴퓨터 사서에게 **\"사과\"**에 대한 책을 찾아달라고 요청하는 상황을 가정해 보겠다.\n",
        "\n",
        "📖 우리의 작은 도서관 (문서 3개)\n",
        "\n",
        "* 📄 문서 1: 사과 주스 그리고 사과 파이\n",
        "\n",
        "* 📄 문서 2: 나는 축구 그리고 야구 좋아해\n",
        "\n",
        "* 📄 문서 3: 나는 사과 그리고 바나나 좋아해\n",
        "\n",
        "🤔 **[문제 상황]**: 컴퓨터는 '의미'를 모른다\n",
        "컴퓨터 사서 🤖는 '사과'가 과일인지, '축구'가 스포츠인지 전혀 모른다. 그저 글자의 나열로만 볼 뿐이다. 따라서 어떤 책이 '사과'에 대한 책인지 판단할 수 없다.\n",
        "\n",
        "💡 **[해결책]**:\n",
        "\n",
        "BoW 모델의 적용 과정\n",
        "\n",
        "1️⃣ 단계: 모든 단어를 모아 '어휘 사전' 만들기\n",
        "도서관에 있는 모든 책(문서)의 모든 단어를 중복 없이 모아 번호를 매긴다. 이것이 '어휘 사전'이 된다.\n",
        "\n",
        "📝 어휘 사전: { 1: 사과, 2: 주스, 3: 그리고, 4: 파이, 5: 나는, 6: 축구, 7: 야구, 8: 좋아해, 9: 바나나 }\n",
        "\n",
        "2️⃣ 단계: 모든 것을 '숫자 벡터'로 변환하기 🔢\n",
        "이제 어휘 사전을 기준으로, 우리의 검색어와 모든 문서를 숫자 배열(벡터)로 변환한다. 배열의 각 자리는 어휘 사전의 단어가 몇 번 등장했는지를 의미한다.\n",
        "\n",
        "🗣️ 나의 검색어 \"사과\"의 벡터:\n",
        "\n",
        "[ 1, 0, 0, 0, 0, 0, 0, 0, 0 ]\n",
        "\n",
        "(설명: 사과 1번, 나머지 0번)\n",
        "\n",
        "\n",
        "* 📄 문서 1 (\"사과 주스 그리고 사과 파이\")의 벡터:\n",
        "\n",
        "  [ 2, 1, 1, 1, 0, 0, 0, 0, 0 ]\n",
        "\n",
        "  (설명: 사과 2번, 주스 1번, 그리고 1번, 파이 1번, 나머지 0번)\n",
        "\n",
        "* 📄 문서 2 (\"나는 축구 그리고 야구 좋아해\")의 벡터:\n",
        "\n",
        " [ 0, 0, 1, 0, 1, 1, 1, 1, 0 ]\n",
        "\n",
        " (설명: 사과 0번, ...)\n",
        "\n",
        "* 📄 문서 3 (\"나는 사과 그리고 바나나 좋아해\")의 벡터:\n",
        "\n",
        " [ 1, 0, 1, 0, 1, 0, 0, 1, 1 ]\n",
        "\n",
        " (설명: 사과 1번, ...)\n",
        "\n",
        "3️⃣ 단계: 수학으로 '가장 가까운' 문서 찾기 🧮\n",
        "이제 컴퓨터는 더 이상 '의미'를 고민할 필요가 없습니다. 그저 수학 문제를 풀면 됩니다.\n",
        "\n",
        "💻 컴퓨터의 임무: 나의 검색어 벡터 [1, 0, 0, ...] 와 가장 \"모양\"이 비슷한(가까운) 문서 벡터는 무엇인가?\n",
        "\n",
        "vs 문서 1: 검색어 벡터 [1,0,...] 와 문서 1 벡터 [2,1,...]는 첫 번째 사과 자리의 숫자가 모두 0이 아닌 값으로 매우 유사하다.\n",
        "\n",
        "vs 문서 2: 검색어 벡터 [1,0,...] 와 문서 2 벡터 [0,0,...]는 첫 번째 사과 자리의 값이 각각 1과 0으로, 매우 다르다.\n",
        "\n",
        "vs 문서 3: 검색어 벡터 [1,0,...] 와 문서 3 벡터 [1,0,...]는 첫 번째 사과 자리의 값이 1로 동일하여 꽤 유사하다.\n",
        "\n",
        "🏆 최종 결과\n",
        "컴퓨터는 이 수학적 거리(유사도) 계산을 통해 다음과 같은 결론을 내린다.\n",
        "\n",
        "🥇 가장 관련성 높은 문서: 문서 1 (가장 가까움)\n",
        "\n",
        "🥈 다음으로 관련성 높은 문서: 문서 3 (그 다음으로 가까움)\n",
        "\n",
        "👎 관련성 없는 문서: 문서 2 (가장 멂)\n",
        "\n",
        "✨ 결론\n",
        "이 예시처럼, 우리는 컴퓨터에게 '사과'의 의미를 가르치지 않았다. 단지 모든 텍스트를 숫자로 바꾸고, 수학적으로 가장 유사한 것을 찾으라고 시켰을 뿐이다.\n",
        "\n",
        "\n",
        "이것이 바로 초기 검색 엔진 🚀이 작동하던 핵심 원리이자, BoW 모델이 정보 검색 분야에서 혁신을 가져온 이유이다."
      ],
      "metadata": {
        "id": "HZ2qcj4gPUCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "이제 우리는 공백이 있는 텍스트로 BoW 모델의 기본 원리를 이해했다. 그렇다면 이 기술이 인문학 연구에서 실제로 어떻게 사용될까? 잠시 그 배경과 활용 사례를 살펴보겠다"
      ],
      "metadata": {
        "id": "T4TdJMZphNF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📚 디지털 인문학에서의 Bag-of-Words 활용\n",
        "\n",
        "디지털 인문학은 컴퓨터 기술을 활용해 문학, 역사, 철학 등 전통적인 인문학 자료를 분석하고 새로운 통찰을 얻는 학문 분야이다. BoW는 텍스트를 정량적 데이터로 바꿔준다는 점에서 디지털 인문학 연구에 매우 유용한 도구이다.\n",
        "\n",
        "* 토픽 모델링 (Topic Modeling):\n",
        "\n",
        "수백, 수천 권의 책이나 신문 기사 전체를 BoW로 변환한 뒤, 통계적 기법(예: LDA)을 적용하여 텍스트 데이터에 잠재된 '주제(Topic)'들을 자동으로 찾아내는 분석이다.\n",
        "\n",
        "활용 예시: 19세기 소설 1,000권을 분석하여 '사랑과 결혼', '전쟁과 사회', '산업혁명과 도시' 등 시대상을 반영하는 주요 주제들이 어떻게 변화하는지 거시적으로 파악할 수 있다.\n",
        "\n",
        "* 저자 판별 (Authorship Attribution):\n",
        "\n",
        "작가마다 자주 사용하는 단어나 단어 빈도에 고유한 스타일이 있다는 점에 착안한 분석이다. 미상의 저자가 쓴 글을 BoW 벡터로 변환한 뒤, 여러 후보 작가들의 작품 벡터와 비교하여 누가 썼을지 추론한다.\n",
        "\n",
        "활용 예시: 특정 고문서의 저자가 알려진 역사적 인물 A인지 B인지 판별하거나, 특정 작가의 미발표 작품으로 추정되는 텍스트의 진위를 가리는 데 사용될 수 있다.\n",
        "\n",
        "* 멀리서 읽기 (Distant Reading):\n",
        "\n",
        "문학 작품을 한 편 한 편 자세히 읽는 '근거리 독서(Close Reading)'와 반대되는 개념이다. BoW를 통해 방대한 양의 텍스트를 '읽지 않고' 데이터로 간주하여, 그 안의 패턴, 경향, 구조적 특징을 발견하는 연구 방법이다.\n",
        "\n",
        "활용 예시: 특정 시대 문학 작품들에서 특정 감정(예: 불안, 희망)과 관련된 단어들의 출현 빈도 변화를 추적하여 시대적 감성의 흐름을 시각화할 수 있다.\n",
        "\n",
        "BoW는 텍스트의 섬세한 의미나 맥락은 놓칠지라도, 방대한 텍스트 데이터로부터 객관적인 패턴과 구조를 발견할 수 있다."
      ],
      "metadata": {
        "id": "NOibJipCfniV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **고대 중국어(한문) 텍스트**에 Bag-of-Words 모델을 적용할 때 고려해야 될 부분은?\n",
        "\n",
        "* 단어를 어떻게 분해할 것인가(토큰화)이다.\n",
        "\n",
        "* 기존에 사용했던 **CountVectorizer**는 기본적으로 **공백(띄어쓰기)**을 기준으로 단어를 나눈다.\n",
        "\n",
        "* 고대 중국어에는 띄어쓰기가 없다. 고대 중국어 텍스트를 CountVectorizer에 그냥 넣으면, 전체를 하나의 거대한 단어로 인식하여 ['佛說阿彌陀經'] 이렇게 처리해 버린다. 우리가 원하는 ['佛', '說', '阿彌陀經'] 이나 ['佛說', '阿彌陀經'] 같은 결과를 얻을 수 없다.\n",
        "\n"
      ],
      "metadata": {
        "id": "OpbEj8Cel5V8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 띄어쓰기가 없는 고대 중국어 텍스트 예시\n",
        "documents = [\n",
        "    \"佛說阿彌陀經\",\n",
        "    \"觀自在菩薩\"\n",
        "]\n",
        "\n",
        "default_vectorizer = CountVectorizer()\n",
        "\n",
        "bow_matrix = default_vectorizer.fit_transform(documents)\n",
        "\n",
        "vocabulary = default_vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"▼ 생성된 어휘 사전 (문제 상황):\")\n",
        "print(vocabulary)\n",
        "\n",
        "print(\"\\n▼ BoW 행렬:\")\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2HC4gxtfrXM",
        "outputId": "876c2be6-f8c8-465d-fed0-ef1cb4fc489a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▼ 생성된 어휘 사전 (문제 상황):\n",
            "['佛說阿彌陀經' '觀自在菩薩']\n",
            "\n",
            "▼ BoW 행렬:\n",
            "[[1 0]\n",
            " [0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**결과 해석**\n",
        "\n",
        "* **get_feature_names_out()**으로 확인한 어휘 사전(vocabulary)이 ['佛說阿彌陀經', '觀自在菩薩']으로 나왔다 = CountVectorizer가 **'佛說阿彌陀經'**이라는 문자열 전체를 띄어쓰기가 없는 하나의 단어로, '觀自在菩薩' 역시 하나의 단어로 인식했음을 의미한다.\n",
        "\n",
        "* ['佛', '說', '阿彌陀經'] 이나 ['佛說', '阿彌陀經'] 과 같이 의미 단위로 분해된 결과가 전혀 나오지 않았다.\n",
        "\n",
        "* 별도의 토크나이저 지정 없이 CountVectorizer를 고대 중국어 텍스트에 적용했을 때 발생하는 문제점"
      ],
      "metadata": {
        "id": "J96rpfIFqUgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 띄어쓰기가 없는 한문 텍스트 예시\n",
        "documents = [\n",
        "    \"佛說摩訶般若波羅蜜多心經\", # 불설마하반야바라밀다심경\n",
        "    \"觀自在菩薩行深般若波羅蜜多時\"  # 관자재보살행심반야바라밀다시\n",
        "]\n",
        "\n",
        "# 글자 단위로 분해하는 함수 정의\n",
        "def char_tokenizer(text):\n",
        "    \"\"\"문자열을 한 글자씩(char) 잘라서 리스트로 반환합니다.\"\"\"\n",
        "    return list(text)\n",
        "\n",
        "# CountVectorizer에 우리가 만든 토크나이저를 지정\n",
        "# tokenizer=char_tokenizer 부분이 핵심입니다.\n",
        "vectorizer = CountVectorizer(tokenizer=char_tokenizer)\n",
        "\n",
        "bow_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"어휘 사전 (글자 단위):\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "# 출력 예시: ['多', '在', '心', '時', '書', '波', '菩', ... ]\n",
        "\n",
        "print(\"\\nBoW 행렬:\")\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "id": "pbyNU8aqqZdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 해결책1: **글자 단위로 분해하기**\n",
        "\n",
        "가장 간단하고 직관적인 방법이다. 모든 글자 하나하나를 독립된 단어로 취급하는 방식이다.\n",
        "\n",
        "  * 장점: 구현이 매우 쉽고, 별도의 라이브러리가 필요 없다. 저자 판별 등 스타일 분석에서는 글자 단위의 빈도 패턴이 유용할 수 있다.\n",
        "  * 단점: '佛陀(부처)'나 '菩薩(보살)'처럼 두 글자 이상이 합쳐져 의미를 만드는 단어의 뜻이 완전히 사라진다."
      ],
      "metadata": {
        "id": "SylHRFvAp0yL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 띄어쓰기가 없는 한문 텍스트 예시\n",
        "documents = [\n",
        "    \"佛說摩訶般若波羅蜜多心經\",\n",
        "    \"觀自在菩薩行深般若波羅蜜多時\"\n",
        "]\n",
        "\n",
        "# <<< 핵심: 사용자 사전에 전문 용어 추가 >>>\n",
        "# jieba가 기본적으로 모르는 불교 용어를 사전에 추가해줍니다.\n",
        "# 이렇게 해야 '摩訶般若波羅蜜多'를 한 단어로 인식할 수 있습니다.\n",
        "jieba.add_word('摩訶般若波羅蜜多')\n",
        "jieba.add_word('觀自在菩薩')\n",
        "jieba.add_word('心經')\n",
        "\n",
        "# jieba를 사용해 단어를 분해하는 함수 정의\n",
        "def jieba_tokenizer(text):\n",
        "    \"\"\"jieba 라이브러리를 사용해 단어를 분해하고 리스트로 반환합니다.\"\"\"\n",
        "    return jieba.lcut(text)\n",
        "\n",
        "# CountVectorizer에 jieba 토크나이저를 지정\n",
        "vectorizer = CountVectorizer(tokenizer=jieba_tokenizer)\n",
        "\n",
        "bow_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"어휘 사전 (단어 단위):\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "# 출력 예시: ['佛說', '心經', '摩訶般若波羅蜜多', '觀自在菩薩', ... ]\n",
        "\n",
        "print(\"\\nBoW 행렬:\")\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "id": "tX-k8oAWX5sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 해결책 2: 사전(辭典) 기반으로 단어(單語) 단위로 분해하기\n",
        "\n",
        "* 고대 중국어 단어 사전을 기반으로 의미 있는 단어(예: 佛陀, 菩薩, 阿耨多羅三藐三菩提)를 식별해내는 방식. 훨씬 더 정교하고 의미 분석에 유리하다.\n",
        "\n",
        "  * 장점: 단어의 의미를 보존할 수 있어 토픽 모델링 등 semantic 분석에 훨씬 적합하다.\n",
        "  * 단점: 어떤 사전을 사용하느냐에 따라 성능이 크게 좌우된다. 해당 분야(예: 불교)의 전문 용어가 사전에 없으면 제대로 분해하지 못할 수 있다. (이 경우, 사용자 사전에 단어를 추가해야 한다.)"
      ],
      "metadata": {
        "id": "Nktf7hj1X7R0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[실습] 실습자가 BoW 모델을 적용하여 텍스트 분석해보자!**"
      ],
      "metadata": {
        "id": "U6vGircgYdNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[실습 예시 작성]\n",
        "* 대상 텍스트 : 명심보감 계선편\n",
        "* 코드 작성자: 강훈혁(drive-shares-dm-noreply@google.com)\n",
        "* 설명 작성자: 정송이"
      ],
      "metadata": {
        "id": "TqQ9TulFdAbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📜 실습: 명심보감으로 배우는 한문 텍스트 Bag-of-Words(BoW) 모델\n",
        "\n",
        "이 코드는 띄어쓰기가 없는 고대 한문 텍스트(명심보감)를 컴퓨터가 이해할 수 있는 숫자 데이터, 즉 Bag-of-Words(BoW) 모델로 변환하는 전체 과정을 보여주는 훌륭한 실습 예제이다.\n",
        "\n",
        "🎯 실습 목표\n",
        "\n",
        "* 띄어쓰기 없는 한문 텍스트의 '단어' 기준을 어떻게 정하는지 이해한다.\n",
        "* CountVectorizer에 **나만의 단어 분해 규칙(Tokenizer)**을 적용하는 방법을 배운다.\n",
        "* 최종적으로 생성된 **숫자 행렬(BoW)**이 어떤 의미를 갖는지 해석한다."
      ],
      "metadata": {
        "id": "YlYhFBu0d-5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚙️ 1단계: 텍스트 준비 및 '단어' 기준 정의하기"
      ],
      "metadata": {
        "id": "YSJq4JIVeQa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 띄어쓰기가 없는 한문 텍스트 예시 - 명심보감 계선편\n",
        "documents = [\n",
        "    \"漢昭烈將終勅後主曰\",\n",
        "    \"勿以善小而不爲勿以惡小而爲之\"\n",
        "]\n",
        "\n",
        "# <<< 핵심: 사용자 사전에 전문 용어 추가 >>>\n",
        "# jieba가 기본적으로 모르는 용어를 사전에 추가해줍니다.\n",
        "jieba.add_word('漢昭烈')\n",
        "jieba.add_word('後主')"
      ],
      "metadata": {
        "id": "N4ZlBqCoeNUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "설명:\n",
        "\n",
        "모든 BoW 모델의 첫걸음은 \"무엇을 한 단어로 볼 것인가?\"를 정하는 것이다.\n",
        "\n",
        "문제: 컴퓨터는 '漢昭烈(한소열)'이 사람 이름이라는 것을 모른다. 그냥 두면 '漢', '昭', '烈' 세 글자로 분해할 수 있다.\n",
        "\n",
        "해결책: jieba.add_word()를 사용해 \"‘漢昭烈’은 하나의 의미를 가진 고유명사이니 절대 쪼개지 마!\"라고 컴퓨터에게 미리 알려준다. '後主(후주)'도 마찬가지이다. 이 과정은 BoW 모델의 품질을 높이는 매우 중요한 작업이다."
      ],
      "metadata": {
        "id": "Ay9ieydyeT7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧠 2단계: 단어 분해기(Tokenizer) 준비 및 BoW 모델 생성"
      ],
      "metadata": {
        "id": "iEnMsaQoeh75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# jieba를 사용해 단어를 분해하는 함수 정의\n",
        "def jieba_tokenizer(text):\n",
        "    \"\"\"jieba 라이브러리를 사용해 단어를 분해하고 리스트로 반환합니다.\"\"\"\n",
        "    return jieba.lcut(text)\n",
        "\n",
        "# CountVectorizer에 jieba 토크나이저를 지정\n",
        "vectorizer = CountVectorizer(tokenizer=jieba_tokenizer)\n",
        "\n",
        "bow_matrix = vectorizer.fit_transform(documents)"
      ],
      "metadata": {
        "id": "cIu39PMrAPOf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a826a81b-9780-4409-80ff-b76c1e2f3828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 3.309 seconds.\n",
            "DEBUG:jieba:Loading model cost 3.309 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어휘 사전 (단어 단위):\n",
            "['不' '之' '以' '勅' '勿' '善小而' '將終' '後主' '惡小而' '曰' '漢昭烈' '爲']\n",
            "\n",
            "BoW 행렬:\n",
            "[[0 0 0 1 0 0 1 1 0 1 1 0]\n",
            " [1 1 2 0 2 1 0 0 1 0 0 2]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "설명:\n",
        "\n",
        "이제 본격적으로 BoW 모델을 만든다.\n",
        "\n",
        "jieba_tokenizer(text) 함수: 위에서 우리가 단어 기준을 알려준 jieba를 이용해 문장을 단어들로 분해하는 **'전용 단어 분해기'**를 만든다.\n",
        "\n",
        "CountVectorizer(tokenizer=jieba_tokenizer): BoW 모델 생성 도구(CountVectorizer)에게 \"원래 내장된 공백 기준 분해기 말고, 우리가 방금 만든 jieba_tokenizer를 사용해 줘!\"라고 작업 도구를 지정해 준다.\n",
        "\n",
        "fit_transform(documents): 이 명령 하나로 두 가지 일이 동시에 일어난다.\n",
        "\n",
        "fit (학습): documents의 모든 문장을 jieba_tokenizer로 분해해 어떤 단어들이 있는지 전체 어휘 사전을 만든다.\n",
        "\n",
        "transform (변환): 위에서 만든 어휘 사전을 기준으로, 각 문서를 숫자 배열(벡터)로 변환한다."
      ],
      "metadata": {
        "id": "LlcXdlV5d1vz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📊 3단계: 결과 분석 및 의미 해석"
      ],
      "metadata": {
        "id": "uid8TiJae0Lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"어휘 사전 (단어 단위):\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nBoW 행렬:\")\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "id": "FdTSy6qse0nE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과 해설\n",
        "\n",
        "어휘 사전: jieba_tokenizer를 통해 분해된 모든 단어의 목록이다. 이 목록의 순서가 바로 아래 숫자 행렬의 열(column) 순서가 된다. '漢昭烈'이 한 단어로 잘 들어가 있는 것을 볼 수 있다.\n",
        "\n",
        "BoW 행렬 (숫자 배열) 해석:\n",
        "\n",
        "첫 번째 문서 (\"漢昭烈將終勅後主曰\"): [0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1]\n",
        "\n",
        "曰: 1번, 將: 1번, 勅: 1번, 漢昭烈: 1번, 後主: 1번 등장했다는 의미이다. 다른 단어는 0번 등장했다.\n",
        "\n",
        "두 번째 문서 (\"勿以善小而不爲勿以惡小而爲之\"): [2, 0, 1, 2, 0, 1, 2, 2, 1, 0, 0, 0]\n",
        "\n",
        "勿: 2번, 善: 1번, 小: 2번, 惡: 1번, 而: 2번, 爲: 2번, 之: 1번 등장했다는 의미"
      ],
      "metadata": {
        "id": "yBfihKRQe2-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✨ 최종 결론\n",
        "\n",
        "이 실습은 띄어쓰기가 없는 한문 텍스트라도\n",
        "\n",
        " 1) 적절한 외부 도구(jieba)를 사용하고,\n",
        "\n",
        " 2) 연구에 맞게 사용자 사전을 정의하며,\n",
        "\n",
        " 3) tokenizer를 직접 만들어 CountVectorizer에 제공하면,\n",
        "\n",
        " 성공적으로 텍스트를 정량적인 Bag-of-Words 모델로 변환할 수 있음을 보여준다. 이는 컴퓨터를 이용한 문헌 연구의 가장 기초적인 첫걸음이다."
      ],
      "metadata": {
        "id": "kyFwGoUQe_UH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[실습 예시 작성]\n",
        "* 대상 텍스트 : 세종실록 102권\n",
        "* 코드 작성자: 강훈혁(drive-shares-dm-noreply@google.com)\n",
        "* 설명 작성자: 정송이"
      ],
      "metadata": {
        "id": "kGBGRMXnARC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "실습: 세종실록 텍스트를 활용한 Bag-of-Words(BoW) 모델 구축\n",
        "\n",
        "본 코드는 세종실록의 훈민정음 창제 관련 기록을 대상으로 Bag-of-Words 모델을 구축하는 실습 과정임.\n",
        "\n",
        "이를 통해 학습자는 특정 도메인(역사 기록)의 한문 텍스트를 정량적 데이터로 변환하는 핵심 원리를 이해할 수 있음."
      ],
      "metadata": {
        "id": "jXxC7PptfYPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1단계: 데이터 준비 및 핵심 단어 정의"
      ],
      "metadata": {
        "id": "I3LMxs9MfZ7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 대상 텍스트: 세종실록 102권, 훈민정음 창제 기록\n",
        "documents = [\n",
        "    \"是月上親制諺文二十八字其字倣古篆分爲初中終聲合之然後乃成字\",\n",
        "    \"凡干文字及本國俚語皆可得而書字雖簡要轉換無窮是謂訓民正音\"\n",
        "]\n",
        "\n",
        "# <<< 핵심: 사용자 사전에 고유 명사 추가 >>>\n",
        "# 분석 텍스트에만 등장하는 고유 명사(諺文, 訓民正音 등)를\n",
        "# 하나의 단어로 인식시키기 위해 사용자 사전에 등록함.\n",
        "# 이 과정은 분석의 정확도를 결정하는 핵심적인 단계이다.\n",
        "jieba.add_word('諺文')\n",
        "jieba.add_word('古篆')\n",
        "jieba.add_word('初中終聲')\n",
        "jieba.add_word('本國')\n",
        "jieba.add_word('訓民正音')"
      ],
      "metadata": {
        "id": "DUrgJcMgfiec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "설명:\n",
        "\n",
        "BoW 모델의 성능은 '단어'를 어떻게 정의하느냐에 따라 결정됨. '諺文(언문)', '訓民正音(훈민정음)' 등은 해당 텍스트의 핵심 개념이므로, jieba.add_word()를 통해 분해되지 않는 하나의 단위로 명확히 지정함."
      ],
      "metadata": {
        "id": "Io_lSr0Zfk_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2단계: 토크나이저 정의 및 BoW 모델 생성"
      ],
      "metadata": {
        "id": "2Ul2ys0Gfm54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# jieba를 사용해 단어를 분해하는 함수(토크나이저) 정의\n",
        "def jieba_tokenizer(text):\n",
        "    return jieba.lcut(text)\n",
        "\n",
        "# CountVectorizer에 위에서 정의한 jieba 토크나이저를 사용하도록 설정\n",
        "vectorizer = CountVectorizer(tokenizer=jieba_tokenizer)\n",
        "\n",
        "# 문서를 학습(fit)하고 BoW 행렬로 변환(transform)\n",
        "bow_matrix = vectorizer.fit_transform(documents)"
      ],
      "metadata": {
        "id": "9HvODgNPfpyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "설명:\n",
        "\n",
        "사용자 사전이 적용된 jieba를 단어 분해 규칙(tokenizer)으로 설정함.\n",
        "\n",
        "CountVectorizer는 이 규칙에 따라 전체 문서에서 어휘 사전을 구축(fit)하고, 각 문서를 단어 빈도수 기반의 숫자 벡터로 변환(transform)하는 작업을 수행함."
      ],
      "metadata": {
        "id": "oJVEmgw4fqz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3단계: 결과 분석"
      ],
      "metadata": {
        "id": "JtqXHIYZfwOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"어휘 사전 (단어 단위):\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nBoW 행렬:\")\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "id": "KG4yChmVAQYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24a76175-108a-4d9e-97f4-79e5c463c304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어휘 사전 (단어 단위):\n",
            "['上' '乃' '二十八' '俚語' '倣' '其字' '凡干' '分' '初中終聲' '及' '古篆' '可' '合之然' '字' '後'\n",
            " '得' '成字' '文字' '是' '書' '月' '本國' '無窮' '爲' '皆' '簡要' '而' '親制' '訓民正音' '諺文' '謂'\n",
            " '轉換' '雖']\n",
            "\n",
            "BoW 행렬:\n",
            "[[1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0]\n",
            " [0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과 해석\n",
        "\n",
        "* 어휘 사전: jieba와 사용자 사전을 통해 추출된 총 47개의 고유한 단어 목록임. 이 목록이 BoW 벡터의 기준(차원)이 됨. '諺文', '訓民正音' 등 핵심 단어가 하나의 단위로 잘 포함되어 있음을 확인함.\n",
        "\n",
        "* BoW 행렬: 각 행은 하나의 문서를, 각 열은 어휘 사전의 해당 단어를 의미함.\n",
        "\n",
        "* 첫 번째 문서 벡터: 어휘 사전의 '乃', '然後', '分', '古篆' 등 19개 단어가 각각 1회, '字'가 2회 출현했음을 나타냄.\n",
        "\n",
        "* 두 번째 문서 벡터: 어휘 사전의 '俚語', '及', '可', '國' 등 15개 단어가 각각 1회 출현했음을 나타냄."
      ],
      "metadata": {
        "id": "M81LalLLAR7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "결론\n",
        "\n",
        "본 실습은 jieba와 같은 외부 라이브러리와 사용자 사전을 효과적으로 활용하면, 띄어쓰기가 없는 역사 기록물(한문) 또한 통계 및 머신러닝 분석이 가능한 Bag-of-Words 모델로 성공적으로 변환할 수 있음을 보임."
      ],
      "metadata": {
        "id": "VsSLXc1jf6VU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## \"그래서 이걸로 무엇을 할 수 있는가?\"\n",
        "\n",
        "해당 코드및 설명 작성자: 정송이\n",
        "\n",
        "[응용] BoW 모델 활용하기: 위의 실습코드인 세종실록의 두 문장을 기반으로 두 문장이 내용적으로 얼마나 유사한지 BoW 모델과 코사인 유사도를 통해 계산해보기\n",
        "\n",
        "* 디지털 인문학 연구에서는 연구 대상이 되는 원본 텍스트(source text)를 어떤 기준으로 '분석 단위'로 나눌 것인지 결정하는 것이 매우 중요하다.\n",
        "\n",
        "* 위의 실습 코드를 작성하면서, 실습자는 기술을 배우는 동시에 **'분석 단위를 설정하는 연구자의 개입'**이라는 중요한 디지털 인문학 방법론을 함께 고민하였을 것이다.\n"
      ],
      "metadata": {
        "id": "f1aD4vCRh1L_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# 위에서 만든 세종실록 BoW 행렬(bow_matrix)로 유사도 계산\n",
        "similarity_matrix = cosine_similarity(bow_matrix)\n",
        "\n",
        "print(\"문서 간 코사인 유사도 행렬:\")\n",
        "print(similarity_matrix)\n",
        "# [[1.        0.234567]  <- 1번-1번, 1번-2번 유사도\n",
        "#  [0.234567  1.       ]]  <- 2번-1번, 2번-2번 유사도"
      ],
      "metadata": {
        "id": "TIPr8ZS9iATC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[최종 응용] 서로 다른 두 문헌의 유사도 계산하기"
      ],
      "metadata": {
        "id": "K9ImEJ5UiB_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import jieba\n",
        "\n",
        "# 비교할 두 개의 다른 문서\n",
        "compare_docs = [\n",
        "    \"漢昭烈將終勅後主曰\",        # 문서 1: 명심보감\n",
        "    \"是月上親制諺文二十八字\"    # 문서 2: 세종실록 (간결하게 일부만 사용)\n",
        "]\n",
        "\n",
        "# 명심보감과 세종실록의 단어를 모두 사용자 사전에 추가\n",
        "jieba.add_word('漢昭烈')\n",
        "jieba.add_word('後主')\n",
        "jieba.add_word('諺文')\n",
        "\n",
        "# BoW 모델 생성 (토크나이저는 이전 단계에서 정의한 jieba_tokenizer 사용)\n",
        "vectorizer = CountVectorizer(tokenizer=jieba_tokenizer)\n",
        "bow_matrix = vectorizer.fit_transform(compare_docs)\n",
        "\n",
        "# 코사인 유사도 계산\n",
        "similarity = cosine_similarity(bow_matrix)\n",
        "\n",
        "print(\"명심보감과 세종실록 문장의 유사도:\", similarity[0][1])"
      ],
      "metadata": {
        "id": "LPoOYgzTjdJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "해석:\n",
        "\n",
        "이 코드를 실행하면, 두 문서의 내용이 매우 다르기 때문에 유사도 점수가 낮게 나올 것이다.\n",
        "\n",
        "BoW 모델은 두 텍스트의 주제적 차이를 숫자로 명확하게 보여주는 모델이다."
      ],
      "metadata": {
        "id": "y2q5aaTajepQ"
      }
    }
  ]
}
