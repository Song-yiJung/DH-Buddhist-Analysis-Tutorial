{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8REu526C3wxFHHGHR5hIh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Song-yiJung/DH-Buddhist-Analysis-Tutorial/blob/main/7_GloVe%EB%AA%A8%EB%8D%B8_%EB%A7%8C%EB%93%A4%EA%B8%B0_%ED%95%9C%EB%AC%B8%ED%85%8D%EC%8A%A4%ED%8A%B8%EC%97%90%EB%8A%94%EB%B6%80%EC%A0%81%ED%95%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📜 GloVe의 원리와 활용\n",
        "\n",
        "Word2Vec은 '예측 기반' 임베딩 방식으로, 한 번에 텍스트의 작은 부분(window)만 보기 때문에 문헌 전체에 나타나는 거시적인 통계 정보를 효율적으로 활용하지 못하는 한계점을 가진다.\n",
        "\n",
        "이러한 단점을 보완하기 위해 등장한 모델이 바로 **GloVe (Global Vectors for Word Representation)**이다."
      ],
      "metadata": {
        "id": "0kkkXPUSxDRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. GloVe의 핵심 개념: '전체'를 보고 '관계'를 예측하다\n",
        "\n",
        "GloVe의 핵심 아이디어는 **\"단어 의미는 전체 문헌에서 함께 등장하는 횟수, 즉 '동시 등장 확률'에 중요한 정보가 담겨있다\"**는 것이다.\n",
        "\n",
        "Word2Vec이 텍스트를 부분적으로 탐색하며 문맥을 학습하는 '탐험가'라면, GloVe는 **먼저 전체 지도를 그린 뒤, 각 지역 간의 관계를 파악하는 '지리학자'**에 비유할 수 있다.\n",
        "\n",
        "## GloVe의 작동 원리 (통계 + 예측의 결합)\n",
        "\n",
        "1. [통계] 동시 등장 행렬(Co-occurrence Matrix) 구축:\n",
        "\n",
        "* 먼저, 전체 문헌 집합(Corpus)을 한 번 훑어서 어떤 단어가 어떤 단어와 함께 자주 등장하는지 그 횟수를 모두 세어 거대한 표(행렬)를 만든다. 이 과정에서 '전역(Global)' 통계 정보를 확보한다.\n",
        "\n",
        "2. [예측] 행렬의 정보를 예측하는 벡터 학습:\n",
        "\n",
        "* 다음으로, 이 '동시 등장' 정보가 잘 표현되도록 각 단어의 벡터 값을 학습(예측)한다. 단순 등장 횟수를 그대로 사용하는 것이 아니라, **단어 간의 동시 등장 '비율'**을 통해 그 관계를 학습하는 것이 특징이다.\n",
        "\n",
        "* 예를 들어, 특정 조건 하에서 ('얼음'이 등장할 확률) / ('수증기'가 등장할 확률)의 비율은 특정 의미를 내포하는데, GloVe는 바로 이 '비율' 관계를 벡터 공간에 효과적으로 표현한다.\n",
        "\n",
        "이처럼 GloVe는 전체 통계를 먼저 계산하는 '통계 기반' 방식과 벡터 간의 관계를 학습하는 '예측 기반' 방식의 장점을 결합한 중간적 접근법이다."
      ],
      "metadata": {
        "id": "PQLNfWDWxPPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 등장 배경: 두 세계의 장점을 합치다\n",
        "\n",
        "* 누가/언제: GloVe는 2014년, 스탠포드 대학교의 제프리 페닝턴(Jeffrey Pennington), 리처드 소처(Richard Socher), 크리스토퍼 매닝(Christopher D. Manning)에 의해 개발되었다.\n",
        "\n",
        "* 왜: 당시 단어 임베딩 분야는 크게 두 가지 흐름, 즉 거시적 통계(행렬 분해)를 사용하는 방법과 지역적 문맥(window)을 사용하는 Word2Vec과 같은 방법이 있었다. GloVe 연구팀은 두 방법론의 장점만을 결합하여, 단어 간의 유추 관계를 잘 파악하면서도 전역적인 통계 정보까지 효율적으로 활용하는 새로운 모델을 만들고자 했다."
      ],
      "metadata": {
        "id": "a5RMXhcyxjPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 디지털 인문학에서의 활용\n",
        "GloVe가 생성한 벡터는 Word2Vec과 마찬가지로 의미 분석에 사용되나, 특히 단어 간의 관계를 분석하는 데 강점을 보인다.\n",
        "\n",
        "## 1. 개념 간의 '관계' 정밀 분석 및 유추\n",
        "\n",
        "* 활용 방안: GloVe는 왕 - 남자 + 여자 = 여왕과 같은 의미 유추(Analogy) 문제에 매우 강력하다. 이 특징을 활용하여 역사적 개념들 사이의 관계를 탐색할 수 있다. 예를 들어, (태종 - 정도전) + (세조 = ?) 와 같은 탐색적 질문을 통해 인물, 사건, 제도 간의 복잡한 관계망을 분석하는 것이 가능하다.\n",
        "\n",
        "## 2. 전체 문헌 집합의 의미 구조 시각화\n",
        "\n",
        "* 활용 방안: 특정 문헌 집합 전체의 의미 구조를 파악하는 데 유용하다. GloVe로 학습한 단어 벡터들을 차원 축소 기법(t-SNE 등)을 이용해 2차원 평면에 시각화하면, 의미적으로 관련된 단어들이 응집력 있는 군집(cluster)을 형성하는 경향이 있다. 이를 통해 문헌 집합의 거시적 주제 분포를 시각적으로 확인할 수 있다.\n",
        "\n",
        "## 3. 사전 훈련된 모델(Pre-trained Model)을 활용한 빠른 연구\n",
        "* 활용 방안: 모델을 직접 훈련시키는 데는 많은 시간과 자원이 필요하다. 스탠포드 연구팀은 위키피디아 등 방대한 데이터로 미리 훈련시킨 GloVe 벡터들을 공개했다. 현대어를 다루는 연구자는 이 사전 훈련된 모델을 다운로드하여 즉시 연구에 활용할 수 있다. 이는 단어 유사도 측정, 문서 분류 등의 작업을 빠르게 수행하게 하여 연구의 진입 장벽을 크게 낮춘다."
      ],
      "metadata": {
        "id": "Kw4FchENxn0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 코드 예시: 사전 훈련된 GloVe 모델 활용하기\n",
        "\n",
        "GloVe는 직접 훈련시키는 과정이 복잡하므로, 가장 일반적인 활용 사례인 **'사전 훈련된 모델'**을 불러와 사용하는 코드를 소개한다."
      ],
      "metadata": {
        "id": "9D8PqKhDxvGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# numpy, scipy를 포함한 관련 라이브러리를 최신 버전으로 강제 업그레이드합니다.\n",
        "!pip install --upgrade numpy scipy gensim -q\n",
        "\n",
        "print(\"✅ 라이브러리 설치 및 업그레이드 완료!\")\n",
        "print(\"🔴 중요: 이제 메뉴에서 [런타임] > [런타임 다시 시작]을 실행하여 변경사항을 적용해주세요.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWCUmmNOya3X",
        "outputId": "31f08462-4480-4324-9064-2919d4cc5ad9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h✅ 라이브러리 설치 및 업그레이드 완료!\n",
            "🔴 중요: 이제 메뉴에서 [런타임] > [런타임 다시 시작]을 실행하여 변경사항을 적용해주세요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. 파일 삭제 및 재다운로드 ---\n",
        "# 이전에 잘못 다운로드된 파일이 있다면 깨끗하게 삭제합니다.\n",
        "!rm -f glove.6B.*\n",
        "\n",
        "# -q 옵션을 제거하여 다운로드 진행 상황을 확인합니다.\n",
        "# 파일 크기가 매우 크므로(약 822MB), 완료까지 수 분 이상 소요될 수 있습니다.\n",
        "print(\"GloVe 모델 다운로드를 시작합니다. 완료까지 잠시 기다려주세요...\")\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n",
        "# 다운로드된 파일 압축 해제\n",
        "!unzip -q glove.6B.zip\n",
        "print(\"✅ 다운로드 및 압축 해제 완료!\")\n",
        "\n",
        "\n",
        "# --- 2. 라이브러리 불러오기 및 모델 로드 ---\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# 다운로드된 GloVe 파일을 gensim에서 사용할 수 있는 형식으로 로드\n",
        "glove_model = KeyedVectors.load_word2vec_format('glove.6B.50d.txt', binary=False, no_header=True)\n",
        "print(\"✅ 사전 훈련된 GloVe 모델 로드 완료.\")\n",
        "\n",
        "\n",
        "# --- 3. 결과 탐색 ---\n",
        "print(\"\\n--- 🚀 GloVe 모델 탐색 시작 ---\")\n",
        "\n",
        "# 질문 1: 'king'과 유사한 단어 탐색\n",
        "print(\"\\n[질문 1] 'king'과 의미적으로 가장 유사한 단어는?\")\n",
        "try:\n",
        "    similar_to_king = glove_model.most_similar('king', topn=5)\n",
        "    print(\"▶️ 결과:\", similar_to_king)\n",
        "except KeyError:\n",
        "    print(\"▶️ 결과: 모델 어휘 사전에 'king'이 없음.\")\n",
        "\n",
        "# 질문 2: 의미 연산을 통한 유추 ('왕 - 남자 + 여자 = ?')\n",
        "print(\"\\n[질문 2] 'king' - 'man' + 'woman' = ?\")\n",
        "try:\n",
        "    analogy = glove_model.most_similar(positive=['king', 'woman'], negative=['man'], topn=3)\n",
        "    print(\"▶️ 결과:\", analogy)\n",
        "except KeyError as e:\n",
        "    print(f\"▶️ 결과: '{e.args[0]}' 단어가 어휘 사전에 없어 연산이 불가능함.\")\n",
        "\n",
        "print(\"\\n--- 탐색 종료 ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ldpDofl0UOU",
        "outputId": "b87e8cca-165a-4a6c-dbc2-95779cd15b98"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GloVe 모델 다운로드를 시작합니다. 완료까지 잠시 기다려주세요...\n",
            "--2025-07-05 04:21:31--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-07-05 04:21:31--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-07-05 04:21:31--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.00MB/s    in 2m 39s  \n",
            "\n",
            "2025-07-05 04:24:10 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "✅ 다운로드 및 압축 해제 완료!\n",
            "✅ 사전 훈련된 GloVe 모델 로드 완료.\n",
            "\n",
            "--- 🚀 GloVe 모델 탐색 시작 ---\n",
            "\n",
            "[질문 1] 'king'과 의미적으로 가장 유사한 단어는?\n",
            "▶️ 결과: [('prince', 0.8236179351806641), ('queen', 0.7839043140411377), ('ii', 0.7746230363845825), ('emperor', 0.7736247777938843), ('son', 0.766719400882721)]\n",
            "\n",
            "[질문 2] 'king' - 'man' + 'woman' = ?\n",
            "▶️ 결과: [('queen', 0.8523604273796082), ('throne', 0.7664334177970886), ('prince', 0.7592144012451172)]\n",
            "\n",
            "--- 탐색 종료 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 결과 분석\n",
        "\n",
        "### 1. 'king'과 유사한 단어: prince, queen, emperor 등\n",
        "이 결과는 모델이 단순히 표면적인 텍스트를 넘어 '왕족' 또는 '군주'라는 추상적인 개념을 학습했음을 의미한다. 'king'이라는 단어와 비슷한 문맥에서 자주 사용되는 'prince'(왕자), 'queen'(여왕), 'emperor'(황제) 등의 단어들을 의미 공간에서 가장 가까운 이웃으로 정확하게 찾아냈다.\n",
        "\n",
        "### 2. 'king' - 'man' + 'woman' = ?의 결과: queen\n",
        "이것은 GloVe의 성능을 보여주는 가장 유명한 예시이다. 이 결과가 의미하는 바는 다음과 같다.\n",
        "\n",
        "모델은 **단어 간의 관계(벡터의 방향과 거리)**까지 학습했다. 즉, 'king' 벡터에서 'man' 벡터를 뺀 값은 '왕'이라는 개념에서 '남성성'이라는 속성을 제거한 것과 같다.\n",
        "\n",
        "여기에 'woman' 벡터를 더하자, 모델은 의미 공간에서 **'왕의 개념' + '여성성'**을 가장 잘 나타내는 단어로 'queen'을 찾아냈다.\n",
        "\n",
        "이는 모델이 단순히 'king'과 'queen'이 비슷하다고 아는 것을 넘어, '남성:여성'의 관계가 'king:queen'의 관계와 유사하다는 정교한 유추(Analogy)를 해냈음을 보여주는 매우 강력한 증거이다."
      ],
      "metadata": {
        "id": "Ti7YH_oG1RBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 코드는 대규모 영문 데이터로 학습된 모델을 활용하는 예시이지만, 사전 훈련된 모델을 다운로드하여 KeyedVectors로 변환한 뒤, Word2Vec과 거의 동일한 방식으로 활용할 수 있다는 점을 보여주는 좋은 사례이다."
      ],
      "metadata": {
        "id": "vn8PoCjLx1SS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GloVe 방법론을 한문텍스트에 적용할 때의 불편한 점\n",
        "가장 큰 이유는 GloVe의 작동 방식에 있다. GloVe를 훈련시키려면, 먼저 전체 텍스트(코퍼스)에 있는 모든 단어들의 **'동시 등장 행렬(Co-occurrence Matrix)'**이라는 거대한 표를 만들어야 한다.\n",
        "\n",
        "전처리 부담: 이 행렬을 만드는 과정은 상당한 양의 메모리와 계산을 필요로 한다. 텍스트가 클수록 이 작업은 기하급수적으로 무거워진다.\n",
        "\n",
        "구현의 복잡성: gensim 라이브러리가 Word2Vec과 FastText 훈련 기능을 깔끔하게 제공하는 것과 달리, GloVe 훈련을 위한 간단하고 표준적인 파이썬 라이브러리는 상대적으로 부족하다. 보통 스탠포드에서 제공하는 C 언어 기반의 원본 코드를 컴파일해서 사용해야 하는 등, 과정이 훨씬 더 번거롭다.\n",
        "\n",
        "## Word2Vec/FastText가 더 편리한 이유\n",
        "반면, Word2Vec과 FastText는 전체 통계 행렬을 미리 만들 필요 없이, 텍스트를 처음부터 끝까지 한번 훑어 내려가면서(on-the-fly) 주변 단어들의 관계를 학습한다. 이 방식은 gensim 라이브러리에 매우 효율적으로 구현되어 있어, 연구자가 직접 준비한 한문 텍스트 뭉치를 단 몇 줄의 코드로 쉽고 빠르게 학습시킬 수 있다.\n",
        "\n",
        "## 결론\n",
        "개념적으로는 GloVe도 한문 텍스트에 적용 가능하고, 만약 대규모 한문 코퍼스로 잘 훈련시킨다면 매우 뛰어난 성능의 '한문 GloVe 모델'을 만들 수 있다.\n",
        "\n",
        "하지만 대부분의 연구자가 자신의 연구 자료(예: 특정 작가의 문집, 특정 시기 사료)를 가지고 직접 모델을 만들어 분석하는 상황에서는, 구현이 훨씬 간편하고 빠른 Word2Vec이나 FastText가 더 현실적이고 편리한 선택이다."
      ],
      "metadata": {
        "id": "H_tyWawM13LH"
      }
    }
  ]
}