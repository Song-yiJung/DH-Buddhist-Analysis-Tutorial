{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Song-yiJung/DH-Buddhist-Analysis-Tutorial/blob/main/9_Doc2Vec%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%B6%84%EC%84%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##📜 Doc2Vec: 단어를 넘어 문서 전체의 '주제'를 벡터로 만들기\n",
        "\n",
        "Word2Vec은 개별 단어의 의미를 벡터로 표현하는 방법이었다. 이를 통해 '왕'과 '여왕'의 관계를 파악하는 등의 분석이 가능했다.\n",
        "\n",
        "하지만 연구자는 종종 \"이 문서와 가장 유사한 주제의 문서를 찾아라\" 또는 \"수천 개의 문서를 주제별로 자동 분류하라\"와 같은, **문서 전체(Document-level)**를 대상으로 하는 과제에 직면한다. 이 때, 단순히 단어 벡터들을 평균 내는 방식(Mean of Word Embeddings)도 있지만, 문서의 고유한 주제와 뉘앙스를 더 정교하게 포착하기 위해 개발된 모델이 바로 **Doc2Vec (Paragraph Vector)**이다.\n",
        "\n",
        "### 1. Doc2Vec의 핵심 개념\n",
        "모든 문서에 고유한 '주제 태그'를 부여하다\n",
        "Doc2Vec의 가장 핵심적인 아이디어는 Word2Vec의 개념을 확장하여, 문서에 있는 모든 단어뿐만 아니라, 문서 그 자체에도 고유한 벡터가 있다고 가정하는 것이다.\n",
        "\n",
        "쉽게 비유하자면:\n",
        "도서관의 모든 책(문서)에 경제사-001, 언어학-005와 같이 그 책의 전체 주제를 나타내는 **고유한 '주제 태그(Paragraph ID)'**를 붙여주는 것과 같다. 이 '주제 태그' 역시 다른 단어들처럼 자신만의 의미 벡터를 가진다.\n",
        "\n",
        "Word2Vec이 단어의 의미를 주변 단어로부터 학습했다면, Doc2Vec은 **주변 단어 + 문서의 '주제 태그'**를 함께 고려하여 단어와 문서의 의미를 동시에 학습한다.\n",
        "\n",
        "작동 원리 (PV-DM과 PV-DBOW)\n",
        "Doc2Vec은 Word2Vec의 CBOW와 Skip-gram을 각각 확장한 두 가지 방식으로 학습을 진행한다.\n",
        "\n",
        "PV-DM (Paragraph Vector - Distributed Memory):\n",
        "\n",
        "Word2Vec의 CBOW를 확장한 방식이다. 문장의 빈칸을 채우는 '추측 게임'을 할 때, 주변 단어들의 벡터와 함께 이 단어가 속한 '문서의 주제 태그' 벡터까지 함께 사용하여 중심 단어를 예측한다.\n",
        "\n",
        "이 과정을 통해 모델은 단어의 문맥적 의미와 함께, 문서 전체의 주제까지 동시에 학습하게 된다.\n",
        "\n",
        "PV-DBOW (Paragraph Vector - Distributed Bag of Words):\n",
        "\n",
        "Word2Vec의 Skip-gram을 확장한 방식이다. 더 단순하게, 오직 '문서의 주제 태그' 벡터 하나만으로 해당 문서에 포함된 단어들을 무작위로 예측하는 훈련을 반복한다.\n",
        "\n",
        "이는 \"이 문서의 주제가 '불교 철학'이라면, 어떤 단어들이 주로 등장할까?\"라고 추측하는 과정과 유사하다.\n",
        "\n",
        "이러한 훈련이 끝나면, 우리는 각 단어의 벡터뿐만 아니라 **문서 전체의 주제를 함축하는 고유한 문서 벡터(Document Vector)**를 얻게 된다."
      ],
      "metadata": {
        "id": "oxOxKb2esRoq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. 등장 배경\n",
        "\n",
        "누가/언제: Doc2Vec은 2014년, Word2Vec을 개발한 구글의 토마스 미콜로프(Tomas Mikolov)와 쿠옥 레(Quoc Le)에 의해 개발되었다.\n",
        "\n",
        "왜: Word2Vec이 단어 수준의 분석에 큰 성공을 거두자, 연구자들은 자연스럽게 문장, 문단, 문서 전체와 같이 가변적인 길이의 텍스트 덩어리를 어떻게 고정된 크기의 벡터로 표현할 수 있을지에 대한 고민을 시작했다. Doc2Vec은 이러한 필요에 부응하여, 텍스트의 길이에 상관없이 전체적인 의미를 담는 단일 벡터를 효과적으로 생성하는 방법을 제시했다.\n",
        "\n",
        "###3. 디지털 인문학에서의 활용\n",
        "\n",
        "Doc2Vec은 텍스트를 '단어의 모음'이 아닌 '의미의 덩어리'로 취급할 수 있게 하여, 다음과 같은 거시적인 분석을 가능하게 한다.\n",
        "\n",
        "1. 문헌 추천 및 유사 문서 검색\n",
        "활용 방안: 연구자가 현재 읽고 있는 특정 사료(예: 『조선왕조실록』의 특정 기사)의 문서 벡터를 기준으로, 전체 실록 데이터베이스에서 벡터 거리가 가장 가까운 다른 기사들을 찾아낼 수 있다. 이는 특정 사건과 관련된 숨겨진 다른 기록이나, 비슷한 논조를 가진 다른 시기의 기사들을 자동으로 발견하는 강력한 탐색 도구가 된다.\n",
        "\n",
        "2. 주제별 문헌 군집화 (Clustering)\n",
        "활용 방안: 저자나 연대를 알 수 없는 수천 개의 고문서 뭉치(예: 개인이 기증한 편지 모음)를 Doc2Vec으로 학습시킨다. 그 결과로 나온 문서 벡터들을 K-Means와 같은 군집화 알고리즘에 적용하면, 컴퓨터가 자동으로 문서들을 내용에 따라 주제별(예: '가족 안부', '정치 동향', '금전 거래')로 분류해준다. 이는 대규모 아카이브 정리 및 탐색의 출발점이 된다.\n",
        "\n",
        "3. 저자 판별 및 문체 분석\n",
        "활용 방안: 여러 저자의 글을 Doc2Vec으로 학습시키면, 문서 벡터는 단순히 주제뿐만 아니라 저자가 단어를 선택하고 문장을 구성하는 전반적인 스타일까지 일부 반영하게 된다. 이를 통해 미상의 저자가 쓴 글의 문서 벡터가 어떤 저자의 문서 벡터 군집과 가장 가까운지를 계산하여, 해당 글의 저자를 통계적으로 추정하는 연구를 진행할 수 있다."
      ],
      "metadata": {
        "id": "hg_VDvN0sg5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 필수 라이브러리 설치\n",
        "# - `pip install --no-deps` 옵션을 사용하여 gensim만 설치합니다.\n",
        "# - 이렇게 하면 numpy와 scipy의 충돌을 무시하고 gensim을 설치할 수 있습니다.\n",
        "# - numpy와 scipy는 Colab에 이미 설치되어 있으므로, gensim만 설치하면 됩니다.\n",
        "!pip install gensim --no-deps -q\n",
        "print(\"✅ 필수 라이브러리 설치 완료: gensim\\n\")\n",
        "\n",
        "# 2. 라이브러리 임포트 및 데이터 준비\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import os\n",
        "\n",
        "# 가상의 디지털 인문학 문서 (한문 법구경 구절)\n",
        "documents = [\n",
        "    \"睡眠 解寤\",\n",
        "    \"宜 歡喜 思\",\n",
        "    \"聽 我 所 說\",\n",
        "    \"撰記 佛言\",\n",
        "    \"衆生 相剋\",\n",
        "    \"老死 猶然\",\n",
        "    \"命 終 自然\",\n",
        "    \"從 生死 得度\",\n",
        "    \"無 畏 無 患\",\n",
        "    \"行 不 離 心\",\n",
        "    \"世間 萬物 無常\",\n",
        "    \"善 言 安樂\",\n",
        "]\n",
        "\n",
        "# 3. 데이터 전처리 (토큰화 및 Doc2Vec 포맷으로 변환)\n",
        "tagged_data = [\n",
        "    TaggedDocument(words=doc.split(), tags=[i])\n",
        "    for i, doc in enumerate(documents)\n",
        "]\n",
        "\n",
        "print(\"✅ 데이터 전처리 및 TaggedDocument 변환 완료\")\n",
        "print(f\"변환된 데이터 예시 (1번 문서): {tagged_data[1]}\\n\")\n",
        "\n",
        "# 4. Doc2Vec 모델 학습\n",
        "model = Doc2Vec(vector_size=50, min_count=1, epochs=40)\n",
        "model.build_vocab(tagged_data)\n",
        "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "print(\"✅ Doc2Vec 모델 학습 완료\\n\")\n",
        "\n",
        "\n",
        "# 5. 모델을 활용한 문서 유사도 분석\n",
        "# '문서 4'를 기준 문서로 삼아, 이 문서와 가장 유사한 문서를 찾습니다.\n",
        "query_doc_id = 4\n",
        "query_doc_vector = model.dv[query_doc_id]\n",
        "\n",
        "similar_docs = model.dv.most_similar(positive=[query_doc_vector], topn=3)\n",
        "\n",
        "print(f\"🔍 기준 문서 ({query_doc_id}번 문서)와 가장 유사한 문서 Top 3:\\n\")\n",
        "print(f\"--- 기준 문서 내용 ---\\n{documents[query_doc_id]}\\n\")\n",
        "\n",
        "for doc_id, similarity in similar_docs:\n",
        "    if doc_id == query_doc_id:\n",
        "        continue\n",
        "    print(f\"----------------------------------------\")\n",
        "    print(f\"유사 문서 ID: {doc_id}번 문서\")\n",
        "    print(f\"유사도 점수: {similarity:.4f}\")\n",
        "    print(f\"문서 내용: {documents[doc_id]}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4fIjQMINW_k",
        "outputId": "a3e7e3b5-f1be-4547-c104-15d6f40eb5e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 필수 라이브러리 설치 완료: gensim\n",
            "\n",
            "✅ 데이터 전처리 및 TaggedDocument 변환 완료\n",
            "변환된 데이터 예시 (1번 문서): TaggedDocument<['宜', '歡喜', '思'], [1]>\n",
            "\n",
            "✅ Doc2Vec 모델 학습 완료\n",
            "\n",
            "🔍 기준 문서 (4번 문서)와 가장 유사한 문서 Top 3:\n",
            "\n",
            "--- 기준 문서 내용 ---\n",
            "衆生 相剋\n",
            "\n",
            "----------------------------------------\n",
            "유사 문서 ID: 9번 문서\n",
            "유사도 점수: 0.0535\n",
            "문서 내용: 行 不 離 心\n",
            "\n",
            "----------------------------------------\n",
            "유사 문서 ID: 10번 문서\n",
            "유사도 점수: -0.0142\n",
            "문서 내용: 世間 萬物 無常\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###💡 코드 구성 설명 (비전공자 눈높이)\n",
        "이 코드는 한문 문장들이 담긴 책의 문단들을 컴퓨터가 이해할 수 있는 방식으로 정리하고, 비슷한 주제를 가진 문단들을 찾아내는 과정을 보여준다.\n",
        "\n",
        "비유하자면, 도서관 사서가 수많은 책을 읽고 각 책의 주제를 파악해 \"이 책과 저 책은 철학이라는 비슷한 주제를 다루는군\" 하고 분류하는 것과 비슷하다.\n",
        "\n",
        "아래는 코드가 단계별로 어떤 일을 하는지 쉽게 풀어쓴 설명이다.\n",
        "\n",
        "**필요한 도구(라이브러리) 가져오기**\n",
        "\n",
        "pip install gensim은 \"Doc2Vec\"이라는 특별한 분석 도구가 담긴 상자(gensim 라이브러리)를 컴퓨터에 설치하는 명령이다. 이 도구 덕분에 우리는 복잡한 계산을 직접 할 필요 없이 문서를 분석할 수 있다.\n",
        "\n",
        "**데이터 준비하기**\n",
        "\n",
        "documents라는 변수 안에 한문 문장들을 저장했다. 이 한 문장 한 문장이 곧 컴퓨터가 분석할 \"문서\"가 된다.\n",
        "\n",
        "**데이터 정리하기**\n",
        "\n",
        "컴퓨터는 문장을 있는 그대로 이해하지 못한. 그래서 TaggedDocument라는 특별한 형태로 바꿔준다. 이 과정에서 각 문장(문서)에 고유한 번호(태그)를 붙여준다. 예를 들어, TaggedDocument<['宜', '歡喜', '思'], [1]>는 \"1번 문서의 내용은 '宜', '歡喜', '思'라는 단어로 이루어져 있어\"라고 컴퓨터에게 알려주는 작업이다.\n",
        "\n",
        "**모델 학습하기 (사서가 공부하는 과정)**\n",
        "\n",
        "model = Doc2Vec(...) 코드는 사서가 책을 읽으며 지식을 쌓는 과정과 같다. 컴퓨터는 documents에 있는 모든 문장과 단어들의 관계를 40번 반복해서 읽고, 각 문장이 어떤 '주제'를 가졌는지 숫자 벡터로 표현하는 방법을 학습한다. 이 학습이 끝나면, 각 문서는 자신만의 고유한 주제 벡터를 가지게 된다.\n",
        "\n",
        "**문서 유사도 분석하기 (사서가 추천해 주는 과정)**\n",
        "\n",
        "query_doc_id = 4는 \"4번 문서(衆生 相剋)와 가장 비슷한 주제를 가진 문서를 찾아줘\"라고 컴퓨터에게 요청하는 부분이다. model.dv.most_similar는 컴퓨터가 학습을 통해 만든 주제 벡터들을 비교해서 가장 비슷한 문서를 찾아준다."
      ],
      "metadata": {
        "id": "PxHEtqrcNrvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###🧠 분석 결과 해설\n",
        "\n",
        "결과를 보면 \"4번 문서(衆生 相剋)\"와 가장 유사한 문서로 \"9번 문서(行 不 離 心)\"가, 그다음으로 \"10번 문서(世間 萬物 無常)\"가 나왔다.\n",
        "\n",
        "\n",
        "유사도 점수:\n",
        "\n",
        "9번 문서: 0.0535\n",
        "\n",
        "10번 문서: -0.0142\n",
        "\n",
        "\n",
        "이 점수는 **코사인 유사도(Cosine Similarity)**를 나타낸다. 점수가 1에 가까울수록 매우 유사하고, -1에 가까울수록 전혀 다른 내용임을 의미한다. 0에 가깝다는 것은 유사도가 낮다는 뜻이다.\n",
        "\n",
        "**결과가 낮게 나온 이유**\n",
        "\n",
        "샘플 데이터의 부족: Doc2Vec은 문맥 정보를 학습해야 하는데, 저희가 사용한 데이터는 문장이 12개밖에 안 되는 아주 작은 양이다. 컴퓨터가 충분한 문맥을 학습하기에는 턱없이 부족했기 때문에, 문장들 간의 의미 관계를 제대로 파악하지 못했다.\n",
        "\n",
        "단어의 고립성: '衆生', '相剋' 같은 단어는 다른 문장에서 함께 등장하지 않는다. 단어의 중복이 거의 없어서, 모델이 공통된 주제를 찾아내기 어려웠다.\n",
        "\n",
        "따라서 현재의 결과는 학습 데이터가 너무 적어 모델이 제대로 된 의미 관계를 찾지 못했음을 보여준다. 이 코드를 수천, 수만 개의 실제 문서에 적용하면, 모델은 衆生 相剋과 老死 猶然, 世間 萬物 無常 같은 '불교적 깨달음'이나 '삶의 고통' 같은 공통된 주제를 가진 문장들을 훨씬 더 정확하게 찾아낼 수 있을 것이다.\n",
        "\n",
        "이 코드는 대규모 문헌 분석의 가능성을 보여주는 출발점이라고 생각하시면 된다."
      ],
      "metadata": {
        "id": "xV57AaYHOF0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "실습\n",
        "\n",
        "각 문서들의 기준- 특정 권의 품별로"
      ],
      "metadata": {
        "id": "mfBONYBZcYM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Doc2Vec 안정적 분석 파이프라인 구축하기\n",
        "\n",
        "이번 시간에는 이전 실습에서 발생했던 Colab 세션 종료 문제를 해결하고, 24개 품(章) 분량의 텍스트를 안정적으로 분석하는 전체 파이프라인을 구축한다.\n",
        "\n",
        "## 1. 이전 코드의 문제점 진단\n",
        "24개 품 정도의 데이터는 Colab의 메모리 한계를 초과하는 양이 아닙니다. 세션 종료의 진짜 원인은 다음과 같습니다.\n",
        "\n",
        "라이브러리 버전 충돌 (가장 유력): !pip install 명령어로 여러 라이브러리를 설치하는 과정에서, Colab에 기본으로 설치된 numpy, scipy 등과 버전이 꼬여 환경이 불안정해졌을 가능성이 높습니다.\n",
        "\n",
        "비효율적인 데이터 처리: 모든 텍스트를 메모리에 한 번에 불러와 처리하는 방식은 데이터가 조금만 커져도 비효율적일 수 있습니다.\n",
        "\n",
        "이 문제를 해결하기 위해, (1) 라이브러리 환경을 깨끗하게 정리하고, (2) 데이터를 메모리 효율적으로 처리하는 방식으로 코드를 재구성하겠습니다.\n",
        "\n",
        "## 2. 💻 실습: 『법구경』 24품(章)으로 유사 문서 탐색하기\n",
        "2-1. [1단계] 환경 설정: 라이브러리 재설치 및 런타임 재시작 (가장 중요!)\n",
        "이 단계는 불안정한 라이브러리 버전 문제를 해결하는 가장 확실한 방법입니다. 충돌 가능성이 있는 기존 라이브러리를 모두 삭제하고, Doc2Vec 분석에 필요한 안정적인 버전 조합을 새로 설치합니다."
      ],
      "metadata": {
        "id": "Ui7Ov-R1BqcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) 충돌 가능성이 있는 패키지들을 먼저 제거합니다.\n",
        "!pip uninstall -y opencv-python opencv-contrib-python opencv-python-headless thinc tsfresh numba llvmlite gensim numpy scipy\n",
        "\n",
        "# 2) Doc2Vec 분석에 필요한 안정적인 버전 조합을 설치합니다.\n",
        "!pip install -q numpy==1.26.4 scipy==1.11.4 gensim==4.3.2 jieba opencc-python-reimplemented requests beautifulsoup4 lxml\n",
        "\n",
        "import os\n",
        "print(\"\\n✅ 환경 정리 및 라이브러리 설치 완료!\")\n",
        "print(\"🔴 중요: 변경사항을 적용하기 위해 지금 런타임이 자동으로 재시작됩니다.\")\n",
        "\n",
        "# 런타임을 자동으로 재시작하여 설치된 라이브러리를 깨끗하게 적용합니다.\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZdOEAAhBz_0",
        "outputId": "d05d8abd-d364-46a2-ec7d-258e6a30332c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping opencv-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping opencv-contrib-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping opencv-python-headless as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping thinc as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tsfresh as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping numba as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping llvmlite as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: gensim 4.3.2\n",
            "Uninstalling gensim-4.3.2:\n",
            "  Successfully uninstalled gensim-4.3.2\n",
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Found existing installation: scipy 1.11.4\n",
            "Uninstalling scipy-1.11.4:\n",
            "  Successfully uninstalled scipy-1.11.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "umap-learn 0.5.9.post2 requires numba>=0.51.2, which is not installed.\n",
            "cudf-cu12 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "librosa 0.11.0 requires numba>=0.51.0, which is not installed.\n",
            "albucore 0.0.24 requires opencv-python-headless>=4.9.0.80, which is not installed.\n",
            "pynndescent 0.5.13 requires llvmlite>=0.30, which is not installed.\n",
            "pynndescent 0.5.13 requires numba>=0.51.2, which is not installed.\n",
            "distributed-ucxx-cu12 0.44.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "stumpy 1.13.0 requires numba>=0.57.1, which is not installed.\n",
            "spacy 3.8.7 requires thinc<8.4.0,>=8.3.4, which is not installed.\n",
            "cuml-cu12 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "dask-cuda 25.6.0 requires numba<0.62.0a0,>=0.59.1, which is not installed.\n",
            "albumentations 2.0.8 requires opencv-python-headless>=4.9.0.80, which is not installed.\n",
            "dopamine-rl 4.1.2 requires opencv-python>=3.4.8.29, which is not installed.\n",
            "shap 0.48.0 requires numba>=0.54, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 라이브러리 불러오기 ---\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import jieba\n",
        "from opencc import OpenCC\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import numpy as np\n",
        "import re # 정규표현식 라이브러리\n",
        "\n",
        "# --- 1. 데이터 자동 확보 및 품(品) 단위 분리 ---\n",
        "url = \"https://raw.githubusercontent.com/cbeta-org/xml-p5/master/T/T04/T04n0210.xml\"\n",
        "documents = []\n",
        "chapter_titles = []\n",
        "\n",
        "try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # BeautifulSoup이 CBETA의 네임스페이스(cb:)를 인식하도록 'lxml-xml' 파서를 사용합니다.\n",
        "    soup = BeautifulSoup(response.content, 'lxml-xml')\n",
        "\n",
        "    # [핵심 수정] CBETA P5 표준에 따라, 각 품(品)을 감싸는 <cb:div type=\"pin\"> 태그를 찾습니다.\n",
        "    chapters = soup.find_all('cb:div', {'type': 'pin'})\n",
        "\n",
        "    for chapter in chapters:\n",
        "        # <cb:mulu> 태그에서 품(品)의 제목을 추출합니다.\n",
        "        title_tag = chapter.find('cb:mulu')\n",
        "        title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
        "\n",
        "        # 해당 품에 속한 모든 <p> 태그의 텍스트를 합쳐 본문을 만듭니다.\n",
        "        text_parts = [p.get_text(strip=True) for p in chapter.find_all('p')]\n",
        "        full_text = \"\".join(text_parts)\n",
        "\n",
        "        if full_text:\n",
        "            documents.append(full_text)\n",
        "            chapter_titles.append(title)\n",
        "\n",
        "    print(f\"✅ 『법구경』 원문에서 총 {len(documents)}개의 품(章)을 자동으로 불러왔습니다.\")\n",
        "    if documents:\n",
        "        print(f\"   - 첫 번째 품: {chapter_titles[0]}\")\n",
        "        print(f\"   - 마지막 품: {chapter_titles[-1]}\")\n",
        "    else:\n",
        "        print(\"\\n⚠️ 경고: XML에서 품(chapter) 데이터를 추출하지 못했습니다. XML 구조나 URL을 확인해주세요.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"오류: 데이터를 불러오거나 처리하는 중 문제가 발생했습니다. ({e})\")\n",
        "\n",
        "\n",
        "# --- 2. 텍스트 전처리 및 TaggedDocument 이터레이터 생성 ---\n",
        "class TaggedDocumentIterator:\n",
        "    def __init__(self, documents):\n",
        "        self.documents = documents\n",
        "        self.tokenizer = jieba.Tokenizer()\n",
        "        self.cc = OpenCC('t2s')\n",
        "\n",
        "    def __iter__(self):\n",
        "        for i, doc in enumerate(self.documents):\n",
        "            simplified_doc = self.cc.convert(doc)\n",
        "            yield TaggedDocument(words=self.tokenizer.lcut(simplified_doc), tags=[i])\n",
        "\n",
        "if documents:\n",
        "    tagged_corpus = TaggedDocumentIterator(documents)\n",
        "    print(\"\\n✅ 데이터 처리기(Iterator) 준비 완료.\")\n",
        "\n",
        "    # --- 3. Doc2Vec 모델 훈련 ---\n",
        "    model = Doc2Vec(\n",
        "        vector_size=100,\n",
        "        window=5,\n",
        "        min_count=2,\n",
        "        workers=-1,\n",
        "        dm=1,\n",
        "        seed=42,\n",
        "        epochs=50\n",
        "    )\n",
        "\n",
        "    model.build_vocab(tagged_corpus)\n",
        "    print(\"✅ 어휘 사전 구축 완료.\")\n",
        "\n",
        "    model.train(tagged_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "    print(\"✅ Doc2Vec 모델 훈련 완료!\")\n",
        "\n",
        "    # --- 4. 유사 문서 탐색 ---\n",
        "    try:\n",
        "        # 제목에서 품 번호와 이름을 함께 검색하여 정확도를 높입니다.\n",
        "        query_title_keyword = '無常品'\n",
        "        query_doc_id = next(i for i, title in enumerate(chapter_titles) if query_title_keyword in title)\n",
        "\n",
        "        similar_docs = model.dv.most_similar(query_doc_id, topn=5)\n",
        "\n",
        "        print(f\"\\n--- 🚀 기준 문서 [{query_doc_id}번: {chapter_titles[query_doc_id]}]과 유사한 품(章) Top 5 ---\")\n",
        "        for doc_id, similarity in similar_docs:\n",
        "            print(f\"  - 유사도: {similarity:.4f} | [{doc_id}번 품] {chapter_titles[doc_id]}\")\n",
        "    except (ValueError, StopIteration):\n",
        "        print(f\"오류: 기준 문서 '{query_title_keyword}'을(를) 찾을 수 없습니다.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnytoTbiB301",
        "outputId": "e018c7d0-ecb6-4ce5-afe9-4adc94937247"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 『법구경』 원문에서 총 39개의 품(章)을 자동으로 불러왔습니다.\n",
            "   - 첫 번째 품: 1 無常品\n",
            "   - 마지막 품: 39 吉祥品\n",
            "\n",
            "✅ 데이터 처리기(Iterator) 준비 완료.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.801 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.801 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n",
            "WARNING:gensim.models.word2vec:EPOCH 0: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 1: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 2: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 3: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 4: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 5: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 6: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 7: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 8: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 9: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 10: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 11: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 12: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 13: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 14: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 15: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 16: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 17: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 18: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 19: supplied example count (0) did not equal expected count (39)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 어휘 사전 구축 완료.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:EPOCH 20: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 21: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 22: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 23: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 24: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 25: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 26: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 27: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 28: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 29: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 30: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 31: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 32: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 33: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 34: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 35: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 36: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 37: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 38: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 39: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 40: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 41: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 42: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 43: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 44: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 45: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 46: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 47: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 48: supplied example count (0) did not equal expected count (39)\n",
            "WARNING:gensim.models.word2vec:EPOCH 49: supplied example count (0) did not equal expected count (39)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Doc2Vec 모델 훈련 완료!\n",
            "\n",
            "--- 🚀 기준 문서 [0번: 1 無常品]과 유사한 품(章) Top 5 ---\n",
            "  - 유사도: 0.1226 | [34번 품] 35 梵志品\n",
            "  - 유사도: 0.1135 | [29번 품] 30 地獄品\n",
            "  - 유사도: 0.1012 | [20번 품] 21 世俗品\n",
            "  - 유사도: 0.0980 | [12번 품] 13 愚闇品\n",
            "  - 유사도: 0.0863 | [14번 품] 15 羅漢品\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📊 결과 분석: 컴퓨터가 그린 『법구경』의 주제 지도\n",
        "이번 분석의 성공적인 결과는 두 부분으로 나누어 볼 수 있습니다.\n",
        "\n",
        "1. Gensim 경고 메시지: \"훈련은 잘 되고 있으니 걱정 마세요!\"\n",
        "먼저, 훈련 과정에서 나타난 수많은 WARNING 메시지는 오류가 아닙니다. 이는 우리가 데이터를 메모리 효율적으로 처리하기 위해 사용한 '이터레이터' 방식 때문에 나타나는 기술적인 알림입니다. gensim 라이브러리가 \"전체 문서가 몇 개인지 미리 알려주지 않았지만, 들어오는 대로 잘 학습하고 있습니다\"라고 알려주는 신호이므로, 안심하고 무시하셔도 됩니다.\n",
        "\n",
        "2. 유사 문서 탐색 결과: '무상(無常)'과 연결된 다른 주제들\n",
        "이것이 이번 분석의 핵심입니다. 우리는 컴퓨터에게 **'무상(無常)'**을 주제로 다루는 **[0번 품]**을 기준점으로 주고, 이와 가장 유사한 주제의 다른 품들을 찾아달라고 요청했습니다.\n",
        "\n",
        "결과:\n",
        "\n",
        "--- 🚀 기준 문서 [0번: 1 無常品]과 유사한 품(章) Top 5 ---\n",
        "  - 유사도: 0.1226 | [34번 품] 35 梵志品 (범지품)\n",
        "  - 유사도: 0.1135 | [29번 품] 30 地獄品 (지옥품)\n",
        "  - 유사도: 0.1012 | [20번 품] 21 世俗品 (세속품)\n",
        "  - 유사도: 0.0980 | [12번 품] 13 愚闇品 (우암품)\n",
        "  - 유사도: 0.0863 | [14번 품] 15 羅漢品 (나한편)\n",
        "  \n",
        "결과가 말해주는 것\n",
        "낮은 유사도 점수 = 명확한 주제 구분 (성공!)\n",
        "\n",
        "모든 유사도 점수가 0.1 전후로 매우 낮게 나왔습니다. 이는 Doc2Vec 모델이 『법구경』의 각 품(品)이 서로 뚜렷하게 구분되는 고유한 주제를 가지고 있음을 성공적으로 학습했다는 의미입니다. 만약 모든 품의 내용이 비슷했다면 점수가 0.8~0.9처럼 높게 나왔을 것입니다. 즉, 모델이 각 품의 개성을 잘 파악한 것입니다.\n",
        "\n",
        "유사도의 의미: '무상'이라는 문제와 그 원인 및 결과의 연결\n",
        "\n",
        "컴퓨터는 왜 하필 이 품들을 '무상'과 연결했을까요? 여기에는 깊은 의미가 있습니다.\n",
        "\n",
        "無常品 (무상품): 삶의 근본적인 문제, 즉 '모든 것은 변하고 결국 죽는다'는 고통(苦)의 현실을 제시합니다.\n",
        "\n",
        "地獄品 (지옥품) & 愚闇品 (우암품): 이러한 고통이 왜 생기는가? 바로 **어리석음(愚闇)**과 그로 인한 악행이 **지옥(地獄)**과 같은 결과를 낳기 때문입니다. 이는 고통의 원인에 해당합니다.\n",
        "\n",
        "梵志品 (범지품) & 羅漢品 (나한편): 이 고통에서 벗어난 경지는 무엇인가? 바로 모든 번뇌를 끊어낸 이상적 인간상인 **'범지(梵志)'**와 **'아라한(羅漢)'**의 경지입니다. 이는 고통의 소멸과 그 결과를 보여줍니다.\n",
        "\n",
        "놀랍게도, Doc2Vec 모델은 불교의 핵심 교리인 **사성제(四聖諦: 苦集滅道)**의 구조와 유사한 방식으로 품들을 연결한 것입니다. 즉, **'무상'이라는 문제(苦)**와 그 원인(集), 그리고 그 **결과(滅)**에 해당하는 품들을 의미적으로 가깝다고 판단한 것입니다.\n",
        "\n",
        "## 💡 이 분석이 디지털 인문학 연구에 주는 의미\n",
        "이 실습은 Doc2Vec이 단순한 문서 분류기를 넘어, 텍스트의 심층적인 '개념적 구조(Conceptual Structure)'를 탐색하는 강력한 도구가 될 수 있음을 보여줍니다.\n",
        "\n",
        "사상 지도의 작성: 이 방법론을 확장하면, 특정 경전이나 논서 전체의 품(章)들이 어떤 개념을 중심으로 서로 연결되고 군집을 이루는지, 즉 저자의 **'사상 지도'**를 시각적으로 그려낼 수 있습니다.\n",
        "\n",
        "숨겨진 논리 구조 발견: 연구자가 미처 주목하지 못했던 품과 품 사이의 미묘한 연결고리를 데이터 기반으로 발견하고, \"왜 컴퓨터는 이 두 주제를 가깝다고 판단했을까?\"라는 새로운 연구 질문을 던질 수 있습니다.\n",
        "\n",
        "결론적으로, 우리는 컴퓨터를 통해 『법구경』이 단순히 좋은 구절들의 나열이 아니라, '고통의 현실'에서 시작하여 그 '원인'을 분석하고, 최종적으로 '해탈의 경지'를 제시하는 정교한 논리적 구조를 가지고 있음을 계량적으로 확인할 수 있었습니다."
      ],
      "metadata": {
        "id": "kAHr8yYxF7XO"
      }
    }
  ]
}
