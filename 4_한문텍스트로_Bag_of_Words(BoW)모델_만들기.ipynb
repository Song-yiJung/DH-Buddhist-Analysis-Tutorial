{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqzlDx7mvB1x25uxtIq4d3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Song-yiJung/DH-Buddhist-Analysis-Tutorial/blob/main/4_%ED%95%9C%EB%AC%B8%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%A1%9C_Bag_of_Words(BoW)%EB%AA%A8%EB%8D%B8_%EB%A7%8C%EB%93%A4%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 한문 텍스트로 Bag-of-Words(BoW) 모델👜 만들기\n",
        "\n",
        "## **Bag-of-Words(BoW): '단어 가방 모델', 컴퓨터가 글을 이해하도록 돕는 간단하고 효과적인 방법**\n",
        "\n",
        "* 문장이나 문서에 어떤 단어가 몇 번 등장했는지만을 여기는 텍스트 표현 방법.\n",
        "\n",
        "* 왜 단어 가방 모델일까? 단어들의 순서는 전혀 고려하지 않고, 모든 단어를 하나의 **가방**에 넣은 뒤 각 단어의 출현 빈도(frequency)를 세는 방식이기 때문이다.\n",
        "\n",
        "## 핵심원리\n",
        "\n",
        "* 어휘 사전 생성 (Vocabulary Building): 가지고 있는 모든 문서의 모든 단어를 중복 없이 모아 하나의 '어휘 사전'을 만든다.\n",
        "* 벡터화 (Vectorization): 각 문서를 어휘 사전에 있는 단어들의 등장 횟수를 기준으로 숫자 벡터(Vector)로 변환한다.\n"
      ],
      "metadata": {
        "id": "Snxi52-0J5mJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [실습] 컴퓨터는 어떻게 글자를 숫자로 바꾸나?"
      ],
      "metadata": {
        "id": "W1IYj96Va4Dp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku9bWnYIJP7k"
      },
      "outputs": [],
      "source": [
        "# 1. 도구 준비하기\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "도구 준비하기\n",
        "* 짧은 한글 문장을 숫자의 배열(벡터)로 바꾸는 과정.\n",
        "* 데이터 분석 도구 상자인 sklearn에서 CountVectorizer(단어 계수기:문장 속 단어의 개수를 세어주는 전문 도구) 도구  불러오기(import )    "
      ],
      "metadata": {
        "id": "tjbSTWCYbE1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 분석할 문장 준비하기\n",
        "documents = [\n",
        "    \"나는 오늘 사과를 먹었다\",\n",
        "    \"나는 오늘 축구를 했다\"\n",
        "]"
      ],
      "metadata": {
        "id": "nSuTaB7_X-CI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 단어 계수기 작동 준비\n",
        "vectorizer = CountVectorizer()"
      ],
      "metadata": {
        "id": "kH5-HzMKdpHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**단어 계수기** 작동 준비\n",
        "\n",
        "* 위에서 가져온 CountVectorizer라는 '단어 계수기' 도구를 실제로 사용할 수 있도록 vectorizer라는 이름으로 하나 복사해서 준비시키는 과정\n",
        "\n",
        "* 이제 vectorizer를 통해 단어를 세고 문장을 숫자로 바꿀 수 있다."
      ],
      "metadata": {
        "id": "y-klQGv5drSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 학습과 변환\n",
        "bow_matrix = vectorizer.fit_transform(documents)"
      ],
      "metadata": {
        "id": "iTLhqEmJd7qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습과 변환\n",
        "* 학습 (fit): vectorizer가 우리가 준 documents의 모든 문장을 훑어보면서 어떤 종류의 단어들이 있는지 목록을 만든다. 이걸 '어휘 사전(Vocabulary)'이라고 부른다.\n",
        "* 변환 (transform): 위에서 만든 '어휘 사전'을 기준으로, 각 문장에 어떤 단어가 몇 번씩 들어있는지 숫자를 센다. 그리고 그 결과를 바탕으로 각 문장을 숫자 배열(벡터)로 변환한다."
      ],
      "metadata": {
        "id": "mcSNqNBreCIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 결과 확인하기\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "print(\"1단계: 생성된 어휘 사전\")\n",
        "print(vocabulary)"
      ],
      "metadata": {
        "id": "B3bDuWFNeNXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "출력: ['나는' '먹었다' '사과를' '오늘' '축구를' '했다']\n",
        "\n",
        "\"네가 만든 '어휘 사전' 좀 보여줘\" 라고 vectorizer에게 요청하는 코드입니다. 그 결과, documents에서 찾아낸 6개의 고유한 단어 목록을 보여줍니다. 이 목록의 순서가 매우 중요합니다. 왜냐하면 이 순서가 바로 아래에서 볼 숫자 배열의 각 자리와 일치하기 때문이다."
      ],
      "metadata": {
        "id": "Hfj-jlLMeQpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n----------------------------------------\\n\")\n",
        "print(\"2단계: 변환된 벡터\")\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "id": "BtVntlFQeTdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"숫자로 변환한 최종 결과를 보기 쉽게 표(행렬) 형태로 보여줘\" 라는 코드\n"
      ],
      "metadata": {
        "id": "z_KNS8R7eVon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "사람이 쓰는 '문장'을 단어의 등장 횟수를 기준으로 컴퓨터가 이해할 수 있는 '숫자 신호'로 깔끔하게 번역해주는 과정이다."
      ],
      "metadata": {
        "id": "CT9GOH6SeYuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📚 Bag-of-Words(BoW) 모델의 기원: 누가, 언제, 왜?\n",
        "\n",
        "BoW는 특정 한 명이 \"발명\"한 모델이라기보다는, 특정 문제를 해결하기 위해 자연스럽게 발전한 개념에 가깝다.\n",
        "\n",
        "* 누가/언제 (Who/When): BoW의 근본적인 아이디어는 1950년대부터 정보 검색(Information Retrieval) 분야에서 등장하기 시작했다. 컴퓨터 과학의 초기 연구자들이 어떻게 하면 방대한 문서 더미에서 사용자가 원하는 정보를 효율적으로 찾아낼 수 있을지를 고민하던 시기이다. 이 개념은 1960년대 코넬 대학교의 제라드 솔튼(Gerard Salton) 교수가 개발한 'SMART 정보 검색 시스템'과 같은 초기 검색 엔진 연구를 통해 구체화되고 널리 알려졌다.\n",
        "\n",
        "* 왜 (Why): 개발 목적은 매우 명확했습니다. 바로 **'컴퓨터를 이용한 문서 검색과 분류'**를 위해서였다.\n",
        "\n",
        "* 문제: 컴퓨터는 '사과'와 '축구'라는 단어의 의미 차이를 모른다. 따라서 \"사과에 대한 문서를 찾아줘\"라는 명령을 그대로 이해할 수 없다.\n",
        "\n",
        "* 해결책 (BoW): 만약 모든 문서를 단어 빈도수 벡터로 바꿔놓는다면 어떨까? 사용자의 검색어(\"사과\") 또한 벡터 [... '사과':1 ...]로 만들 수 있다. 그러면 컴퓨터는 복잡한 의미를 이해할 필요 없이, 단순히 수학적 계산(벡터 간의 유사도 측정)을 통해 검색어 벡터와 가장 '가까운' 문서 벡터들을 찾아내면 된다.\n",
        "\n",
        "이 간단하면서도 강력한 아이디어는 초기 검색 엔진, 스팸 메일 필터링, 문서 자동 분류 등 다양한 분야의 기술적 토대가 되었다.\n"
      ],
      "metadata": {
        "id": "iW13n7YffhuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GpHjB7qxfnRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📚 디지털 인문학에서의 Bag-of-Words 활용\n",
        "\n",
        "디지털 인문학은 컴퓨터 기술을 활용해 문학, 역사, 철학 등 전통적인 인문학 자료를 분석하고 새로운 통찰을 얻는 학문 분야이다. BoW는 텍스트를 정량적 데이터로 바꿔준다는 점에서 디지털 인문학 연구에 매우 유용한 도구이다.\n",
        "\n",
        "* 토픽 모델링 (Topic Modeling):\n",
        "\n",
        "수백, 수천 권의 책이나 신문 기사 전체를 BoW로 변환한 뒤, 통계적 기법(예: LDA)을 적용하여 텍스트 데이터에 잠재된 '주제(Topic)'들을 자동으로 찾아내는 분석이다.\n",
        "\n",
        "활용 예시: 19세기 소설 1,000권을 분석하여 '사랑과 결혼', '전쟁과 사회', '산업혁명과 도시' 등 시대상을 반영하는 주요 주제들이 어떻게 변화하는지 거시적으로 파악할 수 있다.\n",
        "\n",
        "* 저자 판별 (Authorship Attribution):\n",
        "\n",
        "작가마다 자주 사용하는 단어나 단어 빈도에 고유한 스타일이 있다는 점에 착안한 분석이다. 미상의 저자가 쓴 글을 BoW 벡터로 변환한 뒤, 여러 후보 작가들의 작품 벡터와 비교하여 누가 썼을지 추론한다.\n",
        "\n",
        "활용 예시: 특정 고문서의 저자가 알려진 역사적 인물 A인지 B인지 판별하거나, 특정 작가의 미발표 작품으로 추정되는 텍스트의 진위를 가리는 데 사용될 수 있다.\n",
        "\n",
        "* 원거리 독서 (Distant Reading):\n",
        "\n",
        "문학 작품을 한 편 한 편 자세히 읽는 '근거리 독서(Close Reading)'와 반대되는 개념이다. BoW를 통해 방대한 양의 텍스트를 '읽지 않고' 데이터로 간주하여, 그 안의 패턴, 경향, 구조적 특징을 발견하는 연구 방법이다.\n",
        "\n",
        "활용 예시: 특정 시대 문학 작품들에서 특정 감정(예: 불안, 희망)과 관련된 단어들의 출현 빈도 변화를 추적하여 시대적 감성의 흐름을 시각화할 수 있다.\n",
        "\n",
        "BoW는 텍스트의 섬세한 의미나 맥락은 놓칠지라도, 방대한 텍스트 데이터로부터 객관적인 패턴과 구조를 발견할 수 있다."
      ],
      "metadata": {
        "id": "NOibJipCfniV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **고대 중국어(한문) 텍스트**에 Bag-of-Words 모델을 적용할 때 고려해야 될 부분은?\n",
        "\n",
        "* 단어를 어떻게 분해할 것인가(토큰화)이다.\n",
        "\n",
        "* 기존에 사용했던 **CountVectorizer**는 기본적으로 **공백(띄어쓰기)**을 기준으로 단어를 나눈다.\n",
        "\n",
        "* 고대 중국어에는 띄어쓰기가 없다. 고대 중국어 텍스트를 CountVectorizer에 그냥 넣으면, 전체를 하나의 거대한 단어로 인식하여 ['佛說阿彌陀經'] 이렇게 처리해 버린다. 우리가 원하는 ['佛', '說', '阿彌陀經'] 이나 ['佛說', '阿彌陀經'] 같은 결과를 얻을 수 없다.\n",
        "\n"
      ],
      "metadata": {
        "id": "OpbEj8Cel5V8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 띄어쓰기가 없는 고대 중국어 텍스트 예시\n",
        "documents = [\n",
        "    \"佛說阿彌陀經\",\n",
        "    \"觀自在菩薩\"\n",
        "]\n",
        "\n",
        "default_vectorizer = CountVectorizer()\n",
        "\n",
        "bow_matrix = default_vectorizer.fit_transform(documents)\n",
        "\n",
        "vocabulary = default_vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"▼ 생성된 어휘 사전 (문제 상황):\")\n",
        "print(vocabulary)\n",
        "\n",
        "print(\"\\n▼ BoW 행렬:\")\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2HC4gxtfrXM",
        "outputId": "876c2be6-f8c8-465d-fed0-ef1cb4fc489a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▼ 생성된 어휘 사전 (문제 상황):\n",
            "['佛說阿彌陀經' '觀自在菩薩']\n",
            "\n",
            "▼ BoW 행렬:\n",
            "[[1 0]\n",
            " [0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "J96rpfIFqUgz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pbyNU8aqqZdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 해결책: 나만의 '단어 분해기(Tokenizer)'를 제공하기\n",
        "이 문제를 해결하려면 CountVectorizer에 \"공백 대신 이 규칙을 사용해서 단어를 나눠줘!\"라고 우리만의 '단어 분해기(토크나이저)' 함수를 지정해주어야 한다.\n",
        "\n",
        "  * 해결책1: 글자 단위로 분해하기\n",
        "\n",
        "  * 해결책2: 사전 기반으로 단어 단위로 분해하기"
      ],
      "metadata": {
        "id": "SylHRFvAp0yL"
      }
    }
  ]
}