{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNM0dvro81YpEMylPG4j98j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Song-yiJung/DH-Buddhist-Analysis-Tutorial/blob/main/4_%ED%95%9C%EB%AC%B8%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%A1%9C_Bag_of_Words(BoW)%EB%AA%A8%EB%8D%B8_%EB%A7%8C%EB%93%A4%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 한문 텍스트로 Bag-of-Words(BoW) 모델👜 만들기\n",
        "\n",
        "## **Bag-of-Words(BoW): '단어 가방 모델', 컴퓨터가 글을 이해하도록 돕는 간단하고 효과적인 방법**\n",
        "\n",
        "* 문장이나 문서에 어떤 단어가 몇 번 등장했는지만을 여기는 텍스트 표현 방법.\n",
        "\n",
        "* 왜 단어 가방 모델일까? 단어들의 순서는 전혀 고려하지 않고, 모든 단어를 하나의 **가방**에 넣은 뒤 각 단어의 출현 빈도(frequency)를 세는 방식이기 때문이다.\n",
        "\n",
        "## 핵심원리\n",
        "\n",
        "* 어휘 사전 생성 (Vocabulary Building): 가지고 있는 모든 문서의 모든 단어를 중복 없이 모아 하나의 '어휘 사전'을 만든다.\n",
        "* 벡터화 (Vectorization): 각 문서를 어휘 사전에 있는 단어들의 등장 횟수를 기준으로 숫자 벡터(Vector)로 변환한다.\n"
      ],
      "metadata": {
        "id": "Snxi52-0J5mJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [실습] 컴퓨터는 어떻게 글자를 숫자로 바꾸나?"
      ],
      "metadata": {
        "id": "W1IYj96Va4Dp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ku9bWnYIJP7k"
      },
      "outputs": [],
      "source": [
        "# 1. 도구 준비하기\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "도구 준비하기\n",
        "* 짧은 한글 문장을 숫자의 배열(벡터)로 바꾸는 과정.\n",
        "* 데이터 분석 도구 상자인 sklearn에서 CountVectorizer(단어 계수기:문장 속 단어의 개수를 세어주는 전문 도구) 도구  불러오기(import )    "
      ],
      "metadata": {
        "id": "tjbSTWCYbE1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 분석할 문장 준비하기\n",
        "documents = [\n",
        "    \"나는 오늘 사과를 먹었다\",\n",
        "    \"나는 오늘 축구를 했다\"\n",
        "]"
      ],
      "metadata": {
        "id": "nSuTaB7_X-CI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 단어 계수기 작동 준비\n",
        "vectorizer = CountVectorizer()"
      ],
      "metadata": {
        "id": "kH5-HzMKdpHX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**단어 계수기** 작동 준비\n",
        "\n",
        "* 위에서 가져온 CountVectorizer라는 '단어 계수기' 도구를 실제로 사용할 수 있도록 vectorizer라는 이름으로 하나 복사해서 준비시키는 과정\n",
        "\n",
        "* 이제 vectorizer를 통해 단어를 세고 문장을 숫자로 바꿀 수 있다."
      ],
      "metadata": {
        "id": "y-klQGv5drSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 학습과 변환\n",
        "bow_matrix = vectorizer.fit_transform(documents)"
      ],
      "metadata": {
        "id": "iTLhqEmJd7qZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습과 변환\n",
        "* 학습 (fit): vectorizer가 우리가 준 documents의 모든 문장을 훑어보면서 어떤 종류의 단어들이 있는지 목록을 만든다. 이걸 '어휘 사전(Vocabulary)'이라고 부른다.\n",
        "* 변환 (transform): 위에서 만든 '어휘 사전'을 기준으로, 각 문장에 어떤 단어가 몇 번씩 들어있는지 숫자를 센다. 그리고 그 결과를 바탕으로 각 문장을 숫자 배열(벡터)로 변환한다."
      ],
      "metadata": {
        "id": "mcSNqNBreCIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 결과 확인하기\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "print(\"1단계: 생성된 어휘 사전\")\n",
        "print(vocabulary)"
      ],
      "metadata": {
        "id": "B3bDuWFNeNXv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65bc94be-cbb7-4091-9e5f-28ede7e562b8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1단계: 생성된 어휘 사전\n",
            "['나는' '먹었다' '사과를' '오늘' '축구를' '했다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "출력: ['나는' '먹었다' '사과를' '오늘' '축구를' '했다']\n",
        "\n",
        "\"네가 만든 '어휘 사전' 좀 보여줘\" 라고 vectorizer에게 요청하는 코드입니다. 그 결과, documents에서 찾아낸 6개의 고유한 단어 목록을 보여줍니다. 이 목록의 순서가 매우 중요합니다. 왜냐하면 이 순서가 바로 아래에서 볼 숫자 배열의 각 자리와 일치하기 때문이다."
      ],
      "metadata": {
        "id": "Hfj-jlLMeQpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n----------------------------------------\\n\")\n",
        "print(\"2단계: 변환된 벡터\")\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "id": "BtVntlFQeTdy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6feac73-0a0a-4651-bfa4-e724d0a2afc7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----------------------------------------\n",
            "\n",
            "2단계: 변환된 벡터\n",
            "[[1 1 1 1 0 0]\n",
            " [1 0 0 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"숫자로 변환한 최종 결과를 보기 쉽게 표(행렬) 형태로 보여줘\" 라는 코드\n"
      ],
      "metadata": {
        "id": "z_KNS8R7eVon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "사람이 쓰는 '문장'을 단어의 등장 횟수를 기준으로 컴퓨터가 이해할 수 있는 '숫자 신호'로 깔끔하게 번역해주는 과정이다."
      ],
      "metadata": {
        "id": "CT9GOH6SeYuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📚 Bag-of-Words(BoW) 모델의 기원: 누가, 언제, 왜?\n",
        "\n",
        "BoW는 특정 한 명이 \"발명\"한 모델이라기보다는, 특정 문제를 해결하기 위해 자연스럽게 발전한 개념에 가깝다.\n",
        "\n",
        "* 누가/언제 (Who/When): BoW의 근본적인 아이디어는 1950년대부터 정보 검색(Information Retrieval) 분야에서 등장하기 시작했다. 컴퓨터 과학의 초기 연구자들이 어떻게 하면 방대한 문서 더미에서 사용자가 원하는 정보를 효율적으로 찾아낼 수 있을지를 고민하던 시기이다. 이 개념은 1960년대 코넬 대학교의 제라드 솔튼(Gerard Salton) 교수가 개발한 'SMART 정보 검색 시스템'과 같은 초기 검색 엔진 연구를 통해 구체화되고 널리 알려졌다.\n",
        "\n",
        "* 왜 (Why): 개발 목적은 매우 명확했습니다. 바로 **'컴퓨터를 이용한 문서 검색과 분류'**를 위해서였다.\n",
        "\n",
        "* 문제: 컴퓨터는 '사과'와 '축구'라는 단어의 의미 차이를 모른다. 따라서 \"사과에 대한 문서를 찾아줘\"라는 명령을 그대로 이해할 수 없다.\n",
        "\n",
        "* 해결책 (BoW): 만약 모든 문서를 단어 빈도수 벡터로 바꿔놓는다면 어떨까? 사용자의 검색어(\"사과\") 또한 벡터 [... '사과':1 ...]로 만들 수 있다. 그러면 컴퓨터는 복잡한 의미를 이해할 필요 없이, 단순히 수학적 계산(벡터 간의 유사도 측정)을 통해 검색어 벡터와 가장 '가까운' 문서 벡터들을 찾아내면 된다.\n",
        "\n",
        "이 간단하면서도 강력한 아이디어는 초기 검색 엔진, 스팸 메일 필터링, 문서 자동 분류 등 다양한 분야의 기술적 토대가 되었다.\n"
      ],
      "metadata": {
        "id": "iW13n7YffhuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GpHjB7qxfnRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📚 디지털 인문학에서의 Bag-of-Words 활용\n",
        "\n",
        "디지털 인문학은 컴퓨터 기술을 활용해 문학, 역사, 철학 등 전통적인 인문학 자료를 분석하고 새로운 통찰을 얻는 학문 분야이다. BoW는 텍스트를 정량적 데이터로 바꿔준다는 점에서 디지털 인문학 연구에 매우 유용한 도구이다.\n",
        "\n",
        "* 토픽 모델링 (Topic Modeling):\n",
        "\n",
        "수백, 수천 권의 책이나 신문 기사 전체를 BoW로 변환한 뒤, 통계적 기법(예: LDA)을 적용하여 텍스트 데이터에 잠재된 '주제(Topic)'들을 자동으로 찾아내는 분석이다.\n",
        "\n",
        "활용 예시: 19세기 소설 1,000권을 분석하여 '사랑과 결혼', '전쟁과 사회', '산업혁명과 도시' 등 시대상을 반영하는 주요 주제들이 어떻게 변화하는지 거시적으로 파악할 수 있다.\n",
        "\n",
        "* 저자 판별 (Authorship Attribution):\n",
        "\n",
        "작가마다 자주 사용하는 단어나 단어 빈도에 고유한 스타일이 있다는 점에 착안한 분석이다. 미상의 저자가 쓴 글을 BoW 벡터로 변환한 뒤, 여러 후보 작가들의 작품 벡터와 비교하여 누가 썼을지 추론한다.\n",
        "\n",
        "활용 예시: 특정 고문서의 저자가 알려진 역사적 인물 A인지 B인지 판별하거나, 특정 작가의 미발표 작품으로 추정되는 텍스트의 진위를 가리는 데 사용될 수 있다.\n",
        "\n",
        "* 멀리서 읽기 (Distant Reading):\n",
        "\n",
        "문학 작품을 한 편 한 편 자세히 읽는 '근거리 독서(Close Reading)'와 반대되는 개념이다. BoW를 통해 방대한 양의 텍스트를 '읽지 않고' 데이터로 간주하여, 그 안의 패턴, 경향, 구조적 특징을 발견하는 연구 방법이다.\n",
        "\n",
        "활용 예시: 특정 시대 문학 작품들에서 특정 감정(예: 불안, 희망)과 관련된 단어들의 출현 빈도 변화를 추적하여 시대적 감성의 흐름을 시각화할 수 있다.\n",
        "\n",
        "BoW는 텍스트의 섬세한 의미나 맥락은 놓칠지라도, 방대한 텍스트 데이터로부터 객관적인 패턴과 구조를 발견할 수 있다."
      ],
      "metadata": {
        "id": "NOibJipCfniV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **고대 중국어(한문) 텍스트**에 Bag-of-Words 모델을 적용할 때 고려해야 될 부분은?\n",
        "\n",
        "* 단어를 어떻게 분해할 것인가(토큰화)이다.\n",
        "\n",
        "* 기존에 사용했던 **CountVectorizer**는 기본적으로 **공백(띄어쓰기)**을 기준으로 단어를 나눈다.\n",
        "\n",
        "* 고대 중국어에는 띄어쓰기가 없다. 고대 중국어 텍스트를 CountVectorizer에 그냥 넣으면, 전체를 하나의 거대한 단어로 인식하여 ['佛說阿彌陀經'] 이렇게 처리해 버린다. 우리가 원하는 ['佛', '說', '阿彌陀經'] 이나 ['佛說', '阿彌陀經'] 같은 결과를 얻을 수 없다.\n",
        "\n"
      ],
      "metadata": {
        "id": "OpbEj8Cel5V8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 띄어쓰기가 없는 고대 중국어 텍스트 예시\n",
        "documents = [\n",
        "    \"佛說阿彌陀經\",\n",
        "    \"觀自在菩薩\"\n",
        "]\n",
        "\n",
        "default_vectorizer = CountVectorizer()\n",
        "\n",
        "bow_matrix = default_vectorizer.fit_transform(documents)\n",
        "\n",
        "vocabulary = default_vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"▼ 생성된 어휘 사전 (문제 상황):\")\n",
        "print(vocabulary)\n",
        "\n",
        "print(\"\\n▼ BoW 행렬:\")\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2HC4gxtfrXM",
        "outputId": "390b0351-4c5e-493d-fd5d-e36377c9cb74"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▼ 생성된 어휘 사전 (문제 상황):\n",
            "['佛說阿彌陀經' '觀自在菩薩']\n",
            "\n",
            "▼ BoW 행렬:\n",
            "[[1 0]\n",
            " [0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**결과 해석**\n",
        "\n",
        "* **get_feature_names_out()**으로 확인한 어휘 사전(vocabulary)이 ['佛說阿彌陀經', '觀自在菩薩']으로 나왔다 = CountVectorizer가 **'佛說阿彌陀經'**이라는 문자열 전체를 띄어쓰기가 없는 하나의 단어로, '觀自在菩薩' 역시 하나의 단어로 인식했음을 의미한다.\n",
        "\n",
        "* ['佛', '說', '阿彌陀經'] 이나 ['佛說', '阿彌陀經'] 과 같이 의미 단위로 분해된 결과가 전혀 나오지 않았다.\n",
        "\n",
        "* 별도의 토크나이저 지정 없이 CountVectorizer를 고대 중국어 텍스트에 적용했을 때 발생하는 문제점"
      ],
      "metadata": {
        "id": "J96rpfIFqUgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 띄어쓰기가 없는 한문 텍스트 예시\n",
        "documents = [\n",
        "    \"佛說摩訶般若波羅蜜多心經\", # 불설마하반야바라밀다심경\n",
        "    \"觀自在菩薩行深般若波羅蜜多時\"  # 관자재보살행심반야바라밀다시\n",
        "]\n",
        "\n",
        "# 글자 단위로 분해하는 함수 정의\n",
        "def char_tokenizer(text):\n",
        "    \"\"\"문자열을 한 글자씩(char) 잘라서 리스트로 반환합니다.\"\"\"\n",
        "    return list(text)\n",
        "\n",
        "# CountVectorizer에 우리가 만든 토크나이저를 지정\n",
        "# tokenizer=char_tokenizer 부분이 핵심입니다.\n",
        "vectorizer = CountVectorizer(tokenizer=char_tokenizer)\n",
        "\n",
        "bow_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"어휘 사전 (글자 단위):\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "# 출력 예시: ['多', '在', '心', '時', '書', '波', '菩', ... ]\n",
        "\n",
        "print(\"\\nBoW 행렬:\")\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "id": "pbyNU8aqqZdR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "268ebef7-e636-4ea0-ea77-315654525a0a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어휘 사전 (글자 단위):\n",
            "['佛' '在' '多' '心' '摩' '時' '波' '深' '經' '羅' '自' '般' '若' '菩' '薩' '蜜' '行' '觀'\n",
            " '訶' '說']\n",
            "\n",
            "BoW 행렬:\n",
            "[[1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1]\n",
            " [0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 해결책1: **글자 단위로 분해하기**\n",
        "\n",
        "가장 간단하고 직관적인 방법이다. 모든 글자 하나하나를 독립된 단어로 취급하는 방식이다.\n",
        "\n",
        "  * 장점: 구현이 매우 쉽고, 별도의 라이브러리가 필요 없다. 저자 판별 등 스타일 분석에서는 글자 단위의 빈도 패턴이 유용할 수 있다.\n",
        "  * 단점: '佛陀(부처)'나 '菩薩(보살)'처럼 두 글자 이상이 합쳐져 의미를 만드는 단어의 뜻이 완전히 사라진다."
      ],
      "metadata": {
        "id": "SylHRFvAp0yL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 띄어쓰기가 없는 한문 텍스트 예시\n",
        "documents = [\n",
        "    \"佛說摩訶般若波羅蜜多心經\",\n",
        "    \"觀自在菩薩行深般若波羅蜜多時\"\n",
        "]\n",
        "\n",
        "# <<< 핵심: 사용자 사전에 전문 용어 추가 >>>\n",
        "# jieba가 기본적으로 모르는 불교 용어를 사전에 추가해줍니다.\n",
        "# 이렇게 해야 '摩訶般若波羅蜜多'를 한 단어로 인식할 수 있습니다.\n",
        "jieba.add_word('摩訶般若波羅蜜多')\n",
        "jieba.add_word('觀自在菩薩')\n",
        "jieba.add_word('心經')\n",
        "\n",
        "# jieba를 사용해 단어를 분해하는 함수 정의\n",
        "def jieba_tokenizer(text):\n",
        "    \"\"\"jieba 라이브러리를 사용해 단어를 분해하고 리스트로 반환합니다.\"\"\"\n",
        "    return jieba.lcut(text)\n",
        "\n",
        "# CountVectorizer에 jieba 토크나이저를 지정\n",
        "vectorizer = CountVectorizer(tokenizer=jieba_tokenizer)\n",
        "\n",
        "bow_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"어휘 사전 (단어 단위):\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "# 출력 예시: ['佛說', '心經', '摩訶般若波羅蜜多', '觀自在菩薩', ... ]\n",
        "\n",
        "print(\"\\nBoW 행렬:\")\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "id": "tX-k8oAWX5sm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9de416d-e2b4-482c-c27f-e3ca2a772aba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.765 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.765 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어휘 사전 (단어 단위):\n",
            "['佛說' '多時' '心經' '摩訶般若波羅蜜多' '波羅蜜' '般若' '行深' '觀自在菩薩']\n",
            "\n",
            "BoW 행렬:\n",
            "[[1 0 1 1 0 0 0 0]\n",
            " [0 1 0 0 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 해결책 2: 사전(辭典) 기반으로 단어(單語) 단위로 분해하기\n",
        "\n",
        "* 고대 중국어 단어 사전을 기반으로 의미 있는 단어(예: 佛陀, 菩薩, 阿耨多羅三藐三菩提)를 식별해내는 방식. 훨씬 더 정교하고 의미 분석에 유리하다.\n",
        "\n",
        "  * 장점: 단어의 의미를 보존할 수 있어 토픽 모델링 등 semantic 분석에 훨씬 적합하다.\n",
        "  * 단점: 어떤 사전을 사용하느냐에 따라 성능이 크게 좌우된다. 해당 분야(예: 불교)의 전문 용어가 사전에 없으면 제대로 분해하지 못할 수 있다. (이 경우, 사용자 사전에 단어를 추가해야 한다.)"
      ],
      "metadata": {
        "id": "Nktf7hj1X7R0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[실습] **ㅇ** 문구를 BoW 모델을 적용하여 텍스트 분석해보자!\n",
        "\n",
        "해당 부분은 6월 14일, 15일에 업로드 예정\n",
        "\n",
        "본 내용을 학습한 모임 참여자의 실습 코드를 공유할 예정입니다."
      ],
      "metadata": {
        "id": "U6vGircgYdNP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cIu39PMrAPOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kGBGRMXnARC5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KG4yChmVAQYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M81LalLLAR7P"
      }
    }
  ]
}